{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importación de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import re, os, datasets, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.parsing.porter import PorterStemmer \n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,  confusion_matrix, classification_report\n",
    "from sklearn import preprocessing as sk_preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration ucberkeley-dlab--measuring-hate-speech-1d47093687320b66\n",
      "Reusing dataset parquet (C:\\Users\\camilo\\.cache\\huggingface\\datasets\\ucberkeley-dlab___parquet\\ucberkeley-dlab--measuring-hate-speech-1d47093687320b66\\0.0.0\\7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n",
      "100%|██████████| 1/1 [00:00<00:00, 50.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>platform</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>respect</th>\n",
       "      <th>insult</th>\n",
       "      <th>humiliate</th>\n",
       "      <th>status</th>\n",
       "      <th>dehumanize</th>\n",
       "      <th>violence</th>\n",
       "      <th>...</th>\n",
       "      <th>annotator_religion_hindu</th>\n",
       "      <th>annotator_religion_jewish</th>\n",
       "      <th>annotator_religion_mormon</th>\n",
       "      <th>annotator_religion_muslim</th>\n",
       "      <th>annotator_religion_nothing</th>\n",
       "      <th>annotator_religion_other</th>\n",
       "      <th>annotator_sexuality_bisexual</th>\n",
       "      <th>annotator_sexuality_gay</th>\n",
       "      <th>annotator_sexuality_straight</th>\n",
       "      <th>annotator_sexuality_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47777</td>\n",
       "      <td>10873</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39773</td>\n",
       "      <td>2790</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47101</td>\n",
       "      <td>3379</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43625</td>\n",
       "      <td>7365</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12538</td>\n",
       "      <td>488</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_id  annotator_id  platform  sentiment  respect  insult  humiliate  \\\n",
       "0       47777         10873         3        0.0      0.0     0.0        0.0   \n",
       "1       39773          2790         2        0.0      0.0     0.0        0.0   \n",
       "2       47101          3379         3        4.0      4.0     4.0        4.0   \n",
       "3       43625          7365         3        2.0      3.0     2.0        1.0   \n",
       "4       12538           488         0        4.0      4.0     4.0        4.0   \n",
       "\n",
       "   status  dehumanize  violence  ...  annotator_religion_hindu  \\\n",
       "0     2.0         0.0       0.0  ...                     False   \n",
       "1     2.0         0.0       0.0  ...                     False   \n",
       "2     4.0         4.0       0.0  ...                     False   \n",
       "3     2.0         0.0       0.0  ...                     False   \n",
       "4     4.0         4.0       4.0  ...                     False   \n",
       "\n",
       "   annotator_religion_jewish  annotator_religion_mormon  \\\n",
       "0                      False                      False   \n",
       "1                      False                      False   \n",
       "2                      False                      False   \n",
       "3                      False                      False   \n",
       "4                      False                      False   \n",
       "\n",
       "   annotator_religion_muslim annotator_religion_nothing  \\\n",
       "0                      False                      False   \n",
       "1                      False                      False   \n",
       "2                      False                       True   \n",
       "3                      False                      False   \n",
       "4                      False                      False   \n",
       "\n",
       "   annotator_religion_other  annotator_sexuality_bisexual  \\\n",
       "0                     False                         False   \n",
       "1                     False                         False   \n",
       "2                     False                         False   \n",
       "3                     False                         False   \n",
       "4                     False                         False   \n",
       "\n",
       "   annotator_sexuality_gay  annotator_sexuality_straight  \\\n",
       "0                    False                          True   \n",
       "1                    False                          True   \n",
       "2                    False                          True   \n",
       "3                    False                          True   \n",
       "4                    False                          True   \n",
       "\n",
       "   annotator_sexuality_other  \n",
       "0                      False  \n",
       "1                      False  \n",
       "2                      False  \n",
       "3                      False  \n",
       "4                      False  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('ucberkeley-dlab/measuring-hate-speech', 'binary')   \n",
    "df = dataset['train'].to_pandas()\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtag(token: str) -> str:\n",
    "    i, end = len(token) - 2, len(token) + 1\n",
    "    response = ''\n",
    "    while i >= 1:\n",
    "        word = token[i:end]\n",
    "        if word[0].isupper():\n",
    "            response = token[i:end] + ' ' + response\n",
    "            end = i\n",
    "        i -= 1\n",
    "    else:\n",
    "        if end-1 > 0:\n",
    "            response = token[1:end] + ' ' + response\n",
    "    return(response.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(document: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace all abbreviations with their corresponding expansion\n",
    "    \"\"\"\n",
    "    document = re.sub(r\"'cause\", \"because\", document)\n",
    "    document = re.sub(r\"o'clock\", \"of the clock\", document)\n",
    "    document = re.sub(r\"won\\'t\", \"will not\", document)\n",
    "    document = re.sub(r\"can\\'t\", \"can not\", document)\n",
    "    document = re.sub(r\"n\\'t\", \" not\", document)\n",
    "    document = re.sub(r\"\\'re\", \" are\", document)\n",
    "    document = re.sub(r\"\\'s\", \" is\", document)\n",
    "    document = re.sub(r\"\\'d\", \" would\", document)\n",
    "    document = re.sub(r\"\\'ll\", \" will\", document)\n",
    "    document = re.sub(r\"\\'t\", \" not\", document)\n",
    "    document = re.sub(r\"\\'ve\", \" have\", document)\n",
    "    document = re.sub(r\"\\'m\", \" am\", document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document: str) -> list:\n",
    "    \"\"\"\n",
    "    expand contractions\n",
    "    replace any @user type with 'user' token\n",
    "    split hashtags into words by upper case\n",
    "    text to lower case\n",
    "    remove new line char\n",
    "    remove non latin chars\n",
    "    remove extra blank spaces\n",
    "    \"\"\"\n",
    "    document = expand_contractions(document)\n",
    "    document = re.sub('@[a-zA-Z0-9_-]{0,150}', 'user', document)\n",
    "    hashtags = re.findall('#[a-zA-Z0-9_-]{0,150}', document)\n",
    "    for hashtag in hashtags:\n",
    "        split = split_hashtag(hashtag)\n",
    "        document = re.sub(hashtag, split, document)\n",
    "    document = document.lower()\n",
    "    document = re.sub('\\n', ' ', document)\n",
    "    document = re.sub('[^a-zA-Z]|[0-9]', ' ', document)\n",
    "    document = re.sub('\\s+', ' ', document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['sentiment', 'respect', 'insult', 'humiliate', 'status', 'dehumanize', 'violence', 'genocide', 'attack_defend', 'hatespeech', 'hate_speech_score']\n",
    "data = df[columns]\n",
    "min_max_scaler = sk_preprocess.MinMaxScaler()\n",
    "data_scaled = min_max_scaler.fit_transform(data.values)\n",
    "dataset = pd.DataFrame(data_scaled, columns=columns)\n",
    "dataset['text'] = df.text\n",
    "dataset['processed'] = df.text.apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>hate_speech_score</th>\n",
       "      <th>text</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52366</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.278005</td>\n",
       "      <td>I love lesbian weddings.</td>\n",
       "      <td>i love lesbian weddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67560</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580601</td>\n",
       "      <td>Bernie is going to give every American free n-...</td>\n",
       "      <td>bernie is going to give every american free n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87262</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.604508</td>\n",
       "      <td>@benshapiro go rape yourself you cause a lot o...</td>\n",
       "      <td>user go rape yourself you cause a lot of violence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85895</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.685792</td>\n",
       "      <td>Cough up your cash and beg.  On your knees. I ...</td>\n",
       "      <td>cough up your cash and beg on your knees i wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124972</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409153</td>\n",
       "      <td>oh that's cute. you didn't even read the thing...</td>\n",
       "      <td>oh that is cute you did not even read the thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85638</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.422131</td>\n",
       "      <td>@DrStevePerry Laws changing will do very littl...</td>\n",
       "      <td>user laws changing will do very little getting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97516</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.836749</td>\n",
       "      <td>yeah, heres a clue for the nigger author of th...</td>\n",
       "      <td>yeah heres a clue for the nigger author of thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80956</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.551230</td>\n",
       "      <td>@Mitumba10 He needs to be in a mental institut...</td>\n",
       "      <td>user he needs to be in a mental institution an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111278</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.670082</td>\n",
       "      <td>My God look at these BARBARIC PPL. And these p...</td>\n",
       "      <td>my god look at these barbaric ppl and these pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78002</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.202186</td>\n",
       "      <td>In the Apostolic Churches there's monasteries ...</td>\n",
       "      <td>in the apostolic churches there is monasteries...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        hatespeech  hate_speech_score  \\\n",
       "52366          0.0           0.278005   \n",
       "67560          0.0           0.580601   \n",
       "87262          0.5           0.604508   \n",
       "85895          1.0           0.685792   \n",
       "124972         0.0           0.409153   \n",
       "85638          0.0           0.422131   \n",
       "97516          1.0           0.836749   \n",
       "80956          0.0           0.551230   \n",
       "111278         1.0           0.670082   \n",
       "78002          0.0           0.202186   \n",
       "\n",
       "                                                     text  \\\n",
       "52366                            I love lesbian weddings.   \n",
       "67560   Bernie is going to give every American free n-...   \n",
       "87262   @benshapiro go rape yourself you cause a lot o...   \n",
       "85895   Cough up your cash and beg.  On your knees. I ...   \n",
       "124972  oh that's cute. you didn't even read the thing...   \n",
       "85638   @DrStevePerry Laws changing will do very littl...   \n",
       "97516   yeah, heres a clue for the nigger author of th...   \n",
       "80956   @Mitumba10 He needs to be in a mental institut...   \n",
       "111278  My God look at these BARBARIC PPL. And these p...   \n",
       "78002   In the Apostolic Churches there's monasteries ...   \n",
       "\n",
       "                                                processed  \n",
       "52366                            i love lesbian weddings   \n",
       "67560   bernie is going to give every american free n ...  \n",
       "87262   user go rape yourself you cause a lot of violence  \n",
       "85895   cough up your cash and beg on your knees i wan...  \n",
       "124972  oh that is cute you did not even read the thin...  \n",
       "85638   user laws changing will do very little getting...  \n",
       "97516   yeah heres a clue for the nigger author of thi...  \n",
       "80956   user he needs to be in a mental institution an...  \n",
       "111278  my god look at these barbaric ppl and these pi...  \n",
       "78002   in the apostolic churches there is monasteries...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sample(10)[['hatespeech','hate_speech_score','text','processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126504, 13)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset[dataset['hatespeech']>0] = 1\n",
    "index_condition = dataset[ (dataset['hatespeech'] != 0) & (dataset['hatespeech']!= 1)].index\n",
    "dataset.drop(index_condition,inplace=True)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the total set: 126504\n",
      "Number of rows in the training set: 88552\n",
      "Number of rows in the validation set: 12651\n",
      "Number of rows in the test set: 25301\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_sample = dataset['hatespeech'].astype(float)\n",
    "X_sample = dataset['processed'].astype(str)\n",
    "\n",
    "X_train_validation, X_test, y_train_validation, y_test = train_test_split(X_sample.values, \n",
    "                                                    y_sample.values, \n",
    "                                                    random_state=42,test_size=0.2)\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train_validation, \n",
    "                                                    y_train_validation, \n",
    "                                                    random_state=42,test_size=0.125)\n",
    "\n",
    "print('Number of rows in the total set: {}'.format(X_sample.shape[0]))\n",
    "print('Number of rows in the training set: {}'.format(X_train.shape[0]))\n",
    "print('Number of rows in the validation set: {}'.format(X_validation.shape[0]))\n",
    "print('Number of rows in the test set: {}'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genera la representación tf del vocabulario\n",
    "count_vector = CountVectorizer()\n",
    "training_data_tf = count_vector.fit_transform(X_train)\n",
    "validation_data_tf = count_vector.transform(X_validation)\n",
    "testing_data_tf = count_vector.transform(X_test)\n",
    "\n",
    "training_validation_x_tf = np.concatenate((X_train,X_validation))\n",
    "cross_validation_x_tf = count_vector.transform(training_validation_x_tf)\n",
    "cross_validation_y_tf = np.concatenate((y_train,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se normaliza la representación vectorial\n",
    "training_data_tf_norm = Normalizer().fit_transform(training_data_tf)\n",
    "validation_data_tf_norm = Normalizer().fit_transform(validation_data_tf)\n",
    "testing_data_tf_norm = Normalizer().fit_transform(testing_data_tf)\n",
    "cross_validation_data_tf_norm = Normalizer().fit_transform(cross_validation_x_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genera la representación tfidf del vocabulario\n",
    "count_vector_tfidf = TfidfVectorizer()\n",
    "training_data_tfidf = count_vector_tfidf.fit_transform(X_train)\n",
    "validation_data_tfidf = count_vector_tfidf.transform(X_validation)\n",
    "testing_data_tfidf = count_vector_tfidf.transform(X_test)\n",
    "\n",
    "training_validation_x_tfidf = np.concatenate((X_train,X_validation))\n",
    "cross_validation_x_tfidf = count_vector.transform(training_validation_x_tfidf)\n",
    "cross_validation_y_tfidf = np.concatenate((y_train,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se normaliza la representación vectorial\n",
    "training_data_tfidf_norm = Normalizer().fit_transform(training_data_tfidf) \n",
    "validation_data_tfidf_norm = Normalizer().fit_transform(validation_data_tfidf)\n",
    "testing_data_tfidf_norm = Normalizer().fit_transform(testing_data_tfidf)\n",
    "cross_validation_data_tfidf_norm = Normalizer().fit_transform(cross_validation_x_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.06083679, 0.05485702, 0.04886174, 0.06283188, 0.04488111,\n",
       "        0.04986572, 0.0458703 , 0.04787278, 0.04488063, 0.04787111]),\n",
       " 'score_time': array([0.02692819, 0.02193975, 0.01894903, 0.01894808, 0.01894927,\n",
       "        0.01994586, 0.01995325, 0.02097607, 0.01795125, 0.01795292]),\n",
       " 'test_accuracy': array([0.81918783, 0.8164213 , 0.8118763 , 0.81897233, 0.80988142,\n",
       "        0.81363636, 0.8201581 , 0.81462451, 0.81778656, 0.81630435]),\n",
       " 'test_precision_macro': array([0.81851758, 0.81745259, 0.80966809, 0.81695564, 0.80885672,\n",
       "        0.8133521 , 0.82054684, 0.81099185, 0.81635947, 0.81598778]),\n",
       " 'test_recall_macro': array([0.78241843, 0.77738894, 0.77445981, 0.78342231, 0.77080236,\n",
       "        0.77497606, 0.78268007, 0.77938349, 0.78128392, 0.77836985]),\n",
       " 'test_f1_macro': array([0.79422695, 0.78991522, 0.78587252, 0.79466359, 0.78272094,\n",
       "        0.78708028, 0.79489006, 0.79008699, 0.79283718, 0.79041189])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento del algoritmo de naive bayes y cross validation para la\n",
    "# representación vectoria de tf\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data_tf_norm,y_train)\n",
    "scores = cross_validate(naive_bayes, cross_validation_data_tf_norm, cross_validation_y_tf, cv=10, scoring=('accuracy','precision_macro','recall_macro','f1_macro'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Accuracy Through Grid Search : 0.8200266701260188\n",
      "best parameter :  {'alpha': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Busqueda de hiperaparametros variando el alpha\n",
    "\n",
    "params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0],}\n",
    "\n",
    "multinomial_nb_grid = GridSearchCV(MultinomialNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
    "multinomial_nb_grid.fit(training_data_tf_norm,y_train)\n",
    "\n",
    "print(f'Best Accuracy Through Grid Search : {multinomial_nb_grid.best_score_}')\n",
    "print('best parameter : ', multinomial_nb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.08377743, 0.125664  , 0.07878304, 0.08278489, 0.1007297 ,\n",
       "        0.09973335, 0.09574342, 0.07081056, 0.06383252, 0.06582594]),\n",
       " 'score_time': array([0.03889608, 0.03590512, 0.03291106, 0.0428791 , 0.04986668,\n",
       "        0.05884123, 0.02892256, 0.03590727, 0.03191113, 0.02892303]),\n",
       " 'test_accuracy': array([0.8208675 , 0.82373283, 0.81839739, 0.81798419, 0.81551383,\n",
       "        0.82262846, 0.8284585 , 0.81996047, 0.8208498 , 0.82282609]),\n",
       " 'test_precision_macro': array([0.81142681, 0.81471724, 0.80858858, 0.80693554, 0.80550247,\n",
       "        0.8131825 , 0.81928692, 0.80892164, 0.81070735, 0.81349899]),\n",
       " 'test_recall_macro': array([0.79475486, 0.79782179, 0.79211498, 0.79371901, 0.78868954,\n",
       "        0.79707611, 0.8041317 , 0.79623187, 0.79582218, 0.79708286]),\n",
       " 'test_f1_macro': array([0.80146473, 0.80462704, 0.79874085, 0.79924341, 0.79540052,\n",
       "        0.80361347, 0.81038999, 0.80158144, 0.80193978, 0.80372524])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluación del mejor modelo\n",
    "naive_bayes_best_1 = MultinomialNB(alpha=0.01)\n",
    "naive_bayes_best_1.fit(training_data_tf_norm,y_train)\n",
    "scores = cross_validate(naive_bayes_best_1, cross_validation_data_tf_norm, cross_validation_y_tf, cv=10, scoring=('accuracy','precision_macro','recall_macro','f1_macro'))\n",
    "scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.92      0.87     16129\n",
      "         1.0       0.82      0.64      0.72      9172\n",
      "\n",
      "    accuracy                           0.82     25301\n",
      "   macro avg       0.82      0.78      0.79     25301\n",
      "weighted avg       0.82      0.82      0.81     25301\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14824, 1305, 3321, 5851)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = naive_bayes_best_1.predict(testing_data_tf_norm)  \n",
    "report = classification_report(y_test ,pred)  \n",
    "print(report)\n",
    "#En terminos de TP, FP, TN, FN\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "(tn,fp,fn,tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.05086827, 0.08876204, 0.10372305, 0.08477235, 0.05086303,\n",
       "        0.04288507, 0.0478723 , 0.04886937, 0.04488015, 0.04388213]),\n",
       " 'score_time': array([0.02692199, 0.03789854, 0.03690124, 0.03490925, 0.01795149,\n",
       "        0.01795173, 0.03091788, 0.01894879, 0.01994658, 0.0189476 ]),\n",
       " 'test_accuracy': array([0.81918783, 0.8164213 , 0.8118763 , 0.81897233, 0.80988142,\n",
       "        0.81363636, 0.8201581 , 0.81462451, 0.81778656, 0.81630435]),\n",
       " 'test_precision_macro': array([0.81851758, 0.81745259, 0.80966809, 0.81695564, 0.80885672,\n",
       "        0.8133521 , 0.82054684, 0.81099185, 0.81635947, 0.81598778]),\n",
       " 'test_recall_macro': array([0.78241843, 0.77738894, 0.77445981, 0.78342231, 0.77080236,\n",
       "        0.77497606, 0.78268007, 0.77938349, 0.78128392, 0.77836985]),\n",
       " 'test_f1_macro': array([0.79422695, 0.78991522, 0.78587252, 0.79466359, 0.78272094,\n",
       "        0.78708028, 0.79489006, 0.79008699, 0.79283718, 0.79041189])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento del algoritmo de naive bayes y cross validation para la\n",
    "# representación vectoria de tfidf\n",
    "\n",
    "naive_bayes_2 = MultinomialNB()\n",
    "naive_bayes_2.fit(training_data_tfidf_norm,y_train)\n",
    "scores = cross_validate(naive_bayes, cross_validation_data_tfidf_norm, cross_validation_y_tfidf, cv=10, scoring=('accuracy','precision_macro','recall_macro','f1_macro'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Accuracy Through Grid Search : 0.8194168553704386\n",
      "best parameter :  {'alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Busqueda de hiperaparametros variando el alpha\n",
    "\n",
    "params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0, ],}\n",
    "\n",
    "multinomial_nb_grid = GridSearchCV(MultinomialNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
    "multinomial_nb_grid.fit(training_data_tfidf_norm,y_train)\n",
    "\n",
    "print(f'Best Accuracy Through Grid Search : {multinomial_nb_grid.best_score_}')\n",
    "print('best parameter : ', multinomial_nb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.07081127, 0.05984163, 0.08178067, 0.09674406, 0.07679486,\n",
       "        0.04587746, 0.0459094 , 0.04291797, 0.04188895, 0.04192305]),\n",
       " 'score_time': array([0.02692842, 0.01894832, 0.04188824, 0.02991867, 0.02393556,\n",
       "        0.0209434 , 0.01795149, 0.01795244, 0.02293992, 0.01794887]),\n",
       " 'test_accuracy': array([0.8208675 , 0.82373283, 0.81839739, 0.81798419, 0.81551383,\n",
       "        0.82262846, 0.8284585 , 0.81996047, 0.8208498 , 0.82282609]),\n",
       " 'test_precision_macro': array([0.81142681, 0.81471724, 0.80858858, 0.80693554, 0.80550247,\n",
       "        0.8131825 , 0.81928692, 0.80892164, 0.81070735, 0.81349899]),\n",
       " 'test_recall_macro': array([0.79475486, 0.79782179, 0.79211498, 0.79371901, 0.78868954,\n",
       "        0.79707611, 0.8041317 , 0.79623187, 0.79582218, 0.79708286]),\n",
       " 'test_f1_macro': array([0.80146473, 0.80462704, 0.79874085, 0.79924341, 0.79540052,\n",
       "        0.80361347, 0.81038999, 0.80158144, 0.80193978, 0.80372524])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluación del mejor modelo\n",
    "naive_bayes_best_2 = MultinomialNB(alpha=0.01)\n",
    "naive_bayes_best_2.fit(training_data_tfidf_norm,y_train)\n",
    "scores = cross_validate(naive_bayes_best_2, cross_validation_data_tfidf_norm, cross_validation_y_tfidf, cv=10, scoring=('accuracy','precision_macro','recall_macro','f1_macro'))\n",
    "scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.8220228449468401\n",
      "Macro-Precision score:  0.810810352340644\n",
      "Macro-Recall score:  0.798612781700051\n",
      "Macro-F1 score:  0.8038038755119001\n",
      "Micro-Precision score:  0.8220228449468401\n",
      "Micro-Recall score:  0.8220228449468401\n",
      "Micro-F1 score:  0.8220228449468401\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo\n",
    "\n",
    "predictions = naive_bayes_best_2.predict(testing_data_tfidf_norm)\n",
    "print('Accuracy score: ', format(accuracy_score(y_test, predictions)))\n",
    "print('Macro-Precision score: ', format(precision_score(y_test, predictions,average='macro')))\n",
    "print('Macro-Recall score: ', format(recall_score(y_test, predictions,average='macro')))\n",
    "print('Macro-F1 score: ', format(f1_score(y_test, predictions,average='macro')))\n",
    "\n",
    "print('Micro-Precision score: ', format(precision_score(y_test, predictions,average='micro')))\n",
    "print('Micro-Recall score: ', format(recall_score(y_test, predictions,average='micro')))\n",
    "print('Micro-F1 score: ', format(f1_score(y_test, predictions,average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.88      0.86     16129\n",
      "         1.0       0.78      0.71      0.74      9172\n",
      "\n",
      "    accuracy                           0.82     25301\n",
      "   macro avg       0.81      0.80      0.80     25301\n",
      "weighted avg       0.82      0.82      0.82     25301\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14254, 1875, 2628, 6544)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = naive_bayes_best_2.predict(testing_data_tfidf_norm)  \n",
    "report = classification_report(y_test ,pred)  \n",
    "print(report)\n",
    "#En terminos de TP, FP, TN, FN\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "(tn,fp,fn,tp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7a79e9e75522a046d95171e373010a5dca4ce6e8605d007854b2218f1d88052"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
