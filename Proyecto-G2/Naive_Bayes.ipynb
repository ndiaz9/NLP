{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importación de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, datasets, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importación del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration ucberkeley-dlab--measuring-hate-speech-1d47093687320b66\n",
      "Reusing dataset parquet (C:\\Users\\Usuario\\.cache\\huggingface\\datasets\\ucberkeley-dlab___parquet\\ucberkeley-dlab--measuring-hate-speech-1d47093687320b66\\0.0.0\\7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff552adb9204dccb6257de1b09c1181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>platform</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>respect</th>\n",
       "      <th>insult</th>\n",
       "      <th>humiliate</th>\n",
       "      <th>status</th>\n",
       "      <th>dehumanize</th>\n",
       "      <th>violence</th>\n",
       "      <th>...</th>\n",
       "      <th>annotator_religion_hindu</th>\n",
       "      <th>annotator_religion_jewish</th>\n",
       "      <th>annotator_religion_mormon</th>\n",
       "      <th>annotator_religion_muslim</th>\n",
       "      <th>annotator_religion_nothing</th>\n",
       "      <th>annotator_religion_other</th>\n",
       "      <th>annotator_sexuality_bisexual</th>\n",
       "      <th>annotator_sexuality_gay</th>\n",
       "      <th>annotator_sexuality_straight</th>\n",
       "      <th>annotator_sexuality_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47777</td>\n",
       "      <td>10873</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39773</td>\n",
       "      <td>2790</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47101</td>\n",
       "      <td>3379</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43625</td>\n",
       "      <td>7365</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12538</td>\n",
       "      <td>488</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_id  annotator_id  platform  sentiment  respect  insult  humiliate  \\\n",
       "0       47777         10873         3        0.0      0.0     0.0        0.0   \n",
       "1       39773          2790         2        0.0      0.0     0.0        0.0   \n",
       "2       47101          3379         3        4.0      4.0     4.0        4.0   \n",
       "3       43625          7365         3        2.0      3.0     2.0        1.0   \n",
       "4       12538           488         0        4.0      4.0     4.0        4.0   \n",
       "\n",
       "   status  dehumanize  violence  ...  annotator_religion_hindu  \\\n",
       "0     2.0         0.0       0.0  ...                     False   \n",
       "1     2.0         0.0       0.0  ...                     False   \n",
       "2     4.0         4.0       0.0  ...                     False   \n",
       "3     2.0         0.0       0.0  ...                     False   \n",
       "4     4.0         4.0       4.0  ...                     False   \n",
       "\n",
       "   annotator_religion_jewish  annotator_religion_mormon  \\\n",
       "0                      False                      False   \n",
       "1                      False                      False   \n",
       "2                      False                      False   \n",
       "3                      False                      False   \n",
       "4                      False                      False   \n",
       "\n",
       "   annotator_religion_muslim annotator_religion_nothing  \\\n",
       "0                      False                      False   \n",
       "1                      False                      False   \n",
       "2                      False                       True   \n",
       "3                      False                      False   \n",
       "4                      False                      False   \n",
       "\n",
       "   annotator_religion_other  annotator_sexuality_bisexual  \\\n",
       "0                     False                         False   \n",
       "1                     False                         False   \n",
       "2                     False                         False   \n",
       "3                     False                         False   \n",
       "4                     False                         False   \n",
       "\n",
       "   annotator_sexuality_gay  annotator_sexuality_straight  \\\n",
       "0                    False                          True   \n",
       "1                    False                          True   \n",
       "2                    False                          True   \n",
       "3                    False                          True   \n",
       "4                    False                          True   \n",
       "\n",
       "   annotator_sexuality_other  \n",
       "0                      False  \n",
       "1                      False  \n",
       "2                      False  \n",
       "3                      False  \n",
       "4                      False  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('ucberkeley-dlab/measuring-hate-speech', 'binary')   \n",
    "df = dataset['train'].to_pandas()\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### funciones de procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtag(token: str) -> str:\n",
    "    \"\"\"\n",
    "    Split hashtags by uppercases\n",
    "    \"\"\"\n",
    "    i, end = len(token) - 2, len(token) + 1\n",
    "    response = ''\n",
    "    while i >= 1:\n",
    "        word = token[i:end]\n",
    "        if word[0].isupper():\n",
    "            response = token[i:end] + ' ' + response\n",
    "            end = i\n",
    "        i -= 1\n",
    "    else:\n",
    "        if end-1 > 0:\n",
    "            response = token[1:end] + ' ' + response\n",
    "    return(response.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(document: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace all abbreviations with their corresponding expansion\n",
    "    \"\"\"\n",
    "    document = re.sub(r\"'cause\", \"because\", document)\n",
    "    document = re.sub(r\"o'clock\", \"of the clock\", document)\n",
    "    document = re.sub(r\"won\\'t\", \"will not\", document)\n",
    "    document = re.sub(r\"can\\'t\", \"can not\", document)\n",
    "    document = re.sub(r\"n\\'t\", \" not\", document)\n",
    "    document = re.sub(r\"\\'re\", \" are\", document)\n",
    "    document = re.sub(r\"\\'s\", \" is\", document)\n",
    "    document = re.sub(r\"\\'d\", \" would\", document)\n",
    "    document = re.sub(r\"\\'ll\", \" will\", document)\n",
    "    document = re.sub(r\"\\'t\", \" not\", document)\n",
    "    document = re.sub(r\"\\'ve\", \" have\", document)\n",
    "    document = re.sub(r\"\\'m\", \" am\", document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document: str) -> list:\n",
    "    \"\"\"\n",
    "    expand contractions\n",
    "    replace any @user type with 'user' token\n",
    "    split hashtags into words by upper case\n",
    "    text to lower case\n",
    "    remove new line char\n",
    "    remove non latin chars\n",
    "    remove extra blank spaces\n",
    "    \"\"\"\n",
    "    document = expand_contractions(document)\n",
    "    document = re.sub('@[a-zA-Z0-9_-]{0,150}', 'user', document)\n",
    "    hashtags = re.findall('#[a-zA-Z0-9_-]{0,150}', document)\n",
    "    for hashtag in hashtags:\n",
    "        split = split_hashtag(hashtag)\n",
    "        document = re.sub(hashtag, split, document)\n",
    "    document = document.lower()\n",
    "    document = re.sub('\\n', ' ', document)\n",
    "    document = re.sub('[^a-zA-Z]|[0-9]', ' ', document)\n",
    "    document = re.sub('\\s+', ' ', document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicar preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135388, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "data['text'] = df.text\n",
    "data['processed'] = df.text.apply(preprocessing)\n",
    "data['hatespeech'] = df.hatespeech.apply(lambda x: math.ceil(x/2))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>text</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38636</th>\n",
       "      <td>0</td>\n",
       "      <td>RT rdunbar83: RT yorkshiremesmac: Our comms co...</td>\n",
       "      <td>rt rdunbar rt yorkshiremesmac our comms coordi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129290</th>\n",
       "      <td>0</td>\n",
       "      <td>I've always been proud of the Igbo tribe and o...</td>\n",
       "      <td>i have always been proud of the igbo tribe and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35781</th>\n",
       "      <td>0</td>\n",
       "      <td>Shut the f*%k up with the religious crap. Naus...</td>\n",
       "      <td>shut the f k up with the religious crap nausea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94019</th>\n",
       "      <td>1</td>\n",
       "      <td>i hate jews so much i wish the holocaust actua...</td>\n",
       "      <td>i hate jews so much i wish the holocaust actua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92571</th>\n",
       "      <td>1</td>\n",
       "      <td>i hate jews so much i wish the holocaust actua...</td>\n",
       "      <td>i hate jews so much i wish the holocaust actua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13758</th>\n",
       "      <td>0</td>\n",
       "      <td>i thought that lady had a very hairy chest bec...</td>\n",
       "      <td>i thought that lady had a very hairy chest bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29532</th>\n",
       "      <td>0</td>\n",
       "      <td>@4MeSheWILL You can tell be the way she used E...</td>\n",
       "      <td>user you can tell be the way she used english ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125252</th>\n",
       "      <td>0</td>\n",
       "      <td>There is plenty of evidence that women and non...</td>\n",
       "      <td>there is plenty of evidence that women and non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101861</th>\n",
       "      <td>1</td>\n",
       "      <td>Every slant in #LosAngeles should be deported....</td>\n",
       "      <td>every slant in los angeles should be deported ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54218</th>\n",
       "      <td>0</td>\n",
       "      <td>Little boys smh URL</td>\n",
       "      <td>little boys smh url</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        hatespeech                                               text  \\\n",
       "38636            0  RT rdunbar83: RT yorkshiremesmac: Our comms co...   \n",
       "129290           0  I've always been proud of the Igbo tribe and o...   \n",
       "35781            0  Shut the f*%k up with the religious crap. Naus...   \n",
       "94019            1  i hate jews so much i wish the holocaust actua...   \n",
       "92571            1  i hate jews so much i wish the holocaust actua...   \n",
       "13758            0  i thought that lady had a very hairy chest bec...   \n",
       "29532            0  @4MeSheWILL You can tell be the way she used E...   \n",
       "125252           0  There is plenty of evidence that women and non...   \n",
       "101861           1  Every slant in #LosAngeles should be deported....   \n",
       "54218            0                                Little boys smh URL   \n",
       "\n",
       "                                                processed  \n",
       "38636   rt rdunbar rt yorkshiremesmac our comms coordi...  \n",
       "129290  i have always been proud of the igbo tribe and...  \n",
       "35781   shut the f k up with the religious crap nausea...  \n",
       "94019   i hate jews so much i wish the holocaust actua...  \n",
       "92571   i hate jews so much i wish the holocaust actua...  \n",
       "13758   i thought that lady had a very hairy chest bec...  \n",
       "29532   user you can tell be the way she used english ...  \n",
       "125252  there is plenty of evidence that women and non...  \n",
       "101861  every slant in los angeles should be deported ...  \n",
       "54218                                 little boys smh url  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)[['hatespeech','text','processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the total set: 135388\n",
      "Number of rows in the training set: 94771\n",
      "Number of rows in the validation set: 13539\n",
      "Number of rows in the test set: 27078\n"
     ]
    }
   ],
   "source": [
    "y_sample = data['hatespeech'].astype(float)\n",
    "X_sample = data['processed'].astype(str)\n",
    "\n",
    "X_train_validation, X_test, y_train_validation, y_test = train_test_split(X_sample.values, \n",
    "                                                    y_sample.values, \n",
    "                                                    random_state=42,test_size=0.2)\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train_validation, \n",
    "                                                    y_train_validation, \n",
    "                                                    random_state=42,test_size=0.125)\n",
    "\n",
    "print('Number of rows in the total set: {}'.format(X_sample.shape[0]))\n",
    "print('Number of rows in the training set: {}'.format(X_train.shape[0]))\n",
    "print('Number of rows in the validation set: {}'.format(X_validation.shape[0]))\n",
    "print('Number of rows in the test set: {}'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genera la representación tf del vocabulario\n",
    "count_vector = CountVectorizer()\n",
    "training_data_tf = count_vector.fit_transform(X_train)\n",
    "validation_data_tf = count_vector.transform(X_validation)\n",
    "testing_data_tf = count_vector.transform(X_test)\n",
    "\n",
    "training_validation_x_tf = np.concatenate((X_train,X_validation))\n",
    "cross_validation_x_tf = count_vector.transform(training_validation_x_tf)\n",
    "cross_validation_y_tf = np.concatenate((y_train,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se normaliza la representación vectorial\n",
    "training_data_tf_norm = Normalizer().fit_transform(training_data_tf)\n",
    "validation_data_tf_norm = Normalizer().fit_transform(validation_data_tf)\n",
    "testing_data_tf_norm = Normalizer().fit_transform(testing_data_tf)\n",
    "cross_validation_data_tf_norm = Normalizer().fit_transform(cross_validation_x_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genera la representación tfidf del vocabulario\n",
    "count_vector_tfidf = TfidfVectorizer()\n",
    "training_data_tfidf = count_vector_tfidf.fit_transform(X_train)\n",
    "validation_data_tfidf = count_vector_tfidf.transform(X_validation)\n",
    "testing_data_tfidf = count_vector_tfidf.transform(X_test)\n",
    "\n",
    "training_validation_x_tfidf = np.concatenate((X_train,X_validation))\n",
    "cross_validation_x_tfidf = count_vector.transform(training_validation_x_tfidf)\n",
    "cross_validation_y_tfidf = np.concatenate((y_train,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se normaliza la representación vectorial\n",
    "training_data_tfidf_norm = Normalizer().fit_transform(training_data_tfidf) \n",
    "validation_data_tfidf_norm = Normalizer().fit_transform(validation_data_tfidf)\n",
    "testing_data_tfidf_norm = Normalizer().fit_transform(testing_data_tfidf)\n",
    "cross_validation_data_tfidf_norm = Normalizer().fit_transform(cross_validation_x_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.05865812, 0.06015301, 0.05986738, 0.05987811, 0.06423068,\n",
       "        0.06049871, 0.05900192, 0.06279445, 0.05665112, 0.0573132 ]),\n",
       " 'score_time': array([0.02625418, 0.03969049, 0.0245204 , 0.04143858, 0.02700233,\n",
       "        0.04408097, 0.03174829, 0.02500081, 0.02318835, 0.0410881 ]),\n",
       " 'test_accuracy': array([0.7850614 , 0.78810821, 0.78903148, 0.77979873, 0.78746191,\n",
       "        0.79300157, 0.79106269, 0.78561536, 0.780722  , 0.79614071]),\n",
       " 'test_precision_macro': array([0.78570529, 0.78930827, 0.78842753, 0.77921725, 0.78722441,\n",
       "        0.79459859, 0.79143619, 0.78565969, 0.78191154, 0.79886662]),\n",
       " 'test_recall_macro': array([0.76242002, 0.76548471, 0.76818317, 0.75751899, 0.76602934,\n",
       "        0.77075954, 0.76960083, 0.76364263, 0.75698964, 0.77343515]),\n",
       " 'test_f1_macro': array([0.7689634 , 0.77217867, 0.77428983, 0.7636883 , 0.77227541,\n",
       "        0.7775681 , 0.77603032, 0.76998995, 0.76366186, 0.78057223])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento del algoritmo de naive bayes y cross validation para la\n",
    "# representación vectoria de tf\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data_tf_norm,y_train)\n",
    "scores = cross_validate(naive_bayes, cross_validation_data_tf_norm, cross_validation_y_tf, cv=10, scoring=('accuracy','precision_macro','recall_macro','f1_macro'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Accuracy Through Grid Search : 0.7898407626266004\n",
      "best parameter :  {'alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Busqueda de hiperaparametros variando el alpha\n",
    "\n",
    "params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0],}\n",
    "\n",
    "multinomial_nb_grid = GridSearchCV(MultinomialNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
    "multinomial_nb_grid.fit(training_data_tf_norm,y_train)\n",
    "\n",
    "print(f'Best Accuracy Through Grid Search : {multinomial_nb_grid.best_score_}')\n",
    "print('best parameter : ', multinomial_nb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.05198264, 0.05609059, 0.05823493, 0.04116297, 0.06541467,\n",
       "        0.07554078, 0.05039787, 0.05966139, 0.05334234, 0.05624938]),\n",
       " 'score_time': array([0.02574801, 0.02860379, 0.03216743, 0.02490067, 0.04143429,\n",
       "        0.0450387 , 0.03273392, 0.02899003, 0.02650571, 0.03732538]),\n",
       " 'test_accuracy': array([0.78884683, 0.79604838, 0.79161666, 0.78561536, 0.78903148,\n",
       "        0.79798726, 0.79447881, 0.79115502, 0.78413812, 0.79484812]),\n",
       " 'test_precision_macro': array([0.78395036, 0.79163292, 0.78644888, 0.78031415, 0.78350643,\n",
       "        0.79370959, 0.78946526, 0.78603009, 0.77983677, 0.79115298]),\n",
       " 'test_recall_macro': array([0.7730812 , 0.78071983, 0.77677609, 0.77002572, 0.77438491,\n",
       "        0.78274895, 0.77983537, 0.77617035, 0.76678896, 0.7783325 ]),\n",
       " 'test_f1_macro': array([0.7770842 , 0.7847831 , 0.78045638, 0.77384429, 0.77788812,\n",
       "        0.78683858, 0.78351925, 0.77990234, 0.77131245, 0.78289906])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluación del mejor modelo\n",
    "naive_bayes_best_1 = MultinomialNB(alpha=0.01)\n",
    "naive_bayes_best_1.fit(training_data_tf_norm,y_train)\n",
    "scores = cross_validate(naive_bayes_best_1, cross_validation_data_tf_norm, cross_validation_y_tf, cv=10, scoring=('accuracy','precision_macro','recall_macro','f1_macro'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.86      0.83     16135\n",
      "         1.0       0.77      0.68      0.72     10943\n",
      "\n",
      "    accuracy                           0.79     27078\n",
      "   macro avg       0.78      0.77      0.78     27078\n",
      "weighted avg       0.79      0.79      0.79     27078\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13879, 2256, 3493, 7450)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = naive_bayes_best_1.predict(testing_data_tf_norm)  \n",
    "report = classification_report(y_test ,pred)  \n",
    "print(report)\n",
    "#En terminos de TP, FP, TN, FN\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "(tn,fp,fn,tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.09742689, 0.06851053, 0.06752038, 0.05949926, 0.04767346,\n",
       "        0.0578413 , 0.05810523, 0.05245495, 0.06115651, 0.05664873]),\n",
       " 'score_time': array([0.04839563, 0.04326344, 0.03900027, 0.03308201, 0.02533412,\n",
       "        0.02209115, 0.02501178, 0.02813196, 0.02402949, 0.0299685 ]),\n",
       " 'test_accuracy': array([0.7850614 , 0.78810821, 0.78903148, 0.77979873, 0.78746191,\n",
       "        0.79300157, 0.79106269, 0.78561536, 0.780722  , 0.79614071]),\n",
       " 'test_precision_macro': array([0.78570529, 0.78930827, 0.78842753, 0.77921725, 0.78722441,\n",
       "        0.79459859, 0.79143619, 0.78565969, 0.78191154, 0.79886662]),\n",
       " 'test_recall_macro': array([0.76242002, 0.76548471, 0.76818317, 0.75751899, 0.76602934,\n",
       "        0.77075954, 0.76960083, 0.76364263, 0.75698964, 0.77343515]),\n",
       " 'test_f1_macro': array([0.7689634 , 0.77217867, 0.77428983, 0.7636883 , 0.77227541,\n",
       "        0.7775681 , 0.77603032, 0.76998995, 0.76366186, 0.78057223])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento del algoritmo de naive bayes y cross validation para la\n",
    "# representación vectoria de tfidf\n",
    "\n",
    "naive_bayes_2 = MultinomialNB()\n",
    "naive_bayes_2.fit(training_data_tfidf_norm,y_train)\n",
    "scores = cross_validate(naive_bayes, cross_validation_data_tfidf_norm, cross_validation_y_tfidf, cv=10, scoring=('accuracy','precision_macro','recall_macro','f1_macro'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Accuracy Through Grid Search : 0.7903894500080397\n",
      "best parameter :  {'alpha': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Busqueda de hiperaparametros variando el alpha\n",
    "\n",
    "params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0, ],}\n",
    "\n",
    "multinomial_nb_grid = GridSearchCV(MultinomialNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
    "multinomial_nb_grid.fit(training_data_tfidf_norm,y_train)\n",
    "\n",
    "print(f'Best Accuracy Through Grid Search : {multinomial_nb_grid.best_score_}')\n",
    "print('best parameter : ', multinomial_nb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.04700851, 0.04215217, 0.04918575, 0.04835224, 0.05004168,\n",
       "        0.05710936, 0.06244302, 0.07300353, 0.08160329, 0.04799962]),\n",
       " 'score_time': array([0.02149653, 0.02297139, 0.01748347, 0.02461386, 0.02482271,\n",
       "        0.0239594 , 0.02918005, 0.0509398 , 0.02957106, 0.02845502]),\n",
       " 'test_accuracy': array([0.78884683, 0.79604838, 0.79161666, 0.78561536, 0.78903148,\n",
       "        0.79798726, 0.79447881, 0.79115502, 0.78413812, 0.79484812]),\n",
       " 'test_precision_macro': array([0.78395036, 0.79163292, 0.78644888, 0.78031415, 0.78350643,\n",
       "        0.79370959, 0.78946526, 0.78603009, 0.77983677, 0.79115298]),\n",
       " 'test_recall_macro': array([0.7730812 , 0.78071983, 0.77677609, 0.77002572, 0.77438491,\n",
       "        0.78274895, 0.77983537, 0.77617035, 0.76678896, 0.7783325 ]),\n",
       " 'test_f1_macro': array([0.7770842 , 0.7847831 , 0.78045638, 0.77384429, 0.77788812,\n",
       "        0.78683858, 0.78351925, 0.77990234, 0.77131245, 0.78289906])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluación del mejor modelo\n",
    "naive_bayes_best_2 = MultinomialNB(alpha=0.01)\n",
    "naive_bayes_best_2.fit(training_data_tfidf_norm,y_train)\n",
    "scores = cross_validate(naive_bayes_best_2, cross_validation_data_tfidf_norm, cross_validation_y_tfidf, cv=10, scoring=('accuracy','precision_macro','recall_macro','f1_macro'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.7855454612600635\n",
      "Macro-Precision score:  0.779093541086759\n",
      "Macro-Recall score:  0.7713980982843713\n",
      "Macro-F1 score:  0.774460716018839\n",
      "Micro-Precision score:  0.7855454612600635\n",
      "Micro-Recall score:  0.7855454612600635\n",
      "Micro-F1 score:  0.7855454612600634\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo\n",
    "\n",
    "predictions = naive_bayes_best_2.predict(testing_data_tfidf_norm)\n",
    "print('Accuracy score: ', format(accuracy_score(y_test, predictions)))\n",
    "print('Macro-Precision score: ', format(precision_score(y_test, predictions,average='macro')))\n",
    "print('Macro-Recall score: ', format(recall_score(y_test, predictions,average='macro')))\n",
    "print('Macro-F1 score: ', format(f1_score(y_test, predictions,average='macro')))\n",
    "\n",
    "print('Micro-Precision score: ', format(precision_score(y_test, predictions,average='micro')))\n",
    "print('Micro-Recall score: ', format(recall_score(y_test, predictions,average='micro')))\n",
    "print('Micro-F1 score: ', format(f1_score(y_test, predictions,average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.85      0.82     16135\n",
      "         1.0       0.75      0.70      0.72     10943\n",
      "\n",
      "    accuracy                           0.79     27078\n",
      "   macro avg       0.78      0.77      0.77     27078\n",
      "weighted avg       0.78      0.79      0.78     27078\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13637, 2498, 3309, 7634)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = naive_bayes_best_2.predict(testing_data_tfidf_norm)  \n",
    "report = classification_report(y_test ,pred)  \n",
    "print(report)\n",
    "#En terminos de TP, FP, TN, FN\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "(tn,fp,fn,tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(naive_bayes_best_2, open('salida/naive_bayes.model', 'wb'))\n",
    "pickle.dump(count_vector_tfidf, open('salida/naive_bayes.vector', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(text):\n",
    "    count_vector = pickle.load(open('salida/naive_bayes.vector', 'rb'))\n",
    "    naive_bayes = pickle.load(open('salida/naive_bayes.model', 'rb'))\n",
    "    data = count_vector.transform([text])\n",
    "    norm = Normalizer().fit_transform(data)\n",
    "    return 'Hate Speech' if naive_bayes.predict(norm)[0] == 1 else 'Not Hate Speech'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hate Speech'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor('everybody will die sooner or later')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7a79e9e75522a046d95171e373010a5dca4ce6e8605d007854b2218f1d88052"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
