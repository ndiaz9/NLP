{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importación de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, datasets, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importación del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration ucberkeley-dlab--measuring-hate-speech-1d47093687320b66\n",
      "Reusing dataset parquet (C:\\Users\\camilo\\.cache\\huggingface\\datasets\\ucberkeley-dlab___parquet\\ucberkeley-dlab--measuring-hate-speech-1d47093687320b66\\0.0.0\\7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n",
      "100%|██████████| 1/1 [00:00<00:00, 52.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>platform</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>respect</th>\n",
       "      <th>insult</th>\n",
       "      <th>humiliate</th>\n",
       "      <th>status</th>\n",
       "      <th>dehumanize</th>\n",
       "      <th>violence</th>\n",
       "      <th>...</th>\n",
       "      <th>annotator_religion_hindu</th>\n",
       "      <th>annotator_religion_jewish</th>\n",
       "      <th>annotator_religion_mormon</th>\n",
       "      <th>annotator_religion_muslim</th>\n",
       "      <th>annotator_religion_nothing</th>\n",
       "      <th>annotator_religion_other</th>\n",
       "      <th>annotator_sexuality_bisexual</th>\n",
       "      <th>annotator_sexuality_gay</th>\n",
       "      <th>annotator_sexuality_straight</th>\n",
       "      <th>annotator_sexuality_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47777</td>\n",
       "      <td>10873</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39773</td>\n",
       "      <td>2790</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47101</td>\n",
       "      <td>3379</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43625</td>\n",
       "      <td>7365</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12538</td>\n",
       "      <td>488</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_id  annotator_id  platform  sentiment  respect  insult  humiliate  \\\n",
       "0       47777         10873         3        0.0      0.0     0.0        0.0   \n",
       "1       39773          2790         2        0.0      0.0     0.0        0.0   \n",
       "2       47101          3379         3        4.0      4.0     4.0        4.0   \n",
       "3       43625          7365         3        2.0      3.0     2.0        1.0   \n",
       "4       12538           488         0        4.0      4.0     4.0        4.0   \n",
       "\n",
       "   status  dehumanize  violence  ...  annotator_religion_hindu  \\\n",
       "0     2.0         0.0       0.0  ...                     False   \n",
       "1     2.0         0.0       0.0  ...                     False   \n",
       "2     4.0         4.0       0.0  ...                     False   \n",
       "3     2.0         0.0       0.0  ...                     False   \n",
       "4     4.0         4.0       4.0  ...                     False   \n",
       "\n",
       "   annotator_religion_jewish  annotator_religion_mormon  \\\n",
       "0                      False                      False   \n",
       "1                      False                      False   \n",
       "2                      False                      False   \n",
       "3                      False                      False   \n",
       "4                      False                      False   \n",
       "\n",
       "   annotator_religion_muslim annotator_religion_nothing  \\\n",
       "0                      False                      False   \n",
       "1                      False                      False   \n",
       "2                      False                       True   \n",
       "3                      False                      False   \n",
       "4                      False                      False   \n",
       "\n",
       "   annotator_religion_other  annotator_sexuality_bisexual  \\\n",
       "0                     False                         False   \n",
       "1                     False                         False   \n",
       "2                     False                         False   \n",
       "3                     False                         False   \n",
       "4                     False                         False   \n",
       "\n",
       "   annotator_sexuality_gay  annotator_sexuality_straight  \\\n",
       "0                    False                          True   \n",
       "1                    False                          True   \n",
       "2                    False                          True   \n",
       "3                    False                          True   \n",
       "4                    False                          True   \n",
       "\n",
       "   annotator_sexuality_other  \n",
       "0                      False  \n",
       "1                      False  \n",
       "2                      False  \n",
       "3                      False  \n",
       "4                      False  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('ucberkeley-dlab/measuring-hate-speech', 'binary')   \n",
    "df = dataset['train'].to_pandas()\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### funciones de procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtag(token: str) -> str:\n",
    "    \"\"\"\n",
    "    Split hashtags by uppercases\n",
    "    \"\"\"\n",
    "    i, end = len(token) - 2, len(token) + 1\n",
    "    response = ''\n",
    "    while i >= 1:\n",
    "        word = token[i:end]\n",
    "        if word[0].isupper():\n",
    "            response = token[i:end] + ' ' + response\n",
    "            end = i\n",
    "        i -= 1\n",
    "    else:\n",
    "        if end-1 > 0:\n",
    "            response = token[1:end] + ' ' + response\n",
    "    return(response.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(document: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace all abbreviations with their corresponding expansion\n",
    "    \"\"\"\n",
    "    document = re.sub(r\"'cause\", \"because\", document)\n",
    "    document = re.sub(r\"o'clock\", \"of the clock\", document)\n",
    "    document = re.sub(r\"won\\'t\", \"will not\", document)\n",
    "    document = re.sub(r\"can\\'t\", \"can not\", document)\n",
    "    document = re.sub(r\"n\\'t\", \" not\", document)\n",
    "    document = re.sub(r\"\\'re\", \" are\", document)\n",
    "    document = re.sub(r\"\\'s\", \" is\", document)\n",
    "    document = re.sub(r\"\\'d\", \" would\", document)\n",
    "    document = re.sub(r\"\\'ll\", \" will\", document)\n",
    "    document = re.sub(r\"\\'t\", \" not\", document)\n",
    "    document = re.sub(r\"\\'ve\", \" have\", document)\n",
    "    document = re.sub(r\"\\'m\", \" am\", document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document: str) -> list:\n",
    "    \"\"\"\n",
    "    expand contractions\n",
    "    replace any @user type with 'user' token\n",
    "    split hashtags into words by upper case\n",
    "    text to lower case\n",
    "    remove new line char\n",
    "    remove non latin chars\n",
    "    remove extra blank spaces\n",
    "    \"\"\"\n",
    "    document = expand_contractions(document)\n",
    "    document = re.sub('@[a-zA-Z0-9_-]{0,150}', 'user', document)\n",
    "    hashtags = re.findall('#[a-zA-Z0-9_-]{0,150}', document)\n",
    "    for hashtag in hashtags:\n",
    "        split = split_hashtag(hashtag)\n",
    "        document = re.sub(hashtag, split, document)\n",
    "    document = document.lower()\n",
    "    document = re.sub('\\n', ' ', document)\n",
    "    document = re.sub('[^a-zA-Z]|[0-9]', ' ', document)\n",
    "    document = re.sub('\\s+', ' ', document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicar preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135388, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "data['text'] = df.text\n",
    "data['processed'] = df.text.apply(preprocessing)\n",
    "data['hatespeech'] = df.hatespeech.apply(lambda x: math.ceil(x/2))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>text</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24553</th>\n",
       "      <td>0</td>\n",
       "      <td>@TabindaSulahri1 I'm very sorry we are ashamed...</td>\n",
       "      <td>user i am very sorry we are ashamed muslim we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111766</th>\n",
       "      <td>1</td>\n",
       "      <td>There are not any jew signatures on our Declar...</td>\n",
       "      <td>there are not any jew signatures on our declar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54786</th>\n",
       "      <td>0</td>\n",
       "      <td>swedish cock is my favourite kind of cock ! hehe</td>\n",
       "      <td>swedish cock is my favourite kind of cock hehe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76341</th>\n",
       "      <td>0</td>\n",
       "      <td>David Rook what about all those visa dodgers? ...</td>\n",
       "      <td>david rook what about all those visa dodgers w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24671</th>\n",
       "      <td>0</td>\n",
       "      <td>If white genocide means this guy won't exist, ...</td>\n",
       "      <td>if white genocide means this guy will not exis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89623</th>\n",
       "      <td>0</td>\n",
       "      <td>Every one worships their god on their festival...</td>\n",
       "      <td>every one worships their god on their festival...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104981</th>\n",
       "      <td>1</td>\n",
       "      <td>Fuck you fuck you you stupid cheating bitch go...</td>\n",
       "      <td>fuck you fuck you you stupid cheating bitch go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55823</th>\n",
       "      <td>0</td>\n",
       "      <td>Good! Deport all of them.</td>\n",
       "      <td>good deport all of them</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120442</th>\n",
       "      <td>0</td>\n",
       "      <td>illegal immigrants =/= legal immigrants. in ev...</td>\n",
       "      <td>illegal immigrants legal immigrants in every o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90141</th>\n",
       "      <td>0</td>\n",
       "      <td>America is the trashiest country in the world,...</td>\n",
       "      <td>america is the trashiest country in the world ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        hatespeech                                               text  \\\n",
       "24553            0  @TabindaSulahri1 I'm very sorry we are ashamed...   \n",
       "111766           1  There are not any jew signatures on our Declar...   \n",
       "54786            0   swedish cock is my favourite kind of cock ! hehe   \n",
       "76341            0  David Rook what about all those visa dodgers? ...   \n",
       "24671            0  If white genocide means this guy won't exist, ...   \n",
       "89623            0  Every one worships their god on their festival...   \n",
       "104981           1  Fuck you fuck you you stupid cheating bitch go...   \n",
       "55823            0                          Good! Deport all of them.   \n",
       "120442           0  illegal immigrants =/= legal immigrants. in ev...   \n",
       "90141            0  America is the trashiest country in the world,...   \n",
       "\n",
       "                                                processed  \n",
       "24553   user i am very sorry we are ashamed muslim we ...  \n",
       "111766  there are not any jew signatures on our declar...  \n",
       "54786      swedish cock is my favourite kind of cock hehe  \n",
       "76341   david rook what about all those visa dodgers w...  \n",
       "24671   if white genocide means this guy will not exis...  \n",
       "89623   every one worships their god on their festival...  \n",
       "104981  fuck you fuck you you stupid cheating bitch go...  \n",
       "55823                            good deport all of them   \n",
       "120442  illegal immigrants legal immigrants in every o...  \n",
       "90141   america is the trashiest country in the world ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)[['hatespeech','text','processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the total set: 135388\n",
      "Number of rows in the training set: 94771\n",
      "Number of rows in the validation set: 13539\n",
      "Number of rows in the test set: 27078\n"
     ]
    }
   ],
   "source": [
    "y_sample = data['hatespeech'].astype(float)\n",
    "X_sample = data['processed'].astype(str)\n",
    "\n",
    "X_train_validation, X_test, y_train_validation, y_test = train_test_split(X_sample.values, \n",
    "                                                    y_sample.values, \n",
    "                                                    random_state=42,test_size=0.2)\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train_validation, \n",
    "                                                    y_train_validation, \n",
    "                                                    random_state=42,test_size=0.125)\n",
    "\n",
    "print('Number of rows in the total set: {}'.format(X_sample.shape[0]))\n",
    "print('Number of rows in the training set: {}'.format(X_train.shape[0]))\n",
    "print('Number of rows in the validation set: {}'.format(X_validation.shape[0]))\n",
    "print('Number of rows in the test set: {}'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genera la representación tf del vocabulario\n",
    "count_vector = CountVectorizer()\n",
    "training_data_tf = count_vector.fit_transform(X_train)\n",
    "validation_data_tf = count_vector.transform(X_validation)\n",
    "testing_data_tf = count_vector.transform(X_test)\n",
    "\n",
    "training_validation_x_tf = np.concatenate((X_train,X_validation))\n",
    "cross_validation_x_tf = count_vector.transform(training_validation_x_tf)\n",
    "cross_validation_y_tf = np.concatenate((y_train,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se normaliza la representación vectorial\n",
    "training_data_tf_norm = Normalizer().fit_transform(training_data_tf)\n",
    "validation_data_tf_norm = Normalizer().fit_transform(validation_data_tf)\n",
    "testing_data_tf_norm = Normalizer().fit_transform(testing_data_tf)\n",
    "cross_validation_data_tf_norm = Normalizer().fit_transform(cross_validation_x_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genera la representación tfidf del vocabulario\n",
    "count_vector_tfidf = TfidfVectorizer()\n",
    "training_data_tfidf = count_vector_tfidf.fit_transform(X_train)\n",
    "validation_data_tfidf = count_vector_tfidf.transform(X_validation)\n",
    "testing_data_tfidf = count_vector_tfidf.transform(X_test)\n",
    "\n",
    "training_validation_x_tfidf = np.concatenate((X_train,X_validation))\n",
    "cross_validation_x_tfidf = count_vector.transform(training_validation_x_tfidf)\n",
    "cross_validation_y_tfidf = np.concatenate((y_train,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se normaliza la representación vectorial\n",
    "training_data_tfidf_norm = Normalizer().fit_transform(training_data_tfidf) \n",
    "validation_data_tfidf_norm = Normalizer().fit_transform(validation_data_tfidf)\n",
    "testing_data_tfidf_norm = Normalizer().fit_transform(testing_data_tfidf)\n",
    "cross_validation_data_tfidf_norm = Normalizer().fit_transform(cross_validation_x_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.13962603, 0.12067723, 0.17154098, 0.18650246, 0.17054296,\n",
       "        0.19148898, 0.11468911, 0.09873581, 0.13164902, 0.10870838]),\n",
       " 'score_time': array([0.09873724, 0.05984712, 0.09674096, 0.03689909, 0.0678196 ,\n",
       "        0.05385423, 0.04189706, 0.04587746, 0.05485439, 0.04188633]),\n",
       " 'test_accuracy': array([0.7850614 , 0.78810821, 0.78903148, 0.77979873, 0.78746191,\n",
       "        0.79300157, 0.79106269, 0.78561536, 0.780722  , 0.79614071]),\n",
       " 'test_precision': array([0.78760815, 0.79284316, 0.7865505 , 0.77747025, 0.78649835,\n",
       "        0.7993311 , 0.79257221, 0.78579326, 0.78532993, 0.8068054 ]),\n",
       " 'test_recall': array([0.64297106, 0.64601367, 0.65808656, 0.63986333, 0.65284738,\n",
       "        0.65330296, 0.65626424, 0.6476082 , 0.63166287, 0.65353075]),\n",
       " 'test_f1': array([0.70797792, 0.71193674, 0.71660672, 0.70198675, 0.71346776,\n",
       "        0.71897719, 0.71800623, 0.71003996, 0.70016412, 0.72212434])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento del algoritmo de naive bayes y cross validation para la\n",
    "# representación vectoria de tf\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data_tf_norm,y_train)\n",
    "scores = cross_validate(naive_bayes, cross_validation_data_tf_norm, cross_validation_y_tf, cv=10, scoring=('accuracy','precision','recall','f1'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Accuracy Through Grid Search : 0.7898407626266004\n",
      "best parameter :  {'alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Busqueda de hiperaparametros variando el alpha\n",
    "\n",
    "params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0],}\n",
    "\n",
    "multinomial_nb_grid = GridSearchCV(MultinomialNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
    "multinomial_nb_grid.fit(training_data_tf_norm,y_train)\n",
    "\n",
    "print(f'Best Accuracy Through Grid Search : {multinomial_nb_grid.best_score_}')\n",
    "print('best parameter : ', multinomial_nb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.09374809, 0.07380414, 0.08377647, 0.2014606 , 0.13364649,\n",
       "        0.10571742, 0.08776617, 0.07879496, 0.09175491, 0.07579541]),\n",
       " 'score_time': array([0.03391027, 0.02193999, 0.06383061, 0.07879257, 0.04088688,\n",
       "        0.04388213, 0.04089832, 0.02592492, 0.04188848, 0.03889656]),\n",
       " 'test_accuracy': array([0.78949312, 0.79503278, 0.7924476 , 0.78570769, 0.78838519,\n",
       "        0.79549441, 0.79494045, 0.79004709, 0.78487674, 0.79549441]),\n",
       " 'test_precision': array([0.77059276, 0.77749361, 0.77224199, 0.76478116, 0.76570415,\n",
       "        0.78210117, 0.77574371, 0.76934827, 0.76963351, 0.78283485]),\n",
       " 'test_recall': array([0.68421053, 0.69248292, 0.69202733, 0.68063781, 0.68861048,\n",
       "        0.68678815, 0.69498861, 0.68838269, 0.66970387, 0.6856492 ]),\n",
       " 'test_f1': array([0.72483707, 0.73253012, 0.72993753, 0.72026034, 0.72511394,\n",
       "        0.73135233, 0.7331491 , 0.72661698, 0.71619976, 0.73102611])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluación del mejor modelo\n",
    "naive_bayes_best_1 = MultinomialNB(alpha=0.1)\n",
    "naive_bayes_best_1.fit(training_data_tf_norm,y_train)\n",
    "scores = cross_validate(naive_bayes_best_1, cross_validation_data_tf_norm, cross_validation_y_tf, cv=10, scoring=('accuracy','precision','recall','f1'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.7887214713051185\n",
      "Precision score:  0.7732886749005652\n",
      "Recall score:  0.6751347893630631\n",
      "F1 score:  0.7208859833146314\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = naive_bayes_best_1.predict(testing_data_tf_norm)\n",
    "print('Accuracy score: ', format(accuracy_score(y_test, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.87      0.83     16135\n",
      "         1.0       0.77      0.68      0.72     10943\n",
      "\n",
      "    accuracy                           0.79     27078\n",
      "   macro avg       0.79      0.77      0.78     27078\n",
      "weighted avg       0.79      0.79      0.79     27078\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13969, 2166, 3555, 7388)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = naive_bayes_best_1.predict(testing_data_tf_norm)  \n",
    "report = classification_report(y_test ,pred)  \n",
    "print(report)\n",
    "#En terminos de TP, FP, TN, FN\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "(tn,fp,fn,tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.0817678 , 0.06582189, 0.08477306, 0.12765622, 0.10172582,\n",
       "        0.08676553, 0.08676696, 0.08377504, 0.08777213, 0.1057179 ]),\n",
       " 'score_time': array([0.03191566, 0.03390908, 0.03191519, 0.04886889, 0.03490543,\n",
       "        0.02891922, 0.04488683, 0.03391671, 0.04287767, 0.03490782]),\n",
       " 'test_accuracy': array([0.7850614 , 0.78810821, 0.78903148, 0.77979873, 0.78746191,\n",
       "        0.79300157, 0.79106269, 0.78561536, 0.780722  , 0.79614071]),\n",
       " 'test_precision': array([0.78760815, 0.79284316, 0.7865505 , 0.77747025, 0.78649835,\n",
       "        0.7993311 , 0.79257221, 0.78579326, 0.78532993, 0.8068054 ]),\n",
       " 'test_recall': array([0.64297106, 0.64601367, 0.65808656, 0.63986333, 0.65284738,\n",
       "        0.65330296, 0.65626424, 0.6476082 , 0.63166287, 0.65353075]),\n",
       " 'test_f1': array([0.70797792, 0.71193674, 0.71660672, 0.70198675, 0.71346776,\n",
       "        0.71897719, 0.71800623, 0.71003996, 0.70016412, 0.72212434])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento del algoritmo de naive bayes y cross validation para la\n",
    "# representación vectoria de tfidf\n",
    "\n",
    "naive_bayes_2 = MultinomialNB()\n",
    "naive_bayes_2.fit(training_data_tfidf_norm,y_train)\n",
    "scores = cross_validate(naive_bayes, cross_validation_data_tfidf_norm, cross_validation_y_tfidf, cv=10, scoring=('accuracy','precision','recall','f1'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Accuracy Through Grid Search : 0.7903894500080397\n",
      "best parameter :  {'alpha': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Busqueda de hiperaparametros variando el alpha\n",
    "\n",
    "params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0, ],}\n",
    "\n",
    "multinomial_nb_grid = GridSearchCV(MultinomialNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
    "multinomial_nb_grid.fit(training_data_tfidf_norm,y_train)\n",
    "\n",
    "print(f'Best Accuracy Through Grid Search : {multinomial_nb_grid.best_score_}')\n",
    "print('best parameter : ', multinomial_nb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.10172892, 0.08975339, 0.08776498, 0.11170077, 0.10072923,\n",
       "        0.17256117, 0.18251252, 0.09674168, 0.16954732, 0.08876181]),\n",
       " 'score_time': array([0.0369029 , 0.04587817, 0.03590345, 0.03789806, 0.03989291,\n",
       "        0.09472346, 0.05285811, 0.03789902, 0.05485177, 0.03789806]),\n",
       " 'test_accuracy': array([0.7877389 , 0.79180131, 0.79152433, 0.78303019, 0.78635398,\n",
       "        0.79586372, 0.79383252, 0.78727726, 0.78210692, 0.79567907]),\n",
       " 'test_precision': array([0.78046162, 0.78626978, 0.78096995, 0.77142097, 0.77258403,\n",
       "        0.79311273, 0.78344284, 0.77519789, 0.77701965, 0.79506641]),\n",
       " 'test_recall': array([0.6625655 , 0.66788155, 0.67494305, 0.66036446, 0.67015945,\n",
       "        0.6715262 , 0.67904328, 0.66924829, 0.64851936, 0.66810934]),\n",
       " 'test_f1': array([0.71669747, 0.72225644, 0.7240958 , 0.71158567, 0.71773603,\n",
       "        0.72727273, 0.72751678, 0.71833741, 0.7069779 , 0.72607996])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluación del mejor modelo\n",
    "naive_bayes_best_2 = MultinomialNB(alpha=0.5)\n",
    "naive_bayes_best_2.fit(training_data_tfidf_norm,y_train)\n",
    "scores = cross_validate(naive_bayes_best_2, cross_validation_data_tfidf_norm, cross_validation_y_tfidf, cv=10, scoring=('accuracy','precision','recall','f1'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.7881305857153409\n",
      "Precision score:  0.7732521520050388\n",
      "Recall score:  0.6731243717444942\n",
      "F1 score:  0.719722507206019\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo\n",
    "\n",
    "predictions = naive_bayes_best_2.predict(testing_data_tfidf_norm)\n",
    "print('Accuracy score: ', format(accuracy_score(y_test, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.87      0.83     16135\n",
      "         1.0       0.77      0.67      0.72     10943\n",
      "\n",
      "    accuracy                           0.79     27078\n",
      "   macro avg       0.78      0.77      0.77     27078\n",
      "weighted avg       0.79      0.79      0.79     27078\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13975, 2160, 3577, 7366)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = naive_bayes_best_2.predict(testing_data_tfidf_norm)  \n",
    "report = classification_report(y_test ,pred)  \n",
    "print(report)\n",
    "#En terminos de TP, FP, TN, FN\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "(tn,fp,fn,tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(naive_bayes_best_1, open('salida/naive_bayes.model', 'wb'))\n",
    "pickle.dump(count_vector, open('salida/naive_bayes.vector', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(text):\n",
    "    count_vector = pickle.load(open('salida/naive_bayes.vector', 'rb'))\n",
    "    naive_bayes = pickle.load(open('salida/naive_bayes.model', 'rb'))\n",
    "    data = count_vector.transform([text])\n",
    "    norm = Normalizer().fit_transform(data)\n",
    "    return 'Hate Speech' if naive_bayes.predict(norm)[0] == 1 else 'Not Hate Speech'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hate Speech'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor('everybody will die sooner or later')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7a79e9e75522a046d95171e373010a5dca4ce6e8605d007854b2218f1d88052"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
