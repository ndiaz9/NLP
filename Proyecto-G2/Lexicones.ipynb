{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3d4585-f7a6-4574-ac7d-94ba5c6e5c54",
   "metadata": {},
   "source": [
    "# Lexicones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042f0121-01cf-42fd-a7d1-0e84a22bbf9f",
   "metadata": {},
   "source": [
    "Se realizaran entrenamiento agregando caracteristicas extraidas de diferentes lexicones disponibles en la web junto con un lexicon creado a partir del set de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5092fbdb-c07c-4094-a7a2-4bf84307983b",
   "metadata": {},
   "source": [
    "## SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708f819e-4475-4bb6-a192-d167b28fe0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38154b55-ca70-4ebc-b66d-a96b895f96c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download('wordnet')\n",
    "download('sentiwordnet')\n",
    "download('omw-1.4')\n",
    "download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e292e7c-42af-4bcd-aba8-62bc53a1c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank tags to Wordnet tags\n",
    "    \"\"\"\n",
    "    first_letter = tag[0]\n",
    "    answers = {'J': wn.ADJ, 'N': wn.NOUN, 'R': wn.ADV, 'V': wn.VERB}\n",
    "    return answers.get(first_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d96689f9-720f-4036-8116-492aa24a0739",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_sentiment(word, tag):\n",
    "    \"\"\" \n",
    "    returns list of pos neg and objective score. But returns empty list if not present in senti wordnet. \n",
    "    \"\"\"\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    valid_wn_tags = (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB)\n",
    "    if wn_tag not in valid_wn_tags: \n",
    "        return (0.0, 0.0, 1.0)\n",
    "\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "    if not lemma: \n",
    "        return (0.0, 0.0, 1.0)\n",
    "\n",
    "    synsets = wn.synsets(word, pos=wn_tag)\n",
    "    if not synsets: \n",
    "        return (0.0, 0.0, 1.0)\n",
    "\n",
    "    # Take the first sense, the most common\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "    return (swn_synset.pos_score(), swn_synset.neg_score(), swn_synset.obj_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cfbfa58-c554-427d-9abb-739f84ac5773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexicon_sentiwordnet(document: str) -> list:\n",
    "    \"\"\"\n",
    "    return the lexicon features between others:\n",
    "    - sum of positive, negative and objetive scores\n",
    "    - mean of the sum of positive, negative and objetive scores\n",
    "    - cant of words with positive, negative and objetive scores over 0.5\n",
    "    \"\"\"\n",
    "    neg_scores, pos_scores, obj_scores= [], [], []\n",
    "    words = word_tokenize(document)\n",
    "    cant_words = len(words)\n",
    "    pos_words = pos_tag(words)\n",
    "    for word, tag in pos_words:\n",
    "        scores = get_sentiment(word, tag)\n",
    "        pos_scores.append(scores[0])\n",
    "        neg_scores.append(scores[1])\n",
    "        obj_scores.append(scores[2])\n",
    "    pos_score, neg_score, obj_score = sum(pos_scores), sum(neg_scores), sum(obj_scores)\n",
    "    pond_pos, pond_neg, pond_obj = pos_score/cant_words, neg_score/cant_words, obj_score/cant_words\n",
    "    cant_pos, cant_neg, cant_obj = len([item for item in pos_scores if item >= 0.5]), len([item for item in neg_scores if item >= 0.5]), len([item for item in obj_scores if item >= 0.5])\n",
    "    most_important = 1 if neg_score > pos_score else 0\n",
    "    return [\n",
    "        pos_score, neg_score, obj_score, \n",
    "        pond_pos, pond_neg, pond_obj, \n",
    "        cant_pos/cant_words, cant_neg/cant_words, cant_obj/cant_words, \n",
    "        cant_pos, cant_neg, cant_obj,\n",
    "        most_important,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7cc149c-2e41-4b60-b69c-36a9fd308253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.125, 1.875, 0.3333333333333333, 0.041666666666666664, 0.625, 0.3333333333333333, 0.0, 0.6666666666666666, 1, 0, 2, 0]\n",
      "[0.25, 1.0, 1.75, 0.08333333333333333, 0.3333333333333333, 0.5833333333333334, 0.0, 0.3333333333333333, 0.6666666666666666, 0, 1, 2, 1]\n",
      "[0.5, 0.0, 4.5, 0.1, 0.0, 0.9, 0.2, 0.0, 1.0, 1, 0, 5, 0]\n",
      "[0.0, 0.75, 4.25, 0.0, 0.15, 0.85, 0.0, 0.2, 0.8, 0, 1, 4, 1]\n",
      "[0.625, 1.125, 7.25, 0.06944444444444445, 0.125, 0.8055555555555556, 0.0, 0.1111111111111111, 0.8888888888888888, 0, 1, 8, 1]\n"
     ]
    }
   ],
   "source": [
    "print(lexicon_sentiwordnet('this is good'))\n",
    "print(lexicon_sentiwordnet('this is awful'))\n",
    "print(lexicon_sentiwordnet('i love you so much'))\n",
    "print(lexicon_sentiwordnet('i hate you so much'))\n",
    "print(lexicon_sentiwordnet('Learn the fucking language you fucking useless immigrant.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0e1f37-3cda-452a-97af-a2c0f17f8bd0",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa555105-02eb-401f-ac0b-412e689dcd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, re, math, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing as sk_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9996dc31-cb8d-498c-9843-c166b2ce6d41",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "390b83fd-b840-44b9-b874-951c4c8001c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration ucberkeley-dlab--measuring-hate-speech-1d47093687320b66\n",
      "Reusing dataset parquet (C:\\Users\\Usuario\\.cache\\huggingface\\datasets\\ucberkeley-dlab___parquet\\ucberkeley-dlab--measuring-hate-speech-1d47093687320b66\\0.0.0\\7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31858cf07bfb4e0abac8b0c39495b382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>platform</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>respect</th>\n",
       "      <th>insult</th>\n",
       "      <th>humiliate</th>\n",
       "      <th>status</th>\n",
       "      <th>dehumanize</th>\n",
       "      <th>violence</th>\n",
       "      <th>...</th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>hate_speech_score</th>\n",
       "      <th>infitms</th>\n",
       "      <th>outfitms</th>\n",
       "      <th>annotator_severity</th>\n",
       "      <th>std_err</th>\n",
       "      <th>annotator_infitms</th>\n",
       "      <th>annotator_outfitms</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>annotator_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>135388.000000</td>\n",
       "      <td>135388.000000</td>\n",
       "      <td>135388.000000</td>\n",
       "      <td>135388.000000</td>\n",
       "      <td>135388.000000</td>\n",
       "      <td>135388.000000</td>\n",
       "      <td>135388.000000</td>\n",
       "      <td>135388.000000</td>\n",
       "      <td>135388.000000</td>\n",
       "      <td>135388.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>135388.000000</td>\n",
       "      <td>135388.000000</td>\n",
       "      <td>135388.000000</td>\n",
       "      <td>135388.000000</td>\n",
       "      <td>135388.000000</td>\n",
       "      <td>135388.000000</td>\n",
       "      <td>135388.000000</td>\n",
       "      <td>135388.000000</td>\n",
       "      <td>135388.000000</td>\n",
       "      <td>135388.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23528.597283</td>\n",
       "      <td>5566.552523</td>\n",
       "      <td>1.281199</td>\n",
       "      <td>2.954280</td>\n",
       "      <td>2.828751</td>\n",
       "      <td>2.563152</td>\n",
       "      <td>2.278333</td>\n",
       "      <td>2.698784</td>\n",
       "      <td>1.845651</td>\n",
       "      <td>1.051666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.744527</td>\n",
       "      <td>-0.567465</td>\n",
       "      <td>1.034367</td>\n",
       "      <td>1.001085</td>\n",
       "      <td>-0.018735</td>\n",
       "      <td>0.300572</td>\n",
       "      <td>1.007152</td>\n",
       "      <td>1.011709</td>\n",
       "      <td>0.014535</td>\n",
       "      <td>37.912799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12386.724630</td>\n",
       "      <td>3230.864231</td>\n",
       "      <td>1.023495</td>\n",
       "      <td>1.231662</td>\n",
       "      <td>1.309693</td>\n",
       "      <td>1.389984</td>\n",
       "      <td>1.370983</td>\n",
       "      <td>0.898430</td>\n",
       "      <td>1.402575</td>\n",
       "      <td>1.345656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.932267</td>\n",
       "      <td>2.380312</td>\n",
       "      <td>0.496865</td>\n",
       "      <td>0.791993</td>\n",
       "      <td>0.487189</td>\n",
       "      <td>0.236407</td>\n",
       "      <td>0.269875</td>\n",
       "      <td>0.676072</td>\n",
       "      <td>0.613035</td>\n",
       "      <td>11.643449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.340000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>-1.820000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>-1.578693</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18147.750000</td>\n",
       "      <td>2720.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.330000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>-0.380000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>-0.341100</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20052.000000</td>\n",
       "      <td>5600.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.340000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.110405</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>32035.000000</td>\n",
       "      <td>8363.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.410000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>1.170000</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>0.449555</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50070.000000</td>\n",
       "      <td>11142.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>2.010000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.987511</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          comment_id   annotator_id       platform      sentiment  \\\n",
       "count  135388.000000  135388.000000  135388.000000  135388.000000   \n",
       "mean    23528.597283    5566.552523       1.281199       2.954280   \n",
       "std     12386.724630    3230.864231       1.023495       1.231662   \n",
       "min         1.000000       1.000000       0.000000       0.000000   \n",
       "25%     18147.750000    2720.000000       0.000000       2.000000   \n",
       "50%     20052.000000    5600.000000       1.000000       3.000000   \n",
       "75%     32035.000000    8363.000000       2.000000       4.000000   \n",
       "max     50070.000000   11142.000000       3.000000       4.000000   \n",
       "\n",
       "             respect         insult      humiliate         status  \\\n",
       "count  135388.000000  135388.000000  135388.000000  135388.000000   \n",
       "mean        2.828751       2.563152       2.278333       2.698784   \n",
       "std         1.309693       1.389984       1.370983       0.898430   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         2.000000       2.000000       1.000000       2.000000   \n",
       "50%         3.000000       3.000000       3.000000       3.000000   \n",
       "75%         4.000000       4.000000       3.000000       3.000000   \n",
       "max         4.000000       4.000000       4.000000       4.000000   \n",
       "\n",
       "          dehumanize       violence  ...     hatespeech  hate_speech_score  \\\n",
       "count  135388.000000  135388.000000  ...  135388.000000      135388.000000   \n",
       "mean        1.845651       1.051666  ...       0.744527          -0.567465   \n",
       "std         1.402575       1.345656  ...       0.932267           2.380312   \n",
       "min         0.000000       0.000000  ...       0.000000          -8.340000   \n",
       "25%         1.000000       0.000000  ...       0.000000          -2.330000   \n",
       "50%         2.000000       0.000000  ...       0.000000          -0.340000   \n",
       "75%         3.000000       2.000000  ...       2.000000           1.410000   \n",
       "max         4.000000       4.000000  ...       2.000000           6.300000   \n",
       "\n",
       "             infitms       outfitms  annotator_severity        std_err  \\\n",
       "count  135388.000000  135388.000000       135388.000000  135388.000000   \n",
       "mean        1.034367       1.001085           -0.018735       0.300572   \n",
       "std         0.496865       0.791993            0.487189       0.236407   \n",
       "min         0.100000       0.070000           -1.820000       0.020000   \n",
       "25%         0.710000       0.560000           -0.380000       0.030000   \n",
       "50%         0.960000       0.830000           -0.020000       0.340000   \n",
       "75%         1.300000       1.220000            0.350000       0.420000   \n",
       "max         5.900000       9.000000            1.360000       1.900000   \n",
       "\n",
       "       annotator_infitms  annotator_outfitms     hypothesis  annotator_age  \n",
       "count      135388.000000       135388.000000  135388.000000  135388.000000  \n",
       "mean            1.007152            1.011709       0.014535      37.912799  \n",
       "std             0.269875            0.676072       0.613035      11.643449  \n",
       "min             0.390000            0.280000      -1.578693      18.000000  \n",
       "25%             0.810000            0.670000      -0.341100      29.000000  \n",
       "50%             0.970000            0.850000       0.110405      35.000000  \n",
       "75%             1.170000            1.130000       0.449555      45.000000  \n",
       "max             2.010000            9.000000       0.987511      81.000000  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('ucberkeley-dlab/measuring-hate-speech', 'binary')   \n",
    "df = dataset['train'].to_pandas()\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8015a1a8-b7eb-4397-b6c9-71f5c9c2651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtag(token: str) -> str:\n",
    "    \"\"\"\n",
    "    split the hashtags by uppercase letters\n",
    "    \"\"\"\n",
    "    i, end = len(token) - 2, len(token) + 1\n",
    "    response = ''\n",
    "    while i >= 1:\n",
    "        word = token[i:end]\n",
    "        if word[0].isupper():\n",
    "            response = token[i:end] + ' ' + response\n",
    "            end = i\n",
    "        i -= 1\n",
    "    else:\n",
    "        if end-1 > 0:\n",
    "            response = token[1:end] + ' ' + response\n",
    "    return(response.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f2dc43a-d17e-40c4-8d35-ae84b8be5d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(document: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace all abbreviations with their corresponding expansion\n",
    "    \"\"\"\n",
    "    document = re.sub(r\"'cause\", \"because\", document)\n",
    "    document = re.sub(r\"o'clock\", \"of the clock\", document)\n",
    "    document = re.sub(r\"won\\'t\", \"will not\", document)\n",
    "    document = re.sub(r\"can\\'t\", \"can not\", document)\n",
    "    document = re.sub(r\"n\\'t\", \" not\", document)\n",
    "    document = re.sub(r\"\\'re\", \" are\", document)\n",
    "    document = re.sub(r\"\\'s\", \" is\", document)\n",
    "    document = re.sub(r\"\\'d\", \" would\", document)\n",
    "    document = re.sub(r\"\\'ll\", \" will\", document)\n",
    "    document = re.sub(r\"\\'t\", \" not\", document)\n",
    "    document = re.sub(r\"\\'ve\", \" have\", document)\n",
    "    document = re.sub(r\"\\'m\", \" am\", document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ae22ba0-b3dc-4d6b-a86d-6f5929b6f5d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessing(document: str) -> list:\n",
    "    \"\"\"\n",
    "    expand contractions\n",
    "    replace any @user type with 'user' token\n",
    "    split hashtags into words by upper case\n",
    "    text to lower case\n",
    "    remove new line char\n",
    "    remove non latin chars\n",
    "    remove extra blank spaces\n",
    "    \"\"\"\n",
    "    document = expand_contractions(document)\n",
    "    document = re.sub('@[a-zA-Z0-9_-]{0,150}', 'user', document)\n",
    "    hashtags = re.findall('#[a-zA-Z0-9_-]{0,150}', document)\n",
    "    for hashtag in hashtags:\n",
    "        split = split_hashtag(hashtag)\n",
    "        document = re.sub(hashtag, split, document)\n",
    "    document = document.lower()\n",
    "    document = re.sub('\\n', ' ', document)\n",
    "    document = re.sub('[^a-zA-Z]|[0-9]', ' ', document)\n",
    "    document = re.sub('\\s+', ' ', document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "631d52f8-74d2-4a3e-95db-76f2ac9307e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(testing_label, predictions):\n",
    "    \"\"\"\n",
    "    extrac the metrics from the testing labels and predictions\n",
    "    \"\"\"\n",
    "    print('Precision score macro: ', format(precision_score(testing_label, predictions, average='macro')))\n",
    "    print('Recall score macro: ', format(recall_score(testing_label, predictions, average='macro')))\n",
    "    print('F1 score macro: ', format(f1_score(testing_label, predictions, average='macro')))\n",
    "    print('Precision score macro: ', format(precision_score(testing_label, predictions, average='micro')))\n",
    "    print('Recall score micro: ', format(recall_score(testing_label, predictions, average='micro')))\n",
    "    print('F1 score micro: ', format(f1_score(testing_label, predictions, average='micro')))\n",
    "    print('Accuracy: ', format(accuracy_score(testing_label, predictions)))\n",
    "    print('--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0381f340-15b3-440f-8c07-de679495dc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135388, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "data['processed'] = df.text.apply(preprocessing)\n",
    "data['hatespeech'] = df.hatespeech.apply(lambda x: math.ceil(x/2))\n",
    "# data['hatespeech'] = df.hate_speech_score.apply(lambda x: 1 if x > -0.5 else 0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4152f8c-3fd4-4880-8e20-3efac7a76c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = pd.DataFrame.from_records(data.processed.apply(lexicon_sentiwordnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb06035d-2279-4a04-a160-68c7c3cd2fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.250</td>\n",
       "      <td>1.750</td>\n",
       "      <td>47.000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.033654</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.250</td>\n",
       "      <td>0.125</td>\n",
       "      <td>8.625</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000</td>\n",
       "      <td>2.125</td>\n",
       "      <td>39.875</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.049419</td>\n",
       "      <td>0.927326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.625</td>\n",
       "      <td>0.500</td>\n",
       "      <td>22.875</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.375</td>\n",
       "      <td>0.375</td>\n",
       "      <td>26.250</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135383</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.375</td>\n",
       "      <td>24.875</td>\n",
       "      <td>0.028846</td>\n",
       "      <td>0.014423</td>\n",
       "      <td>0.956731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135384</th>\n",
       "      <td>0.375</td>\n",
       "      <td>1.000</td>\n",
       "      <td>27.625</td>\n",
       "      <td>0.012931</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.952586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135385</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.250</td>\n",
       "      <td>18.250</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.960526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135386</th>\n",
       "      <td>0.375</td>\n",
       "      <td>1.000</td>\n",
       "      <td>27.625</td>\n",
       "      <td>0.012931</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.952586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135387</th>\n",
       "      <td>1.750</td>\n",
       "      <td>3.000</td>\n",
       "      <td>42.250</td>\n",
       "      <td>0.037234</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.898936</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135388 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0      1       2         3         4         5         6   \\\n",
       "0       3.250  1.750  47.000  0.062500  0.033654  0.903846  0.019231   \n",
       "1       1.250  0.125   8.625  0.125000  0.012500  0.862500  0.100000   \n",
       "2       1.000  2.125  39.875  0.023256  0.049419  0.927326  0.000000   \n",
       "3       0.625  0.500  22.875  0.026042  0.020833  0.953125  0.000000   \n",
       "4       0.375  0.375  26.250  0.013889  0.013889  0.972222  0.000000   \n",
       "...       ...    ...     ...       ...       ...       ...       ...   \n",
       "135383  0.750  0.375  24.875  0.028846  0.014423  0.956731  0.000000   \n",
       "135384  0.375  1.000  27.625  0.012931  0.034483  0.952586  0.000000   \n",
       "135385  0.500  0.250  18.250  0.026316  0.013158  0.960526  0.000000   \n",
       "135386  0.375  1.000  27.625  0.012931  0.034483  0.952586  0.000000   \n",
       "135387  1.750  3.000  42.250  0.037234  0.063830  0.898936  0.021277   \n",
       "\n",
       "              7         8   9   10  11  12  \n",
       "0       0.019231  0.942308   1   1  49   0  \n",
       "1       0.000000  0.900000   1   0   9   0  \n",
       "2       0.046512  0.953488   0   2  41   1  \n",
       "3       0.000000  1.000000   0   0  24   0  \n",
       "4       0.000000  1.000000   0   0  27   0  \n",
       "...          ...       ...  ..  ..  ..  ..  \n",
       "135383  0.000000  1.000000   0   0  26   0  \n",
       "135384  0.034483  1.000000   0   1  29   1  \n",
       "135385  0.000000  1.000000   0   0  19   0  \n",
       "135386  0.034483  1.000000   0   1  29   1  \n",
       "135387  0.063830  0.914894   1   3  43   1  \n",
       "\n",
       "[135388 rows x 13 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be9193e4-c735-4031-b82f-69562cdb9636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the total set: 135388\n",
      "Number of rows in the training set: 94771\n",
      "Number of rows in the validation set: 13539\n",
      "Number of rows in the test set: 27078\n"
     ]
    }
   ],
   "source": [
    "y_sample = data['hatespeech'].astype(float)\n",
    "X_sample = lexicon.astype(float) #data['lexicon'].astype(list)\n",
    "\n",
    "X_train_validation, X_test, y_train_validation, y_test = train_test_split(X_sample.values, \n",
    "                                                    y_sample.values, \n",
    "                                                    random_state=42,test_size=0.2)\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train_validation, \n",
    "                                                    y_train_validation, \n",
    "                                                    random_state=42,test_size=0.125)\n",
    "\n",
    "print('Number of rows in the total set: {}'.format(X_sample.shape[0]))\n",
    "print('Number of rows in the training set: {}'.format(X_train.shape[0]))\n",
    "print('Number of rows in the validation set: {}'.format(X_validation.shape[0]))\n",
    "print('Number of rows in the test set: {}'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c8320ac-9b81-418a-8833-7812caefde0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Accuracy Through Grid Search : 0.5907714073865876\n",
      "best parameter :  {'alpha': 0.36}\n"
     ]
    }
   ],
   "source": [
    "params = {'alpha': [(i/10) for i in range(1,11)],}\n",
    "multinomial_nb_grid = GridSearchCV(MultinomialNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
    "multinomial_nb_grid.fit(X_train, y_train)\n",
    "alpha = multinomial_nb_grid.best_params_.get('alpha')\n",
    "\n",
    "params = {'alpha': [(i/100) + alpha - 0.1 for i in range(1,11)] + [(i/100) + alpha for i in range(1,11)],}\n",
    "multinomial_nb_grid = GridSearchCV(MultinomialNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
    "multinomial_nb_grid.fit(X_train, y_train)\n",
    "alpha = multinomial_nb_grid.best_params_.get('alpha')\n",
    "\n",
    "print(f'Best Accuracy Through Grid Search : {multinomial_nb_grid.best_score_}')\n",
    "print('best parameter : ', multinomial_nb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f897a392-e3b1-4c92-8800-11e15c45c32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score macro:  0.5687441837225425\n",
      "Recall score macro:  0.5645196659672183\n",
      "F1 score macro:  0.5644593721439964\n",
      "Precision score macro:  0.591845778861068\n",
      "Recall score micro:  0.591845778861068\n",
      "F1 score micro:  0.591845778861068\n",
      "Accuracy:  0.591845778861068\n",
      "--------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.71      0.67     16135\n",
      "         1.0       0.49      0.42      0.46     10943\n",
      "\n",
      "    accuracy                           0.59     27078\n",
      "   macro avg       0.57      0.56      0.56     27078\n",
      "weighted avg       0.58      0.59      0.59     27078\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11408, 4727, 6325, 4618)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes = MultinomialNB(alpha=alpha)\n",
    "naive_bayes.fit(X_train,y_train)\n",
    "pred = naive_bayes.predict(X_test)  \n",
    "print_metrics(y_test, pred)\n",
    "\n",
    "report = classification_report(y_test, pred)  \n",
    "print(report)\n",
    "#En terminos de TP, FP, TN, FN\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "(tn,fp,fn,tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e12edeab-cd5a-4573-8b63-a946bf427c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(naive_bayes, open('salida/swn_lexicon_naive_bayes.model', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bd37dfbc-927c-4d17-8194-eeccb6bcd13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(text):\n",
    "    naive_bayes = pickle.load(open('salida/swn_lexicon_naive_bayes.model', 'rb'))\n",
    "    data = lexicon_sentiwordnet(text)\n",
    "    return 'Hate Speech' if naive_bayes.predict([data])[0] == 1 else 'Not Hate Speech'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d413956f-7482-4eea-acda-6963b2daa863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hate Speech'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor('everybody will die sooner or later')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c75c819-ba17-49db-9efe-145445dff0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "tuned hpyerparameters :(best parameters)  {'C': 1.0, 'penalty': 'l2'}\n",
      "accuracy : 0.6265945855613392\n"
     ]
    }
   ],
   "source": [
    "grid = {\n",
    "    \"C\": np.logspace(-3,3,7), \n",
    "    \"penalty\": [\"none\", \"l2\"]\n",
    "}\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=0, multi_class='ovr')\n",
    "logreg_cv = GridSearchCV(logreg, grid, n_jobs=-1, cv=5, verbose=5)\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "C = logreg_cv.best_params_.get('C')\n",
    "penalty = logreg_cv.best_params_.get('penalty')\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"accuracy :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba751312-4299-4bf3-9671-05c2a690c21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score macro:  0.6245116196904977\n",
      "Recall score macro:  0.6162536450895261\n",
      "F1 score macro:  0.6125701595482325\n",
      "Precision score macro:  0.6213531280005908\n",
      "Recall score micro:  0.6213531280005908\n",
      "F1 score micro:  0.6213531280005908\n",
      "Accuracy:  0.6213531280005908\n",
      "--------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.49      0.55     12989\n",
      "         1.0       0.61      0.74      0.67     14089\n",
      "\n",
      "    accuracy                           0.62     27078\n",
      "   macro avg       0.62      0.62      0.61     27078\n",
      "weighted avg       0.62      0.62      0.61     27078\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6374, 6615, 3638, 10451)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regresion = LogisticRegression(max_iter=1000, random_state=0, multi_class='ovr', C=C, penalty=penalty)\n",
    "logistic_regresion.fit(X_train, y_train)\n",
    "pred = logistic_regresion.predict(X_test)  \n",
    "print_metrics(y_test, pred)\n",
    "\n",
    "report = classification_report(y_test, pred)  \n",
    "print(report)\n",
    "#En terminos de TP, FP, TN, FN\n",
    "report = classification_report(y_test, pred)  \n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "(tn,fp,fn,tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5da464-6fbc-4538-be05-af38de6b5821",
   "metadata": {},
   "source": [
    "# Dataset Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1af8e3c3-6bd2-4ac3-915d-a94f7ce83948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_prediction(training_data, testing_data, training_label):\n",
    "    \"\"\"\n",
    "    get the naive bayes prediction from training and testing data\n",
    "    \"\"\"\n",
    "    naive_bayes = MultinomialNB()\n",
    "    naive_bayes.fit(training_data, training_label)\n",
    "    return naive_bayes.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "530feab4-f978-4a45-95fb-812bc9c0518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_prediction(training_data, testing_data, training_label):\n",
    "    \"\"\"\n",
    "    get the logistic regression prediction from training and testing data\n",
    "    \"\"\"\n",
    "    logistic = LogisticRegression(random_state=0, multi_class='multinomial')\n",
    "    logistic.fit(training_data, training_label)\n",
    "    return logistic.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1eecec7-b4a4-4111-9099-c8ffb6bc3727",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ['hatespeech', 'hate_speech_score']\n",
    "columns = ['sentiment', 'respect', 'insult', \n",
    "           'humiliate', 'status', 'dehumanize', \n",
    "           'violence', 'genocide', 'attack_defend', \n",
    "           'hatespeech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e65de63-0519-4f5a-a5ac-2abbec58717b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score macro:  0.7376187904647078\n",
      "Recall score macro:  0.5792161831189642\n",
      "F1 score macro:  0.5573721148778248\n",
      "Precision score macro:  0.7069699879360859\n",
      "Recall score micro:  0.7069699879360859\n",
      "F1 score micro:  0.7069699879360859\n",
      "Accuracy:  0.7069699879360859\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "data = df[columns]\n",
    "min_max_scaler = sk_preprocess.MinMaxScaler()\n",
    "data_scaled = min_max_scaler.fit_transform(data.values)\n",
    "dataset = pd.DataFrame(data_scaled, columns=columns)\n",
    "y_sample = dataset.hatespeech.apply(lambda x: int(x))\n",
    "X_sample = dataset[columns]\n",
    "X_sample = X_sample.drop(['hatespeech'], axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.3, random_state=0)\n",
    "nb_features = naive_bayes_prediction(x_train, x_test, y_train.values)\n",
    "print_metrics(y_test, nb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b4300e9-263e-4bee-8f86-42952ed64c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135388, 9)\n",
      "(135388,)\n"
     ]
    }
   ],
   "source": [
    "print(X_sample.shape)\n",
    "print(y_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "614bc409-b7d8-41f3-9a24-f1d7abb158b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[columns]\n",
    "min_max_scaler = sk_preprocess.MinMaxScaler()\n",
    "data_scaled = min_max_scaler.fit_transform(data.values)\n",
    "dataset = pd.DataFrame(data_scaled, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "021d47bf-c045-4cb2-a850-deb35b9be784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score macro:  0.8046798113080479\n",
      "Recall score macro:  0.7958342039904578\n",
      "F1 score macro:  0.7984444642829169\n",
      "Precision score macro:  0.803308959302755\n",
      "Recall score micro:  0.803308959302755\n",
      "F1 score micro:  0.803308959302755\n",
      "Accuracy:  0.803308959302755\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "y_sample = df.hate_speech_score.apply(lambda x: 1 if (x > 0) else 0)\n",
    "X_sample = dataset[columns]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.3, random_state=0)\n",
    "nb_features = naive_bayes_prediction(x_train, x_test, y_train.values)\n",
    "print_metrics(y_test, nb_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
