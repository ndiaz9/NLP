{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importación de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, datasets, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importación del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration ucberkeley-dlab--measuring-hate-speech-1d47093687320b66\n",
      "Reusing dataset parquet (C:\\Users\\camilo\\.cache\\huggingface\\datasets\\ucberkeley-dlab___parquet\\ucberkeley-dlab--measuring-hate-speech-1d47093687320b66\\0.0.0\\7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n",
      "100%|██████████| 1/1 [00:00<00:00, 47.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>platform</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>respect</th>\n",
       "      <th>insult</th>\n",
       "      <th>humiliate</th>\n",
       "      <th>status</th>\n",
       "      <th>dehumanize</th>\n",
       "      <th>violence</th>\n",
       "      <th>...</th>\n",
       "      <th>annotator_religion_hindu</th>\n",
       "      <th>annotator_religion_jewish</th>\n",
       "      <th>annotator_religion_mormon</th>\n",
       "      <th>annotator_religion_muslim</th>\n",
       "      <th>annotator_religion_nothing</th>\n",
       "      <th>annotator_religion_other</th>\n",
       "      <th>annotator_sexuality_bisexual</th>\n",
       "      <th>annotator_sexuality_gay</th>\n",
       "      <th>annotator_sexuality_straight</th>\n",
       "      <th>annotator_sexuality_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47777</td>\n",
       "      <td>10873</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39773</td>\n",
       "      <td>2790</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47101</td>\n",
       "      <td>3379</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43625</td>\n",
       "      <td>7365</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12538</td>\n",
       "      <td>488</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_id  annotator_id  platform  sentiment  respect  insult  humiliate  \\\n",
       "0       47777         10873         3        0.0      0.0     0.0        0.0   \n",
       "1       39773          2790         2        0.0      0.0     0.0        0.0   \n",
       "2       47101          3379         3        4.0      4.0     4.0        4.0   \n",
       "3       43625          7365         3        2.0      3.0     2.0        1.0   \n",
       "4       12538           488         0        4.0      4.0     4.0        4.0   \n",
       "\n",
       "   status  dehumanize  violence  ...  annotator_religion_hindu  \\\n",
       "0     2.0         0.0       0.0  ...                     False   \n",
       "1     2.0         0.0       0.0  ...                     False   \n",
       "2     4.0         4.0       0.0  ...                     False   \n",
       "3     2.0         0.0       0.0  ...                     False   \n",
       "4     4.0         4.0       4.0  ...                     False   \n",
       "\n",
       "   annotator_religion_jewish  annotator_religion_mormon  \\\n",
       "0                      False                      False   \n",
       "1                      False                      False   \n",
       "2                      False                      False   \n",
       "3                      False                      False   \n",
       "4                      False                      False   \n",
       "\n",
       "   annotator_religion_muslim annotator_religion_nothing  \\\n",
       "0                      False                      False   \n",
       "1                      False                      False   \n",
       "2                      False                       True   \n",
       "3                      False                      False   \n",
       "4                      False                      False   \n",
       "\n",
       "   annotator_religion_other  annotator_sexuality_bisexual  \\\n",
       "0                     False                         False   \n",
       "1                     False                         False   \n",
       "2                     False                         False   \n",
       "3                     False                         False   \n",
       "4                     False                         False   \n",
       "\n",
       "   annotator_sexuality_gay  annotator_sexuality_straight  \\\n",
       "0                    False                          True   \n",
       "1                    False                          True   \n",
       "2                    False                          True   \n",
       "3                    False                          True   \n",
       "4                    False                          True   \n",
       "\n",
       "   annotator_sexuality_other  \n",
       "0                      False  \n",
       "1                      False  \n",
       "2                      False  \n",
       "3                      False  \n",
       "4                      False  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('ucberkeley-dlab/measuring-hate-speech', 'binary')   \n",
    "df = dataset['train'].to_pandas()\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### funciones de procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtag(token: str) -> str:\n",
    "    \"\"\"\n",
    "    Split hashtags by uppercases\n",
    "    \"\"\"\n",
    "    i, end = len(token) - 2, len(token) + 1\n",
    "    response = ''\n",
    "    while i >= 1:\n",
    "        word = token[i:end]\n",
    "        if word[0].isupper():\n",
    "            response = token[i:end] + ' ' + response\n",
    "            end = i\n",
    "        i -= 1\n",
    "    else:\n",
    "        if end-1 > 0:\n",
    "            response = token[1:end] + ' ' + response\n",
    "    return(response.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(document: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace all abbreviations with their corresponding expansion\n",
    "    \"\"\"\n",
    "    document = re.sub(r\"'cause\", \"because\", document)\n",
    "    document = re.sub(r\"o'clock\", \"of the clock\", document)\n",
    "    document = re.sub(r\"won\\'t\", \"will not\", document)\n",
    "    document = re.sub(r\"can\\'t\", \"can not\", document)\n",
    "    document = re.sub(r\"n\\'t\", \" not\", document)\n",
    "    document = re.sub(r\"\\'re\", \" are\", document)\n",
    "    document = re.sub(r\"\\'s\", \" is\", document)\n",
    "    document = re.sub(r\"\\'d\", \" would\", document)\n",
    "    document = re.sub(r\"\\'ll\", \" will\", document)\n",
    "    document = re.sub(r\"\\'t\", \" not\", document)\n",
    "    document = re.sub(r\"\\'ve\", \" have\", document)\n",
    "    document = re.sub(r\"\\'m\", \" am\", document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document: str) -> list:\n",
    "    \"\"\"\n",
    "    expand contractions\n",
    "    replace any @user type with 'user' token\n",
    "    split hashtags into words by upper case\n",
    "    text to lower case\n",
    "    remove new line char\n",
    "    remove non latin chars\n",
    "    remove extra blank spaces\n",
    "    \"\"\"\n",
    "    document = expand_contractions(document)\n",
    "    document = re.sub('@[a-zA-Z0-9_-]{0,150}', 'user', document)\n",
    "    hashtags = re.findall('#[a-zA-Z0-9_-]{0,150}', document)\n",
    "    for hashtag in hashtags:\n",
    "        split = split_hashtag(hashtag)\n",
    "        document = re.sub(hashtag, split, document)\n",
    "    document = document.lower()\n",
    "    document = re.sub('\\n', ' ', document)\n",
    "    document = re.sub('[^a-zA-Z]|[0-9]', ' ', document)\n",
    "    document = re.sub('\\s+', ' ', document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicar preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135388, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "data['text'] = df.text\n",
    "data['processed'] = df.text.apply(preprocessing)\n",
    "data['hatespeech'] = df.hatespeech.apply(lambda x: math.ceil(x/2))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>text</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70569</th>\n",
       "      <td>0</td>\n",
       "      <td>You're far more likely to be gunned down at Wa...</td>\n",
       "      <td>you are far more likely to be gunned down at w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27166</th>\n",
       "      <td>0</td>\n",
       "      <td>@masango1 Thats the slave mentality school of ...</td>\n",
       "      <td>user thats the slave mentality school of thoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15286</th>\n",
       "      <td>1</td>\n",
       "      <td>lol thot acts all loving yet tries to go out w...</td>\n",
       "      <td>lol thot acts all loving yet tries to go out w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36083</th>\n",
       "      <td>0</td>\n",
       "      <td>How about our homeless veterans, they don't ge...</td>\n",
       "      <td>how about our homeless veterans they do not ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15132</th>\n",
       "      <td>0</td>\n",
       "      <td>Satan is gathering all of them together... May...</td>\n",
       "      <td>satan is gathering all of them together may al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116701</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm sure there were several others, but Islam ...</td>\n",
       "      <td>i am sure there were several others but islam ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127428</th>\n",
       "      <td>0</td>\n",
       "      <td>There is plenty of evidence that women and non...</td>\n",
       "      <td>there is plenty of evidence that women and non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37427</th>\n",
       "      <td>0</td>\n",
       "      <td>for people talking about how awkward or uncomf...</td>\n",
       "      <td>for people talking about how awkward or uncomf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62289</th>\n",
       "      <td>1</td>\n",
       "      <td>While I will not really *defend* the church, I...</td>\n",
       "      <td>while i will not really defend the church i ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43136</th>\n",
       "      <td>1</td>\n",
       "      <td>actually a little more 547 but regardless that...</td>\n",
       "      <td>actually a little more but regardless thats a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        hatespeech                                               text  \\\n",
       "70569            0  You're far more likely to be gunned down at Wa...   \n",
       "27166            0  @masango1 Thats the slave mentality school of ...   \n",
       "15286            1  lol thot acts all loving yet tries to go out w...   \n",
       "36083            0  How about our homeless veterans, they don't ge...   \n",
       "15132            0  Satan is gathering all of them together... May...   \n",
       "116701           0  I'm sure there were several others, but Islam ...   \n",
       "127428           0  There is plenty of evidence that women and non...   \n",
       "37427            0  for people talking about how awkward or uncomf...   \n",
       "62289            1  While I will not really *defend* the church, I...   \n",
       "43136            1  actually a little more 547 but regardless that...   \n",
       "\n",
       "                                                processed  \n",
       "70569   you are far more likely to be gunned down at w...  \n",
       "27166   user thats the slave mentality school of thoug...  \n",
       "15286   lol thot acts all loving yet tries to go out w...  \n",
       "36083   how about our homeless veterans they do not ge...  \n",
       "15132   satan is gathering all of them together may al...  \n",
       "116701  i am sure there were several others but islam ...  \n",
       "127428  there is plenty of evidence that women and non...  \n",
       "37427   for people talking about how awkward or uncomf...  \n",
       "62289   while i will not really defend the church i ca...  \n",
       "43136   actually a little more but regardless thats a ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)[['hatespeech','text','processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the total set: 135388\n",
      "Number of rows in the training set: 94771\n",
      "Number of rows in the validation set: 13539\n",
      "Number of rows in the test set: 27078\n"
     ]
    }
   ],
   "source": [
    "y_sample = data['hatespeech'].astype(float)\n",
    "X_sample = data['processed'].astype(str)\n",
    "\n",
    "X_train_validation, X_test, y_train_validation, y_test = train_test_split(X_sample.values, \n",
    "                                                    y_sample.values, \n",
    "                                                    random_state=42,test_size=0.2)\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train_validation, \n",
    "                                                    y_train_validation, \n",
    "                                                    random_state=42,test_size=0.125)\n",
    "\n",
    "print('Number of rows in the total set: {}'.format(X_sample.shape[0]))\n",
    "print('Number of rows in the training set: {}'.format(X_train.shape[0]))\n",
    "print('Number of rows in the validation set: {}'.format(X_validation.shape[0]))\n",
    "print('Number of rows in the test set: {}'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genera la representación tf del vocabulario\n",
    "count_vector = CountVectorizer()\n",
    "training_data_tf = count_vector.fit_transform(X_train)\n",
    "validation_data_tf = count_vector.transform(X_validation)\n",
    "testing_data_tf = count_vector.transform(X_test)\n",
    "\n",
    "training_validation_x_tf = np.concatenate((X_train,X_validation))\n",
    "cross_validation_x_tf = count_vector.transform(training_validation_x_tf)\n",
    "cross_validation_y_tf = np.concatenate((y_train,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se normaliza la representación vectorial\n",
    "training_data_tf_norm = Normalizer().fit_transform(training_data_tf)\n",
    "validation_data_tf_norm = Normalizer().fit_transform(validation_data_tf)\n",
    "testing_data_tf_norm = Normalizer().fit_transform(testing_data_tf)\n",
    "cross_validation_data_tf_norm = Normalizer().fit_transform(cross_validation_x_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genera la representación tfidf del vocabulario\n",
    "count_vector_tfidf = TfidfVectorizer()\n",
    "training_data_tfidf = count_vector_tfidf.fit_transform(X_train)\n",
    "validation_data_tfidf = count_vector_tfidf.transform(X_validation)\n",
    "testing_data_tfidf = count_vector_tfidf.transform(X_test)\n",
    "\n",
    "training_validation_x_tfidf = np.concatenate((X_train,X_validation))\n",
    "cross_validation_x_tfidf = count_vector.transform(training_validation_x_tfidf)\n",
    "cross_validation_y_tfidf = np.concatenate((y_train,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se normaliza la representación vectorial\n",
    "training_data_tfidf_norm = Normalizer().fit_transform(training_data_tfidf) \n",
    "validation_data_tfidf_norm = Normalizer().fit_transform(validation_data_tfidf)\n",
    "testing_data_tfidf_norm = Normalizer().fit_transform(testing_data_tfidf)\n",
    "cross_validation_data_tfidf_norm = Normalizer().fit_transform(cross_validation_x_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([34.16673875, 34.42043376, 37.03759408, 37.1931653 , 39.48011065]),\n",
       " 'score_time': array([0.49567389, 0.50863814, 0.53556895, 0.59241486, 0.54354477]),\n",
       " 'test_f1_macro': array([0.59675233, 0.5922228 , 0.57995417, 0.594867  , 0.60029776]),\n",
       " 'test_accuracy': array([0.69111809, 0.68779429, 0.68359339, 0.69047179, 0.69287231]),\n",
       " 'test_precision_macro': array([0.79752028, 0.78994021, 0.80403243, 0.79990302, 0.79750812]),\n",
       " 'test_recall_macro': array([0.62178679, 0.61829061, 0.6114582 , 0.62074128, 0.62411963])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento del algoritmo de naive bayes y cross validation para la\n",
    "# representación vectoria de tf\n",
    "clf_rf = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "scores_rf = cross_validate(clf_rf, cross_validation_data_tf_norm, cross_validation_y_tf, cv=5, scoring=('f1_macro','accuracy','precision_macro','recall_macro'))\n",
    "scores_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Accuracy Through Grid Search : 0.7012061427259215\n",
      "best parameter :  {'max_depth': 12, 'max_features': 'sqrt', 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "# Busqueda de hiperaparametros variando el alpha\n",
    "\n",
    "params = { \n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth' : [6,8,10,12],\n",
    "}\n",
    "\n",
    "random_forest_grid = GridSearchCV(RandomForestClassifier(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
    "random_forest_grid.fit(training_data_tf_norm,y_train)\n",
    "\n",
    "print(f'Best Accuracy Through Grid Search : {random_forest_grid.best_score_}')\n",
    "print('best parameter : ', random_forest_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([231.67593455, 170.14602423, 155.98456144, 158.26661444,\n",
       "        173.52192259, 161.16397071, 154.10320163, 151.75842309,\n",
       "        166.09123468, 100.31335902]),\n",
       " 'score_time': array([1.09706497, 0.83080888, 0.8526876 , 0.82579017, 0.82781768,\n",
       "        0.82881761, 0.82334733, 0.87165666, 0.91557622, 0.94450545]),\n",
       " 'test_accuracy': array([0.69891977, 0.70187425, 0.70492106, 0.69153356, 0.70242822,\n",
       "        0.70409011, 0.69688856, 0.70353615, 0.69891977, 0.70058166]),\n",
       " 'test_precision': array([0.92792109, 0.91612903, 0.92280453, 0.910086  , 0.92560175,\n",
       "        0.9424944 , 0.92155369, 0.92748368, 0.92158327, 0.92324723]),\n",
       " 'test_recall': array([0.27865117, 0.29111617, 0.29681093, 0.26514806, 0.28906606,\n",
       "        0.28747153, 0.27562642, 0.29134396, 0.28109339, 0.28496583]),\n",
       " 'test_f1': array([0.42859646, 0.44183232, 0.44915546, 0.41065444, 0.44054852,\n",
       "        0.44056554, 0.42433807, 0.4434044 , 0.43079071, 0.43550914])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluación del mejor modelo\n",
    "random_forest_best_1 = RandomForestClassifier(max_depth=12,max_features='sqrt', n_estimators=500, random_state=0)\n",
    "scores = cross_validate(random_forest_best_1, cross_validation_data_tf_norm, cross_validation_y_tf, cv=10, scoring=('accuracy','precision','recall','f1'))\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.6960262944087451\n",
      "Precision score:  0.922429906542056\n",
      "Recall score:  0.27058393493557525\n",
      "F1 score:  0.4184271885819261\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo\n",
    "\n",
    "predictions = random_forest_best_1.predict(testing_data_tf_norm)\n",
    "print('Accuracy score: ', format(accuracy_score(y_test, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.98      0.79     16135\n",
      "         1.0       0.92      0.27      0.42     10943\n",
      "\n",
      "    accuracy                           0.70     27078\n",
      "   macro avg       0.79      0.63      0.61     27078\n",
      "weighted avg       0.77      0.70      0.64     27078\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15886, 249, 7982, 2961)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = random_forest_best_1.predict(testing_data_tf_norm)  \n",
    "report = classification_report(y_test ,pred)  \n",
    "print(report)\n",
    "#En terminos de TP, FP, TN, FN\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "(tn,fp,fn,tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([28.76288509, 24.29397607, 25.30530691, 46.4389472 , 37.47428966,\n",
       "        23.02441645, 23.10136962, 23.41798401, 25.96352077, 14.02371335]),\n",
       " 'score_time': array([0.16356373, 0.16356397, 0.17951298, 0.42502356, 0.1695478 ,\n",
       "        0.15857744, 0.16160011, 0.1605978 , 0.16558886, 0.24135041]),\n",
       " 'test_accuracy': array([0.68322408, 0.68451667, 0.69328779, 0.68534761, 0.68774813,\n",
       "        0.68257779, 0.69024097, 0.70048934, 0.68996399, 0.68470132]),\n",
       " 'test_precision': array([0.94434137, 0.93632287, 0.93064516, 0.91894198, 0.92567568,\n",
       "        0.95333333, 0.93160967, 0.93343419, 0.92225859, 0.92800702]),\n",
       " 'test_recall': array([0.2319435 , 0.23781321, 0.26287016, 0.2453303 , 0.24965831,\n",
       "        0.22801822, 0.25444191, 0.28109339, 0.25671982, 0.24077449]),\n",
       " 'test_f1': array([0.37241632, 0.37929155, 0.40994671, 0.38727077, 0.3932544 ,\n",
       "        0.36801471, 0.39971372, 0.43207283, 0.40163934, 0.38234762])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento del algoritmo de naive bayes y cross validation para la\n",
    "# representación vectoria de tfidf\n",
    "\n",
    "clf_rf_2 = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "scores = cross_validate(clf_rf_2, cross_validation_data_tfidf_norm, cross_validation_y_tfidf, cv=10, scoring=('accuracy','precision','recall','f1'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Accuracy Through Grid Search : 0.6997182789124718\n",
      "best parameter :  {'max_depth': 12, 'max_features': 'sqrt', 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# Busqueda de hiperaparametros variando el alpha\n",
    "params = { \n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth' : [6,8,10,12],\n",
    "}\n",
    "random_forest_grid = GridSearchCV(RandomForestClassifier(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
    "random_forest_grid.fit(training_data_tfidf_norm,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy Through Grid Search : 0.6997182789124718\n",
      "best parameter :  {'max_depth': 12, 'max_features': 'sqrt', 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best Accuracy Through Grid Search : {random_forest_grid.best_score_}')\n",
    "print('best parameter : ', random_forest_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([103.18501425,  74.94973254,  68.93974233,  67.24169517,\n",
       "         62.57850933,  62.71221471,  63.77370358,  62.23944807,\n",
       "         53.56067204,  27.97713399]),\n",
       " 'score_time': array([0.40990376, 0.37200284, 0.35899973, 0.35803986, 0.39494205,\n",
       "        0.36598277, 0.35009503, 0.33909249, 0.32217002, 0.30720592]),\n",
       " 'test_accuracy': array([0.69061029, 0.70058166, 0.70178192, 0.6936571 , 0.69624227,\n",
       "        0.69864278, 0.69688856, 0.70048934, 0.6936571 , 0.69762718]),\n",
       " 'test_precision': array([0.92892562, 0.91229331, 0.92647059, 0.91104294, 0.92177914,\n",
       "        0.94400631, 0.9221968 , 0.93343419, 0.92072214, 0.92395437]),\n",
       " 'test_recall': array([0.25609478, 0.28906606, 0.28701595, 0.27061503, 0.2738041 ,\n",
       "        0.27266515, 0.27539863, 0.28109339, 0.26719818, 0.27676538]),\n",
       " 'test_f1': array([0.40150027, 0.43902439, 0.43826087, 0.41728135, 0.42219881,\n",
       "        0.42311771, 0.42413612, 0.43207283, 0.41419492, 0.42594216])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluación del mejor modelo\n",
    "random_forest_best_2 = RandomForestClassifier(max_depth=12,max_features='sqrt', n_estimators=200, random_state=0)\n",
    "random_forest_best_2.fit(training_data_tfidf_norm,y_train)\n",
    "scores = cross_validate(random_forest_best_2, cross_validation_data_tfidf_norm, cross_validation_y_tfidf, cv=10, scoring=('accuracy','precision','recall','f1'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.7010488219218554\n",
      "Precision score:  0.9225519287833828\n",
      "Recall score:  0.2841085625514027\n",
      "F1 score:  0.4344302382449521\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo\n",
    "\n",
    "predictions = random_forest_best_2.predict(testing_data_tfidf_norm)\n",
    "print('Accuracy score: ', format(accuracy_score(y_test, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.98      0.80     16135\n",
      "         1.0       0.92      0.28      0.43     10943\n",
      "\n",
      "    accuracy                           0.70     27078\n",
      "   macro avg       0.80      0.63      0.62     27078\n",
      "weighted avg       0.77      0.70      0.65     27078\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15874, 261, 7834, 3109)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = random_forest_best_2.predict(testing_data_tfidf_norm)  \n",
    "report = classification_report(y_test ,pred)  \n",
    "print(report)\n",
    "#En terminos de TP, FP, TN, FN\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "(tn,fp,fn,tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(random_forest_best_2, open('salida/random_forest.model', 'wb'))\n",
    "pickle.dump(count_vector_tfidf, open('salida/random_forest.vector', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ca142a229fb5f37069e46607530c048256c48ed42465a65754ddd3fc147db0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
