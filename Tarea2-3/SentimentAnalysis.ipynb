{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sentiment classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentiment classifier per each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__paths to change__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input variables\n",
    "path = './datos/Multi Domain Sentiment/'\n",
    "\n",
    "# notebook variables\n",
    "books_path = path + 'books/'\n",
    "dvd_path = path + 'dvd/'\n",
    "electronics_path = path + 'electronics/'\n",
    "kitchen_path = path + 'kitchen/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Read documents methods__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_training_data(path: str) -> pd.DataFrame:\n",
    "    nroute = path + 'negative.review'\n",
    "    proute = path + 'positive.review'\n",
    "    negative = pd.read_csv(nroute, delimiter='#label#:', engine='python', header=None, names=['data', 'label'])\n",
    "    positive = pd.read_csv(proute, delimiter='#label#:', engine='python', header=None, names=['data', 'label'])\n",
    "    training = pd.concat([negative, positive], axis=0, ignore_index=True)\n",
    "    training['label'] = training.label.map({'negative':0, 'positive':1})\n",
    "    training = shuffle(training, random_state=0)\n",
    "    training['clean'] = training.data.apply(preprocessing)\n",
    "    return training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testing_data(path: str) -> pd.DataFrame:\n",
    "    route = path + 'unlabeled.review'\n",
    "    testing = pd.read_csv(route, delimiter='#label#:', engine='python', header=None, names=['data', 'label'])\n",
    "    testing['label'] = testing.label.map({'negative':0, 'positive':1})\n",
    "    testing = shuffle(testing, random_state=0)\n",
    "    testing['clean'] = testing.data.apply(preprocessing)\n",
    "    return testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__preprocessing methods__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_characters(word: str) -> str:\n",
    "    # remove control characters\n",
    "    word = re.sub('&.*;', '', word)\n",
    "    # remove problematic characters\n",
    "    word = re.sub('-', '_', word)\n",
    "    word = re.sub('[^a-zA-Z0-9_]', '', word)\n",
    "    word = re.sub('[0-9]+', '_num_', word)\n",
    "    word = re.sub('_+', '_', word)\n",
    "    # remove characters that starts with or ends with _\n",
    "    word = re.sub('^_+', '', word)\n",
    "    word = re.sub('_$', '', word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "def preprocessing(document: str) -> str:\n",
    "    words = document.split()\n",
    "    result = ''\n",
    "    for word in words:\n",
    "        item, cant = word.split(':')\n",
    "        item = remove_characters(item)\n",
    "        item = '_'.join([porter_stemmer.stem(word) for word in item.split('_')])\n",
    "        i, c = 0, int(cant)\n",
    "        while i < c:\n",
    "            result = result + ' ' + item\n",
    "            i += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__model processing methods__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_prediction(training_data, testing_data, training_label):\n",
    "    naive_bayes = MultinomialNB()\n",
    "    naive_bayes.fit(training_data, training_label)\n",
    "    return naive_bayes.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_prediction(training_data, testing_data, training_label):\n",
    "    logistic = LogisticRegression(random_state=0, multi_class='multinomial')\n",
    "    logistic.fit(training_data, training_label)\n",
    "    return logistic.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(testing_label, predictions):\n",
    "    print('Precision score: ', format(precision_score(testing_label, predictions, average='macro')))\n",
    "    print('Recall score: ', format(recall_score(testing_label, predictions, average='macro')))\n",
    "    print('F1 score: ', format(f1_score(testing_label, predictions, average='macro')))\n",
    "    print('Accuracy score: ', format(accuracy_score(testing_label, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__category classifier method__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_classifier(paths: list):\n",
    "    # read the document corpuses\n",
    "    training, testing = pd.DataFrame(), pd.DataFrame()\n",
    "    for path in paths:\n",
    "        new_training = get_training_data(path)\n",
    "        training = pd.concat([training, new_training], axis=0, ignore_index=True)\n",
    "        new_testing = get_testing_data(path)\n",
    "        testing = pd.concat([testing, new_testing], axis=0, ignore_index=True)\n",
    "    print('training shape: ', training.shape)\n",
    "    print('testing shape: ', testing.shape)\n",
    "    # count vectorizer\n",
    "    vectorizer_tf = CountVectorizer()\n",
    "    training_data_tf = vectorizer_tf.fit_transform(training.clean.values)\n",
    "    testing_data_tf = vectorizer_tf.transform(testing.clean.values)\n",
    "    # tfidf vectorizer\n",
    "    vectorizer_tfidf = TfidfVectorizer()\n",
    "    training_data_tfidf = vectorizer_tfidf.fit_transform(training.clean.values)\n",
    "    testing_data_tfidf = vectorizer_tfidf.transform(testing.clean.values)\n",
    "    # make predictions\n",
    "    nb_tf = naive_bayes_prediction(training_data_tf, testing_data_tf, training.label.values)\n",
    "    lr_tf = logistic_regression_prediction(training_data_tf, testing_data_tf, training.label.values)\n",
    "    nb_idf = naive_bayes_prediction(training_data_tfidf, testing_data_tfidf, training.label.values)\n",
    "    lr_idf = logistic_regression_prediction(training_data_tfidf, testing_data_tfidf, training.label.values)\n",
    "    return nb_tf, lr_tf, nb_idf, lr_idf, testing.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(nb_tf, lr_tf, nb_idf, lr_idf, labels):\n",
    "    print('\\nNaive bayes tf metrics: ')\n",
    "    print_metrics(labels, nb_tf)\n",
    "    print('\\nLogistic regression tf metrics: ')\n",
    "    print_metrics(labels, lr_tf)\n",
    "    print('\\nNaive bayes tfidf metrics: ')\n",
    "    print_metrics(labels, nb_idf)\n",
    "    print('\\nLogistic regression tfidf metrics: ')\n",
    "    print_metrics(labels, lr_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__clasiffier by category__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books classifier\n",
      "\n",
      "training shape:  (2000, 3)\n",
      "testing shape:  (4465, 3)\n",
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.8368396694255156\n",
      "Recall score:  0.8328657829801103\n",
      "F1 score:  0.8316437462520466\n",
      "Accuracy score:  0.832026875699888\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.835203031671937\n",
      "Recall score:  0.8350473724599965\n",
      "F1 score:  0.8350968552065444\n",
      "Accuracy score:  0.8351623740201568\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.846643036934299\n",
      "Recall score:  0.8385517424620675\n",
      "F1 score:  0.8366093224911268\n",
      "Accuracy score:  0.8374020156774916\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.8533605268165319\n",
      "Recall score:  0.8532904654646217\n",
      "F1 score:  0.8530782618186867\n",
      "Accuracy score:  0.8530795072788354\n"
     ]
    }
   ],
   "source": [
    "print('Books classifier\\n')\n",
    "nb_tf, lr_tf, nb_idf, lr_idf, labels = category_classifier([books_path])\n",
    "print_predictions(nb_tf, lr_tf, nb_idf, lr_idf, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dvd classifier\n",
      "\n",
      "training shape:  (2000, 3)\n",
      "testing shape:  (3586, 3)\n",
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.8223677169665746\n",
      "Recall score:  0.8223343234868584\n",
      "F1 score:  0.8223458389067001\n",
      "Accuracy score:  0.8223647518126046\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.8337599064874343\n",
      "Recall score:  0.8330821398141572\n",
      "F1 score:  0.8331207811875008\n",
      "Accuracy score:  0.8332403792526492\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.8516176997804146\n",
      "Recall score:  0.8514727717112858\n",
      "F1 score:  0.8513592006576678\n",
      "Accuracy score:  0.8513664249860569\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.8471114855608526\n",
      "Recall score:  0.8468060160770075\n",
      "F1 score:  0.8468479268372847\n",
      "Accuracy score:  0.8469046291132181\n"
     ]
    }
   ],
   "source": [
    "print('Dvd classifier\\n')\n",
    "nb_tf, lr_tf, nb_idf, lr_idf, labels = category_classifier([dvd_path])\n",
    "print_predictions(nb_tf, lr_tf, nb_idf, lr_idf, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electronics classifier\n",
      "\n",
      "training shape:  (2000, 3)\n",
      "testing shape:  (5681, 3)\n",
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.8509050946416147\n",
      "Recall score:  0.8509168376265839\n",
      "F1 score:  0.8509050337588774\n",
      "Accuracy score:  0.8509065305403978\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.8585925303900965\n",
      "Recall score:  0.8582160782968327\n",
      "F1 score:  0.8582464492451272\n",
      "Accuracy score:  0.8582995951417004\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.8615233569670502\n",
      "Recall score:  0.8605738626166436\n",
      "F1 score:  0.860338766379035\n",
      "Accuracy score:  0.8604118993135011\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.8684210330544082\n",
      "Recall score:  0.8682876955462504\n",
      "F1 score:  0.8683112956960536\n",
      "Accuracy score:  0.8683330399577539\n"
     ]
    }
   ],
   "source": [
    "print('Electronics classifier\\n')\n",
    "nb_tf, lr_tf, nb_idf, lr_idf, labels = category_classifier([electronics_path])\n",
    "print_predictions(nb_tf, lr_tf, nb_idf, lr_idf, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kitchen classifier\n",
      "\n",
      "training shape:  (2000, 3)\n",
      "testing shape:  (5945, 3)\n",
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.8756595383090214\n",
      "Recall score:  0.875269908122019\n",
      "F1 score:  0.8753099705235576\n",
      "Accuracy score:  0.8753574432296047\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.8782211775261937\n",
      "Recall score:  0.878235303971042\n",
      "F1 score:  0.8782162137691412\n",
      "Accuracy score:  0.8782169890664424\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.8772637337717557\n",
      "Recall score:  0.8765842211808071\n",
      "F1 score:  0.8766287044604778\n",
      "Accuracy score:  0.8767031118587048\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.8814645466514797\n",
      "Recall score:  0.8811710464274792\n",
      "F1 score:  0.8812090857302257\n",
      "Accuracy score:  0.8812447434819176\n"
     ]
    }
   ],
   "source": [
    "print('Kitchen classifier\\n')\n",
    "nb_tf, lr_tf, nb_idf, lr_idf, labels = category_classifier([kitchen_path])\n",
    "print_predictions(nb_tf, lr_tf, nb_idf, lr_idf, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__complete classifier__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete classifier\n",
      "\n",
      "training shape:  (8000, 3)\n",
      "testing shape:  (19677, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.8601990601990601\n",
      "Recall score:  0.8590536477447666\n",
      "F1 score:  0.8588256274223927\n",
      "Accuracy score:  0.8589215835747319\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.8728142284868005\n",
      "Recall score:  0.8727794560809901\n",
      "F1 score:  0.8727884530633363\n",
      "Accuracy score:  0.8727956497433552\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.8722615321968519\n",
      "Recall score:  0.8689460596756893\n",
      "F1 score:  0.8684633055513065\n",
      "Accuracy score:  0.8687299893276414\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.877392698579508\n",
      "Recall score:  0.8773889166281572\n",
      "F1 score:  0.8773694860386476\n",
      "Accuracy score:  0.8773695177110332\n"
     ]
    }
   ],
   "source": [
    "print('complete classifier\\n')\n",
    "nb_tf, lr_tf, nb_idf, lr_idf, labels = category_classifier([books_path, dvd_path, electronics_path, kitchen_path])\n",
    "print_predictions(nb_tf, lr_tf, nb_idf, lr_idf, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
