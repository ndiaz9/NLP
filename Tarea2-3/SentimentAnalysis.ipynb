{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sentiment classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentiment classifier per each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__paths to change__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input variables\n",
    "path = './datos/Multi Domain Sentiment/'\n",
    "\n",
    "# notebook variables\n",
    "books_path = path + 'books/'\n",
    "dvd_path = path + 'dvd/'\n",
    "electronics_path = path + 'electronics/'\n",
    "kitchen_path = path + 'kitchen/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__preprocessing methods__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_characters(word: str) -> str:\n",
    "    \"\"\"\n",
    "    remove the problematic characters like control characters, and others than alphanumeric\n",
    "    \"\"\"\n",
    "    # remove control characters\n",
    "    word = re.sub('&.*;', '', word)\n",
    "    # remove problematic characters\n",
    "    word = re.sub('-', '_', word)\n",
    "    word = re.sub('[^a-zA-Z0-9_]', '', word)\n",
    "    word = re.sub('[0-9]+', '_num_', word)\n",
    "    word = re.sub('_+', '_', word)\n",
    "    # remove characters that starts with or ends with _\n",
    "    word = re.sub('^_+', '', word)\n",
    "    word = re.sub('_$', '', word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "def preprocessing(document: str) -> dict:\n",
    "    \"\"\"\n",
    "    iterate over all words in document identifing the word and frecuency\n",
    "    alfter that remove all the problematic characters over the word and stemmer the word\n",
    "    and return a dictionary with the word as the key and the frecuency as the value\n",
    "    \"\"\"\n",
    "    words = document.split()\n",
    "    results = {}\n",
    "    for word in words:\n",
    "        item, cant = word.split(':')\n",
    "        item = remove_characters(item)\n",
    "        item = '_'.join([porter_stemmer.stem(word) for word in item.split('_')])\n",
    "        i, c = 0, int(cant)\n",
    "        if results.get(item):\n",
    "            results[item] = results[item] + c\n",
    "        else:\n",
    "            results[item] = c\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Read documents methods__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_training_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    read negative and positive review to extract the training data\n",
    "    refactor the label colummn to get their numeric value\n",
    "    shuffle the data to add random component to the training\n",
    "    and apply the preproccesing method to clean the data\n",
    "    \"\"\"\n",
    "    nroute = path + 'negative.review'\n",
    "    proute = path + 'positive.review'\n",
    "    negative = pd.read_csv(nroute, delimiter='#label#:', engine='python', header=None, names=['data', 'label'])\n",
    "    positive = pd.read_csv(proute, delimiter='#label#:', engine='python', header=None, names=['data', 'label'])\n",
    "    training = pd.concat([negative, positive], axis=0, ignore_index=True)\n",
    "    training['label'] = training.label.map({'negative':0, 'positive':1})\n",
    "    training = shuffle(training, random_state=0)\n",
    "    training['clean'] = training.data.apply(preprocessing)\n",
    "    return training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testing_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    read unlabeled review to extract the testing data\n",
    "    refactor the label colummn to get their numeric value\n",
    "    shuffle the data to add random component to the training\n",
    "    and apply the preproccesing method to clean the data\n",
    "    \"\"\"\n",
    "    route = path + 'unlabeled.review'\n",
    "    testing = pd.read_csv(route, delimiter='#label#:', engine='python', header=None, names=['data', 'label'])\n",
    "    testing['label'] = testing.label.map({'negative':0, 'positive':1})\n",
    "    testing = shuffle(testing, random_state=0)\n",
    "    testing['clean'] = testing.data.apply(preprocessing)\n",
    "    return testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__model processing methods__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_prediction(training_data, testing_data, training_label):\n",
    "    \"\"\"\n",
    "    get the naive bayes prediction from training and testing data\n",
    "    \"\"\"\n",
    "    naive_bayes = MultinomialNB()\n",
    "    naive_bayes.fit(training_data, training_label)\n",
    "    return naive_bayes.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_prediction(training_data, testing_data, training_label):\n",
    "    \"\"\"\n",
    "    get the logistic regression prediction from training and testing data\n",
    "    \"\"\"\n",
    "    logistic = LogisticRegression(random_state=0, multi_class='multinomial')\n",
    "    logistic.fit(training_data, training_label)\n",
    "    return logistic.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(testing_label, predictions):\n",
    "    \"\"\"\n",
    "    extrac the metrics from the testing labels and predictions\n",
    "    \"\"\"\n",
    "    print('Precision score: ', format(precision_score(testing_label, predictions, average='macro')))\n",
    "    print('Recall score: ', format(recall_score(testing_label, predictions, average='macro')))\n",
    "    print('F1 score: ', format(f1_score(testing_label, predictions, average='macro')))\n",
    "    print('Accuracy score: ', format(accuracy_score(testing_label, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__create lexicon from data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lexicon(documents: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    create the lexicon from the input data identifying the positive and negative reviews\n",
    "    extract a dictionary with all the positive and negative frecuencies of a word\n",
    "    normalize the lexicon to have values between 0 and 1\n",
    "    \"\"\"\n",
    "    lexicon = {}\n",
    "    for i in range(len(documents.values)):\n",
    "        label = documents.label.values[i]\n",
    "        for key, value in documents.clean.values[i].items():\n",
    "            if lexicon.get(key):\n",
    "                if label == 1:\n",
    "                    lexicon[key]['pos'] = lexicon[key]['pos'] + value\n",
    "                else:\n",
    "                    lexicon[key]['neg'] = lexicon[key]['neg'] + value\n",
    "            else:\n",
    "                if label == 1:\n",
    "                    lexicon[key] = {'pos': value, 'neg': 0}\n",
    "                else:\n",
    "                    lexicon[key] = {'pos': 0, 'neg': value}\n",
    "    normalized = {}\n",
    "    for key, value in lexicon.items():\n",
    "        total = lexicon[key]['pos'] + lexicon[key]['neg']\n",
    "        normalized[key] = {'pos' : lexicon[key]['pos']/total, 'neg': lexicon[key]['neg']/total}\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexicon_features(document: dict) -> list:\n",
    "    \"\"\"\n",
    "    return the lexicon features between others:\n",
    "        - sum of positive scores\n",
    "        - sum of negative scores\n",
    "        - sum of positive scores divided over amount of words\n",
    "        - sum of negative scores divided over amount of words\n",
    "        - cant of words with positive scores over 0.5\n",
    "        - cant of words with positive scores bellow 0.5\n",
    "        - cant of words with positive scores over 0.75\n",
    "        - cant of words with positive scores bellow 0.75\n",
    "        - cant of words with positive scores over 0.90\n",
    "        - cant of words with positive scores bellow 0.90\n",
    "    \"\"\"\n",
    "    neg_scores, pos_scores, cant_words = [], [], 0\n",
    "    for key, value in document.items():\n",
    "        item = lexicon.get(key, {})\n",
    "        pos_scores.append(item.get('pos', 0))\n",
    "        neg_scores.append(item.get('neg', 0))\n",
    "    cant_words = len(document)\n",
    "    pos_score, neg_score = sum(pos_scores), sum(neg_scores)\n",
    "    len_pos_50, len_neg_50 = len([item for item in pos_scores if item >= 0.5]), len([item for item in pos_scores if item < 0.5])\n",
    "    len_pos_75, len_neg_75 = len([item for item in pos_scores if item >= 0.75]), len([item for item in pos_scores if item < 0.25])\n",
    "    len_pos_90, len_neg_90 = len([item for item in pos_scores if item >= 0.9]), len([item for item in pos_scores if item < 0.1])\n",
    "    pond_pos, pond_neg = sum(pos_scores)/cant_words, sum(neg_scores)/cant_words\n",
    "    result = [pond_pos, pond_neg, pos_score, neg_score, len_pos_50, len_neg_50, len_pos_75, len_neg_75, len_pos_90, len_neg_90]\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_training_data(books_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>foods_after:1 book_but:1 foods_but:1 whiz_and:...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'food_after': 1, 'book_but': 1, 'food_but': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>having_read:1 mccullough's_other:1 read:3 some...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'have_read': 1, 'mccullough_other': 1, 'read'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>a_rather:1 this_series:1 college_and:1 out_the...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'a_rather': 1, 'thi_seri': 1, 'colleg_and': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>and_writes:1 comments:1 found_this:1 smugly_su...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'and_write': 1, 'comment': 1, 'found_thi': 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>lively_writing:1 main_principles:1 catch_of:1 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'live_write': 1, 'main_principl': 1, 'catch_o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>helpful:2 reading_this:1 you_the:1 the_skills:...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'help': 2, 'read_thi': 2, 'you_the': 1, 'the_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>i:7 which:1 me:2 say_about:1 anonymous_here's:...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'i': 7, 'which': 1, 'me': 2, 'say_about': 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>description:1 evolution_of:1 well:1 goes_furth...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'descript': 1, 'evolut_of': 1, 'well': 1, 'go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>worst_book:1 disappointment_for:1 dobbs_book:1...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'worst_book': 1, 'disappoint_for': 1, 'dobb_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>love_in:1 universe_read:1 then_when:1 myself_s...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'love_in': 1, 'univers_read': 1, 'then_when':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   data  label  \\\n",
       "798   foods_after:1 book_but:1 foods_but:1 whiz_and:...      0   \n",
       "1259  having_read:1 mccullough's_other:1 read:3 some...      1   \n",
       "1422  a_rather:1 this_series:1 college_and:1 out_the...      1   \n",
       "610   and_writes:1 comments:1 found_this:1 smugly_su...      0   \n",
       "1940  lively_writing:1 main_principles:1 catch_of:1 ...      1   \n",
       "1486  helpful:2 reading_this:1 you_the:1 the_skills:...      1   \n",
       "261   i:7 which:1 me:2 say_about:1 anonymous_here's:...      0   \n",
       "1465  description:1 evolution_of:1 well:1 goes_furth...      1   \n",
       "721   worst_book:1 disappointment_for:1 dobbs_book:1...      0   \n",
       "1119  love_in:1 universe_read:1 then_when:1 myself_s...      1   \n",
       "\n",
       "                                                  clean  \n",
       "798   {'food_after': 1, 'book_but': 1, 'food_but': 1...  \n",
       "1259  {'have_read': 1, 'mccullough_other': 1, 'read'...  \n",
       "1422  {'a_rather': 1, 'thi_seri': 1, 'colleg_and': 1...  \n",
       "610   {'and_write': 1, 'comment': 1, 'found_thi': 1,...  \n",
       "1940  {'live_write': 1, 'main_principl': 1, 'catch_o...  \n",
       "1486  {'help': 2, 'read_thi': 2, 'you_the': 1, 'the_...  \n",
       "261   {'i': 7, 'which': 1, 'me': 2, 'say_about': 1, ...  \n",
       "1465  {'descript': 1, 'evolut_of': 1, 'well': 1, 'go...  \n",
       "721   {'worst_book': 1, 'disappoint_for': 1, 'dobb_b...  \n",
       "1119  {'love_in': 1, 'univers_read': 1, 'then_when':...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = data.sample(10)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = create_lexicon(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12612151980573033, 0.8738784801942696, 12.486030460767303, 86.51396953923269, 19, 80, 1, 77, 0, 77]\n",
      "[0.9327950310559008, 0.06720496894409937, 64.36285714285715, 4.637142857142857, 68, 1, 59, 0, 59, 0]\n",
      "[0.9583055866289398, 0.04169441337106008, 160.03703296703296, 6.962967032967033, 164, 3, 153, 0, 151, 0]\n",
      "[0.11040241881024003, 0.8895975811897601, 19.762032967032965, 159.23796703296705, 28, 151, 3, 141, 0, 140]\n",
      "[0.9679432890203775, 0.03205671097962235, 260.37674474648156, 8.623255253518412, 266, 3, 252, 0, 247, 0]\n",
      "[0.9139484951695477, 0.08605150483045222, 205.63841141314822, 19.36158858685175, 216, 9, 186, 0, 183, 0]\n",
      "[0.07585790734488604, 0.924142092655114, 21.922935222672066, 267.07706477732796, 32, 257, 2, 248, 0, 248]\n",
      "[0.9570517936439726, 0.042948206356027585, 171.3122710622711, 7.687728937728938, 176, 3, 164, 1, 162, 0]\n",
      "[0.08720484643561567, 0.9127951535643842, 4.534652014652015, 47.46534798534798, 6, 46, 0, 43, 0, 43]\n",
      "[0.9581240919745074, 0.041875908025492514, 200.24793522267206, 8.752064777327936, 205, 4, 191, 0, 188, 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "      <th>clean</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>foods_after:1 book_but:1 foods_but:1 whiz_and:...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'food_after': 1, 'book_but': 1, 'food_but': 1...</td>\n",
       "      <td>[0.12612151980573033, 0.8738784801942696, 12.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>having_read:1 mccullough's_other:1 read:3 some...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'have_read': 1, 'mccullough_other': 1, 'read'...</td>\n",
       "      <td>[0.9327950310559008, 0.06720496894409937, 64.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>a_rather:1 this_series:1 college_and:1 out_the...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'a_rather': 1, 'thi_seri': 1, 'colleg_and': 1...</td>\n",
       "      <td>[0.9583055866289398, 0.04169441337106008, 160....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>and_writes:1 comments:1 found_this:1 smugly_su...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'and_write': 1, 'comment': 1, 'found_thi': 1,...</td>\n",
       "      <td>[0.11040241881024003, 0.8895975811897601, 19.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>lively_writing:1 main_principles:1 catch_of:1 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'live_write': 1, 'main_principl': 1, 'catch_o...</td>\n",
       "      <td>[0.9679432890203775, 0.03205671097962235, 260....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>helpful:2 reading_this:1 you_the:1 the_skills:...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'help': 2, 'read_thi': 2, 'you_the': 1, 'the_...</td>\n",
       "      <td>[0.9139484951695477, 0.08605150483045222, 205....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>i:7 which:1 me:2 say_about:1 anonymous_here's:...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'i': 7, 'which': 1, 'me': 2, 'say_about': 1, ...</td>\n",
       "      <td>[0.07585790734488604, 0.924142092655114, 21.92...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>description:1 evolution_of:1 well:1 goes_furth...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'descript': 1, 'evolut_of': 1, 'well': 1, 'go...</td>\n",
       "      <td>[0.9570517936439726, 0.042948206356027585, 171...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>worst_book:1 disappointment_for:1 dobbs_book:1...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'worst_book': 1, 'disappoint_for': 1, 'dobb_b...</td>\n",
       "      <td>[0.08720484643561567, 0.9127951535643842, 4.53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>love_in:1 universe_read:1 then_when:1 myself_s...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'love_in': 1, 'univers_read': 1, 'then_when':...</td>\n",
       "      <td>[0.9581240919745074, 0.041875908025492514, 200...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   data  label  \\\n",
       "798   foods_after:1 book_but:1 foods_but:1 whiz_and:...      0   \n",
       "1259  having_read:1 mccullough's_other:1 read:3 some...      1   \n",
       "1422  a_rather:1 this_series:1 college_and:1 out_the...      1   \n",
       "610   and_writes:1 comments:1 found_this:1 smugly_su...      0   \n",
       "1940  lively_writing:1 main_principles:1 catch_of:1 ...      1   \n",
       "1486  helpful:2 reading_this:1 you_the:1 the_skills:...      1   \n",
       "261   i:7 which:1 me:2 say_about:1 anonymous_here's:...      0   \n",
       "1465  description:1 evolution_of:1 well:1 goes_furth...      1   \n",
       "721   worst_book:1 disappointment_for:1 dobbs_book:1...      0   \n",
       "1119  love_in:1 universe_read:1 then_when:1 myself_s...      1   \n",
       "\n",
       "                                                  clean  \\\n",
       "798   {'food_after': 1, 'book_but': 1, 'food_but': 1...   \n",
       "1259  {'have_read': 1, 'mccullough_other': 1, 'read'...   \n",
       "1422  {'a_rather': 1, 'thi_seri': 1, 'colleg_and': 1...   \n",
       "610   {'and_write': 1, 'comment': 1, 'found_thi': 1,...   \n",
       "1940  {'live_write': 1, 'main_principl': 1, 'catch_o...   \n",
       "1486  {'help': 2, 'read_thi': 2, 'you_the': 1, 'the_...   \n",
       "261   {'i': 7, 'which': 1, 'me': 2, 'say_about': 1, ...   \n",
       "1465  {'descript': 1, 'evolut_of': 1, 'well': 1, 'go...   \n",
       "721   {'worst_book': 1, 'disappoint_for': 1, 'dobb_b...   \n",
       "1119  {'love_in': 1, 'univers_read': 1, 'then_when':...   \n",
       "\n",
       "                                               features  \n",
       "798   [0.12612151980573033, 0.8738784801942696, 12.4...  \n",
       "1259  [0.9327950310559008, 0.06720496894409937, 64.3...  \n",
       "1422  [0.9583055866289398, 0.04169441337106008, 160....  \n",
       "610   [0.11040241881024003, 0.8895975811897601, 19.7...  \n",
       "1940  [0.9679432890203775, 0.03205671097962235, 260....  \n",
       "1486  [0.9139484951695477, 0.08605150483045222, 205....  \n",
       "261   [0.07585790734488604, 0.924142092655114, 21.92...  \n",
       "1465  [0.9570517936439726, 0.042948206356027585, 171...  \n",
       "721   [0.08720484643561567, 0.9127951535643842, 4.53...  \n",
       "1119  [0.9581240919745074, 0.041875908025492514, 200...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['features'] = test.clean.apply(lexicon_features)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__category classifier method__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_classifier(paths: list):\n",
    "    \"\"\"\n",
    "    process all the data from the paths from params\n",
    "    create the tf and tfidf vectorizers\n",
    "    get the naive bayes and logistic regression predictors\n",
    "    and return those classifier predictors\n",
    "    \"\"\"\n",
    "    # read the document corpuses\n",
    "    training, testing = pd.DataFrame(), pd.DataFrame()\n",
    "    for path in paths:\n",
    "        new_training = get_training_data(path)\n",
    "        training = pd.concat([training, new_training], axis=0, ignore_index=True)\n",
    "        new_testing = get_testing_data(path)\n",
    "        testing = pd.concat([testing, new_testing], axis=0, ignore_index=True)\n",
    "    print('training shape: ', training.shape)\n",
    "    print('testing shape: ', testing.shape)\n",
    "    # tf vectorizer\n",
    "    vectorizer_tf = DictVectorizer()\n",
    "    training_data_tf = vectorizer_tf.fit_transform(training.clean.values)\n",
    "    testing_data_tf = vectorizer_tf.transform(testing.clean.values)\n",
    "    # tfidf vectorizer\n",
    "    vectorizer_tfidf = TfidfTransformer(smooth_idf=False)\n",
    "    training_data_tfidf = vectorizer_tfidf.fit_transform(training_data_tf)\n",
    "    testing_data_tfidf = vectorizer_tfidf.transform(testing_data_tf)\n",
    "    # make predictions\n",
    "    nb_tf = naive_bayes_prediction(training_data_tf, testing_data_tf, training.label.values)\n",
    "    lr_tf = logistic_regression_prediction(training_data_tf, testing_data_tf, training.label.values)\n",
    "    nb_idf = naive_bayes_prediction(training_data_tfidf, testing_data_tfidf, training.label.values)\n",
    "    lr_idf = logistic_regression_prediction(training_data_tfidf, testing_data_tfidf, training.label.values)\n",
    "    return nb_tf, lr_tf, nb_idf, lr_idf, testing.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(nb_tf, lr_tf, nb_idf, lr_idf, labels):\n",
    "    \"\"\"\n",
    "    print the metrics from predictions classifiers\n",
    "    \"\"\"\n",
    "    print('\\nNaive bayes tf metrics: ')\n",
    "    print_metrics(labels, nb_tf)\n",
    "    print('\\nLogistic regression tf metrics: ')\n",
    "    print_metrics(labels, lr_tf)\n",
    "    print('\\nNaive bayes tfidf metrics: ')\n",
    "    print_metrics(labels, nb_idf)\n",
    "    print('\\nLogistic regression tfidf metrics: ')\n",
    "    print_metrics(labels, lr_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__clasiffier by category__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books classifier\n",
      "\n",
      "training shape:  (2000, 3)\n",
      "testing shape:  (4465, 3)\n",
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.8358940198231208\n",
      "Recall score:  0.8313514536437823\n",
      "F1 score:  0.8300080595172086\n",
      "Accuracy score:  0.8304591265397536\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.838528342912385\n",
      "Recall score:  0.8384359502506891\n",
      "F1 score:  0.8384699815477556\n",
      "Accuracy score:  0.838521836506159\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.8441014036371306\n",
      "Recall score:  0.8352453430258973\n",
      "F1 score:  0.8331421200272877\n",
      "Accuracy score:  0.8340425531914893\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.8494902622512832\n",
      "Recall score:  0.8491386022736211\n",
      "F1 score:  0.848808140860472\n",
      "Accuracy score:  0.8488241881298992\n"
     ]
    }
   ],
   "source": [
    "print('Books classifier\\n')\n",
    "nb_tf, lr_tf, nb_idf, lr_idf, labels = category_classifier([books_path])\n",
    "print_predictions(nb_tf, lr_tf, nb_idf, lr_idf, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dvd classifier\n",
      "\n",
      "training shape:  (2000, 3)\n",
      "testing shape:  (3586, 3)\n",
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.822643614054657\n",
      "Recall score:  0.8226632858974203\n",
      "F1 score:  0.8226409107876074\n",
      "Accuracy score:  0.822643614054657\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.8358560784380924\n",
      "Recall score:  0.8353349490598208\n",
      "F1 score:  0.8353765950824774\n",
      "Accuracy score:  0.8354712771890686\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.8502951451182075\n",
      "Recall score:  0.849856111997158\n",
      "F1 score:  0.8496628436558256\n",
      "Accuracy score:  0.8496932515337423\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.8440918159881093\n",
      "Recall score:  0.8437274567426095\n",
      "F1 score:  0.843770616278925\n",
      "Accuracy score:  0.8438371444506414\n"
     ]
    }
   ],
   "source": [
    "print('Dvd classifier\\n')\n",
    "nb_tf, lr_tf, nb_idf, lr_idf, labels = category_classifier([dvd_path])\n",
    "print_predictions(nb_tf, lr_tf, nb_idf, lr_idf, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electronics classifier\n",
      "\n",
      "training shape:  (2000, 3)\n",
      "testing shape:  (5681, 3)\n",
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.8498558577763676\n",
      "Recall score:  0.8498667851239587\n",
      "F1 score:  0.8498497085095853\n",
      "Accuracy score:  0.8498503784544974\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.8589449629502246\n",
      "Recall score:  0.8585681408716328\n",
      "F1 score:  0.8585986319799219\n",
      "Accuracy score:  0.8586516458370005\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.8618312933694336\n",
      "Recall score:  0.8607590967367065\n",
      "F1 score:  0.8605044142759881\n",
      "Accuracy score:  0.8605879246611512\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.8666370357145956\n",
      "Recall score:  0.866533517894025\n",
      "F1 score:  0.8665542253323844\n",
      "Accuracy score:  0.8665727864812534\n"
     ]
    }
   ],
   "source": [
    "print('Electronics classifier\\n')\n",
    "nb_tf, lr_tf, nb_idf, lr_idf, labels = category_classifier([electronics_path])\n",
    "print_predictions(nb_tf, lr_tf, nb_idf, lr_idf, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kitchen classifier\n",
      "\n",
      "training shape:  (2000, 3)\n",
      "testing shape:  (5945, 3)\n",
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.8752071426596483\n",
      "Recall score:  0.8747579343763632\n",
      "F1 score:  0.8747992238201788\n",
      "Accuracy score:  0.8748528174936921\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.8780543578938982\n",
      "Recall score:  0.8780681357998618\n",
      "F1 score:  0.8780481041854347\n",
      "Accuracy score:  0.8780487804878049\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.8776409361086013\n",
      "Recall score:  0.8769164636767445\n",
      "F1 score:  0.8769612003219092\n",
      "Accuracy score:  0.8770395290159798\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.8820583486758249\n",
      "Recall score:  0.8816620817089047\n",
      "F1 score:  0.8817043310095292\n",
      "Accuracy score:  0.8817493692178301\n"
     ]
    }
   ],
   "source": [
    "print('Kitchen classifier\\n')\n",
    "nb_tf, lr_tf, nb_idf, lr_idf, labels = category_classifier([kitchen_path])\n",
    "print_predictions(nb_tf, lr_tf, nb_idf, lr_idf, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__complete classifier__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete classifier\n",
      "\n",
      "training shape:  (8000, 3)\n",
      "testing shape:  (19677, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.8590759757721138\n",
      "Recall score:  0.8576454692166957\n",
      "F1 score:  0.8573751116094139\n",
      "Accuracy score:  0.8574986024292321\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.8729639009833036\n",
      "Recall score:  0.8729330448449437\n",
      "F1 score:  0.8729414014551151\n",
      "Accuracy score:  0.8729481120089444\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.8709388676971344\n",
      "Recall score:  0.8673292012671421\n",
      "F1 score:  0.8668074753843702\n",
      "Accuracy score:  0.8671037251613559\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.8762298669677598\n",
      "Recall score:  0.8762220387401352\n",
      "F1 score:  0.876200524914296\n",
      "Accuracy score:  0.8762006403415155\n"
     ]
    }
   ],
   "source": [
    "print('complete classifier\\n')\n",
    "nb_tf, lr_tf, nb_idf, lr_idf, labels = category_classifier([books_path, dvd_path, electronics_path, kitchen_path])\n",
    "print_predictions(nb_tf, lr_tf, nb_idf, lr_idf, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
