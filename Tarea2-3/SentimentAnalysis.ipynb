{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sentiment classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentiment classifier per each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__paths to change__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input variables\n",
    "path = './datos/Multi Domain Sentiment/'\n",
    "\n",
    "# notebook variables\n",
    "books_path = path + 'books/'\n",
    "dvd_path = path + 'dvd/'\n",
    "electronics_path = path + 'electronics/'\n",
    "kitchen_path = path + 'kitchen/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Read documents methods__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_training_data(path: str) -> pd.DataFrame:\n",
    "    nroute = path + 'negative.review'\n",
    "    proute = path + 'positive.review'\n",
    "    negative = pd.read_csv(nroute, delimiter='#label#:', engine='python', header=None, names=['data', 'label'])\n",
    "    positive = pd.read_csv(proute, delimiter='#label#:', engine='python', header=None, names=['data', 'label'])\n",
    "    training = pd.concat([negative, positive], axis=0, ignore_index=True)\n",
    "    training['label'] = training.label.map({'negative':0, 'positive':1})\n",
    "    training = shuffle(training, random_state=0)\n",
    "    training['clean'] = training.data.apply(preprocessing)\n",
    "    return training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testing_data(path: str) -> pd.DataFrame:\n",
    "    route = path + 'unlabeled.review'\n",
    "    testing = pd.read_csv(route, delimiter='#label#:', engine='python', header=None, names=['data', 'label'])\n",
    "    testing['label'] = testing.label.map({'negative':0, 'positive':1})\n",
    "    testing = shuffle(testing, random_state=0)\n",
    "    testing['clean'] = testing.data.apply(preprocessing)\n",
    "    return testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__preprocessing methods__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_characters(word: str) -> str:\n",
    "    # remove control characters\n",
    "    word = re.sub('&.*;', '', word)\n",
    "    # remove problematic characters\n",
    "    word = re.sub('-', '_', word)\n",
    "    word = re.sub('[^a-zA-Z0-9_]', '', word)\n",
    "    word = re.sub('[0-9]+', '_num_', word)\n",
    "    word = re.sub('_+', '_', word)\n",
    "    # remove characters that starts with or ends with _\n",
    "    word = re.sub('^_+', '', word)\n",
    "    word = re.sub('_$', '', word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "def preprocessing(document: str) -> str:\n",
    "    words = document.split()\n",
    "    result = ''\n",
    "    for word in words:\n",
    "        item, cant = word.split(':')\n",
    "        item = remove_characters(item)\n",
    "        item = '_'.join([porter_stemmer.stem(word) for word in item.split('_')])\n",
    "        i, c = 0, int(cant)\n",
    "        while i < c:\n",
    "            result = result + ' ' + item\n",
    "            i += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__model processing methods__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_prediction(training_data, testing_data, training_label):\n",
    "    naive_bayes = MultinomialNB()\n",
    "    naive_bayes.fit(training_data, training_label)\n",
    "    return naive_bayes.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_prediction(training_data, testing_data, training_label):\n",
    "    logistic = LogisticRegression(random_state=0, multi_class='multinomial')\n",
    "    logistic.fit(training_data, training_label)\n",
    "    return logistic.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(testing_label, predictions):\n",
    "    print('Precision score: ', format(precision_score(testing_label, predictions, average='macro')))\n",
    "    print('Recall score: ', format(recall_score(testing_label, predictions, average='macro')))\n",
    "    print('F1 score: ', format(f1_score(testing_label, predictions, average='macro')))\n",
    "    print('Accuracy score: ', format(accuracy_score(testing_label, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__category classifier method__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_classifier(paths: list):\n",
    "    # read the document corpuses\n",
    "    training, testing = pd.DataFrame(), pd.DataFrame()\n",
    "    for path in paths:\n",
    "        training = get_training_data(path)\n",
    "        training = pd.concat([training, training], axis=0, ignore_index=True)\n",
    "        testing = get_testing_data(path)\n",
    "        testing = pd.concat([testing, testing], axis=0, ignore_index=True)\n",
    "    # count vectorizer\n",
    "    vectorizer_tf = CountVectorizer()\n",
    "    training_data_tf = vectorizer_tf.fit_transform(training.clean.values)\n",
    "    testing_data_tf = vectorizer_tf.transform(testing.clean.values)\n",
    "    # tfidf vectorizer\n",
    "    vectorizer_tfidf = TfidfVectorizer()\n",
    "    training_data_tfidf = vectorizer_tfidf.fit_transform(training.clean.values)\n",
    "    testing_data_tfidf = vectorizer_tfidf.transform(testing.clean.values)\n",
    "    # make predictions\n",
    "    nb_tf = naive_bayes_prediction(training_data_tf, testing_data_tf, training.label.values)\n",
    "    lr_tf = logistic_regression_prediction(training_data_tf, testing_data_tf, training.label.values)\n",
    "    nb_idf = naive_bayes_prediction(training_data_tfidf, testing_data_tfidf, training.label.values)\n",
    "    lr_idf = logistic_regression_prediction(training_data_tfidf, testing_data_tfidf, training.label.values)\n",
    "    return nb_tf, lr_tf, nb_idf, lr_idf, testing.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(nb_tf, lr_tf, nb_idf, lr_idf, labels):\n",
    "    print('\\nNaive bayes tf metrics: ')\n",
    "    print_metrics(labels, nb_tf)\n",
    "    print('\\nLogistic regression tf metrics: ')\n",
    "    print_metrics(labels, lr_tf)\n",
    "    print('\\nNaive bayes tfidf metrics: ')\n",
    "    print_metrics(labels, nb_idf)\n",
    "    print('\\nLogistic regression tfidf metrics: ')\n",
    "    print_metrics(labels, lr_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__clasiffier by category__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books classifier\n",
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.8293096155530433\n",
      "Recall score:  0.8274765284973262\n",
      "F1 score:  0.8267101654596601\n",
      "Accuracy score:  0.8268756998880179\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.8347376151179817\n",
      "Recall score:  0.8346119977588087\n",
      "F1 score:  0.8346545235586331\n",
      "Accuracy score:  0.8347144456886898\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.8489680669537661\n",
      "Recall score:  0.8447477295093943\n",
      "F1 score:  0.843530797474797\n",
      "Accuracy score:  0.8438969764837626\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.8549659325038987\n",
      "Recall score:  0.8550066786218279\n",
      "F1 score:  0.85487033976147\n",
      "Accuracy score:  0.8548712206047032\n"
     ]
    }
   ],
   "source": [
    "print('Books classifier')\n",
    "nb_tf, lr_tf, nb_idf, lr_idf, labels = category_classifier([books_path])\n",
    "print_predictions(nb_tf, lr_tf, nb_idf, lr_idf, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dvd classifier\n",
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.8196788123148908\n",
      "Recall score:  0.819645697373869\n",
      "F1 score:  0.8195749929136973\n",
      "Accuracy score:  0.8195761293920804\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.8358822735780478\n",
      "Recall score:  0.8353305940019031\n",
      "F1 score:  0.8353721373719114\n",
      "Accuracy score:  0.8354712771890686\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.8452684021789713\n",
      "Recall score:  0.8452764575212317\n",
      "F1 score:  0.8452313473418289\n",
      "Accuracy score:  0.8452314556609035\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.8508615343500203\n",
      "Recall score:  0.8504074934370833\n",
      "F1 score:  0.8504554113297786\n",
      "Accuracy score:  0.8505298382598996\n"
     ]
    }
   ],
   "source": [
    "print('Dvd classifier')\n",
    "nb_tf, lr_tf, nb_idf, lr_idf, labels = category_classifier([dvd_path])\n",
    "print_predictions(nb_tf, lr_tf, nb_idf, lr_idf, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electronics classifier\n",
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.8480899214267724\n",
      "Recall score:  0.848077841215007\n",
      "F1 score:  0.8480825935393668\n",
      "Accuracy score:  0.8480901249779969\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.8582400978299682\n",
      "Recall score:  0.8578640157220325\n",
      "F1 score:  0.8578942665103324\n",
      "Accuracy score:  0.8579475444464003\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.8669000825222617\n",
      "Recall score:  0.8663471063071567\n",
      "F1 score:  0.8661833154314358\n",
      "Accuracy score:  0.8662207357859532\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.8705341732827063\n",
      "Recall score:  0.8704000709950511\n",
      "F1 score:  0.8704239487062773\n",
      "Accuracy score:  0.8704453441295547\n"
     ]
    }
   ],
   "source": [
    "print('Electronics classifier')\n",
    "nb_tf, lr_tf, nb_idf, lr_idf, labels = category_classifier([electronics_path])\n",
    "print_predictions(nb_tf, lr_tf, nb_idf, lr_idf, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kitchen classifier\n",
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.8743267651888342\n",
      "Recall score:  0.8741185755415648\n",
      "F1 score:  0.8741504961801903\n",
      "Accuracy score:  0.8741799831791421\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.8783880825057295\n",
      "Recall score:  0.8784024721422221\n",
      "F1 score:  0.8783843167466591\n",
      "Accuracy score:  0.8783851976450799\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.8799720656525165\n",
      "Recall score:  0.8794616754800624\n",
      "F1 score:  0.8795059239575722\n",
      "Accuracy score:  0.8795626576955424\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.8818853764599545\n",
      "Recall score:  0.8816913955588273\n",
      "F1 score:  0.881723453677926\n",
      "Accuracy score:  0.8817493692178301\n"
     ]
    }
   ],
   "source": [
    "print('Kitchen classifier')\n",
    "nb_tf, lr_tf, nb_idf, lr_idf, labels = category_classifier([kitchen_path])\n",
    "print_predictions(nb_tf, lr_tf, nb_idf, lr_idf, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__complete classifier__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete classifier\n",
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.8743267651888342\n",
      "Recall score:  0.8741185755415648\n",
      "F1 score:  0.8741504961801903\n",
      "Accuracy score:  0.8741799831791421\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.8783880825057295\n",
      "Recall score:  0.8784024721422221\n",
      "F1 score:  0.8783843167466591\n",
      "Accuracy score:  0.8783851976450799\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.8799720656525165\n",
      "Recall score:  0.8794616754800624\n",
      "F1 score:  0.8795059239575722\n",
      "Accuracy score:  0.8795626576955424\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.8818853764599545\n",
      "Recall score:  0.8816913955588273\n",
      "F1 score:  0.881723453677926\n",
      "Accuracy score:  0.8817493692178301\n"
     ]
    }
   ],
   "source": [
    "print('complete classifier')\n",
    "nb_tf, lr_tf, nb_idf, lr_idf, labels = category_classifier([books_path, dvd_path, electronics_path, kitchen_path])\n",
    "print_predictions(nb_tf, lr_tf, nb_idf, lr_idf, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
