{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sentiment classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__paths to change__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input variables\n",
    "path = './datos/Multi Domain Sentiment/'\n",
    "\n",
    "# notebook variables\n",
    "books_path = path + 'books/'\n",
    "dvd_path = path + 'dvd/'\n",
    "electronics_path = path + 'electronics/'\n",
    "kitchen_path = path + 'kitchen/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__preprocessing methods__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_characters(word: str) -> str:\n",
    "    \"\"\"\n",
    "    remove the problematic characters like control characters, and others than alphanumeric\n",
    "    \"\"\"\n",
    "    # remove control characters\n",
    "    word = re.sub('&.*;', '', word)\n",
    "    # remove problematic characters\n",
    "    word = re.sub('-', '_', word)\n",
    "    word = re.sub('[^a-zA-Z0-9_]', '', word)\n",
    "    word = re.sub('[0-9]+', '_num_', word)\n",
    "    word = re.sub('_+', '_', word)\n",
    "    # remove characters that starts with or ends with _\n",
    "    word = re.sub('^_+', '', word)\n",
    "    word = re.sub('_$', '', word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document: str) -> dict:\n",
    "    \"\"\"\n",
    "    iterate over all words in document identifing the word and frecuency\n",
    "    remove all the problematic characters over the word\n",
    "    and return a dictionary with the word as the key and the frecuency as the value\n",
    "    \"\"\"\n",
    "    words = document.split()\n",
    "    results = {}\n",
    "    for word in words:\n",
    "        item, cant = word.split(':')\n",
    "        item = remove_characters(item)\n",
    "        results[item] = int(cant)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__create lexicon from data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lexicon(documents: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    create the lexicon from the input data identifying the positive and negative reviews\n",
    "    extract a dictionary with all the positive and negative frecuencies of a word\n",
    "    normalize the lexicon to have values between 0 and 1\n",
    "    \"\"\"\n",
    "    lexicon = {}\n",
    "    for i in range(len(documents.values)):\n",
    "        label = documents.label.values[i]\n",
    "        for key, value in documents.clean.values[i].items():\n",
    "            if lexicon.get(key):\n",
    "                if label == 1:\n",
    "                    lexicon[key]['pos'] = lexicon[key]['pos'] + value\n",
    "                else:\n",
    "                    lexicon[key]['neg'] = lexicon[key]['neg'] + value\n",
    "            else:\n",
    "                if label == 1:\n",
    "                    lexicon[key] = {'pos': value, 'neg': 0}\n",
    "                else:\n",
    "                    lexicon[key] = {'pos': 0, 'neg': value}\n",
    "    normalized = {}\n",
    "    for key, value in lexicon.items():\n",
    "        total = lexicon[key]['pos'] + lexicon[key]['neg']\n",
    "        normalized[key] = {'pos' : lexicon[key]['pos']/total, 'neg': lexicon[key]['neg']/total}\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexicon_features(document: dict, lexicon: dict) -> list:\n",
    "    \"\"\"\n",
    "    return the lexicon features between others:\n",
    "        - sum of positive scores\n",
    "        - sum of negative scores\n",
    "        - sum of positive scores divided over amount of words\n",
    "        - sum of negative scores divided over amount of words\n",
    "        - cant of words with positive scores over 0.5\n",
    "        - cant of words with negative scores over 0.5\n",
    "    \"\"\"\n",
    "    neg_scores, pos_scores, cant_words = [], [], 0\n",
    "    for key, value in document.items():\n",
    "        item = lexicon.get(key, {})\n",
    "        pos_scores.append(item.get('pos', 0))\n",
    "        neg_scores.append(item.get('neg', 0))\n",
    "    cant_words = len(document)\n",
    "    pos_score, neg_score = sum(pos_scores), sum(neg_scores)\n",
    "    pond_pos, pond_neg = pos_score/cant_words, neg_score/cant_words\n",
    "    cant_pos, cant_neg = len([item for item in pos_scores if item >= 0.5]), len([item for item in neg_scores if item >= 0.5])\n",
    "    return [pond_pos, pond_neg, pos_score, neg_score, cant_pos, cant_neg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Read documents methods__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_training_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    read negative and positive review to extract the training data\n",
    "    refactor the label colummn to get their numeric value\n",
    "    shuffle the data to add random component to the training\n",
    "    and apply the preproccesing method to clean the data\n",
    "    \"\"\"\n",
    "    nroute = path + 'negative.review'\n",
    "    proute = path + 'positive.review'\n",
    "    negative = pd.read_csv(nroute, delimiter='#label#:', engine='python', header=None, names=['data', 'label'])\n",
    "    positive = pd.read_csv(proute, delimiter='#label#:', engine='python', header=None, names=['data', 'label'])\n",
    "    training = pd.concat([negative, positive], axis=0, ignore_index=True)\n",
    "    training['label'] = training.label.map({'negative':0, 'positive':1})\n",
    "    training = shuffle(training, random_state=0)\n",
    "    training['clean'] = training.data.apply(preprocessing)\n",
    "    return training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testing_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    read unlabeled review to extract the testing data\n",
    "    refactor the label colummn to get their numeric value\n",
    "    shuffle the data to add random component to the training\n",
    "    and apply the preproccesing method to clean the data\n",
    "    \"\"\"\n",
    "    route = path + 'unlabeled.review'\n",
    "    testing = pd.read_csv(route, delimiter='#label#:', engine='python', header=None, names=['data', 'label'])\n",
    "    testing['label'] = testing.label.map({'negative':0, 'positive':1})\n",
    "    testing = shuffle(testing, random_state=0)\n",
    "    testing['clean'] = testing.data.apply(preprocessing)\n",
    "    return testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__model processing methods__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_prediction(training_data, testing_data, training_label):\n",
    "    \"\"\"\n",
    "    get the naive bayes prediction from training and testing data\n",
    "    \"\"\"\n",
    "    naive_bayes = MultinomialNB()\n",
    "    naive_bayes.fit(training_data, training_label)\n",
    "    return naive_bayes.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_prediction(training_data, testing_data, training_label):\n",
    "    \"\"\"\n",
    "    get the logistic regression prediction from training and testing data\n",
    "    \"\"\"\n",
    "    logistic = LogisticRegression(random_state=0, multi_class='multinomial')\n",
    "    logistic.fit(training_data, training_label)\n",
    "    return logistic.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(testing_label, predictions):\n",
    "    \"\"\"\n",
    "    extrac the metrics from the testing labels and predictions\n",
    "    \"\"\"\n",
    "    print('Precision score: ', format(precision_score(testing_label, predictions, average='macro')))\n",
    "    print('Recall score: ', format(recall_score(testing_label, predictions, average='macro')))\n",
    "    print('F1 score: ', format(f1_score(testing_label, predictions, average='macro')))\n",
    "    print('Accuracy score: ', format(accuracy_score(testing_label, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__category classifier method__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_classifier(paths: list):\n",
    "    \"\"\"\n",
    "    process all the data from the paths from params\n",
    "    create the tf and tfidf vectorizers\n",
    "    get the naive bayes and logistic regression predictors\n",
    "    and return those classifier predictors\n",
    "    \"\"\"\n",
    "    # read the document corpuses\n",
    "    start = time()\n",
    "    training, testing = pd.DataFrame(), pd.DataFrame()\n",
    "    for path in paths:\n",
    "        new_training = get_training_data(path)\n",
    "        training = pd.concat([training, new_training], axis=0, ignore_index=True)\n",
    "        new_testing = get_testing_data(path)\n",
    "        testing = pd.concat([testing, new_testing], axis=0, ignore_index=True)\n",
    "    end = time()\n",
    "    print('\\ngetting training and testing data elapsed time: ' + str(end-start))\n",
    "\n",
    "    print('\\ntraining shape: ', training.shape)\n",
    "    print('\\ntesting shape: ', testing.shape)\n",
    "\n",
    "    # tf vectorizer\n",
    "    start = time()\n",
    "    vectorizer_tf = DictVectorizer()\n",
    "    training_data_tf = vectorizer_tf.fit_transform(training.clean.values)\n",
    "    testing_data_tf = vectorizer_tf.transform(testing.clean.values)\n",
    "    end = time()\n",
    "    print('\\nvectorizer tf data elapsed time: ' + str(end-start))\n",
    "\n",
    "    # tfidf vectorizer\n",
    "    start = time()\n",
    "    vectorizer_tfidf = TfidfTransformer(smooth_idf=False)\n",
    "    training_data_tfidf = vectorizer_tfidf.fit_transform(training_data_tf)\n",
    "    testing_data_tfidf = vectorizer_tfidf.transform(testing_data_tf)\n",
    "    end = time()\n",
    "    print('\\nvectorizer tfidf data elapsed time: ' + str(end-start))\n",
    "\n",
    "    # features matrix\n",
    "    start = time()\n",
    "    lexicon = create_lexicon(training)\n",
    "    training_data_feat = csr_matrix(list(training.clean.apply(lexicon_features, lexicon=lexicon)))\n",
    "    testing_data_feat = csr_matrix(list(testing.clean.apply(lexicon_features, lexicon=lexicon)))\n",
    "    end = time()\n",
    "    print('\\nlexicon features matrix data elapsed time: ' + str(end-start))\n",
    "\n",
    "    # make predictions\n",
    "    start = time()\n",
    "    nb_tf = naive_bayes_prediction(training_data_tf, testing_data_tf, training.label.values)\n",
    "    end = time()\n",
    "    print('\\nnaive bayes prediction ft: ' + str(end-start))\n",
    "\n",
    "    start = time()\n",
    "    lr_tf = logistic_regression_prediction(training_data_tf, testing_data_tf, training.label.values)\n",
    "    end = time()\n",
    "    print('\\nlogistic regression prediction ft: ' + str(end-start))\n",
    "\n",
    "    start = time()\n",
    "    nb_idf = naive_bayes_prediction(training_data_tfidf, testing_data_tfidf, training.label.values)\n",
    "    end = time()\n",
    "    print('\\nnaive bayes prediction ftidf: ' + str(end-start))\n",
    "    \n",
    "    start = time()\n",
    "    lr_idf = logistic_regression_prediction(training_data_tfidf, testing_data_tfidf, training.label.values)\n",
    "    end = time()\n",
    "    print('\\nlogistic regression prediction ftidf: ' + str(end-start))\n",
    "    \n",
    "    start = time()\n",
    "    nb_feat = naive_bayes_prediction(training_data_feat, testing_data_feat, training.label.values)\n",
    "    end = time()\n",
    "    print('\\nnaive bayes prediction features matrix: ' + str(end-start))\n",
    "    \n",
    "    start = time()\n",
    "    lr_feat = logistic_regression_prediction(training_data_feat, testing_data_feat, training.label.values)\n",
    "    end = time()\n",
    "    print('\\nlogistic regression prediction feature matrix: ' + str(end-start))\n",
    "    \n",
    "    print('\\nNaive bayes tf metrics: ')\n",
    "    print_metrics(testing.label.values, nb_tf)\n",
    "\n",
    "    print('\\nLogistic regression tf metrics: ')\n",
    "    print_metrics(testing.label.values, lr_tf)\n",
    "\n",
    "    print('\\nNaive bayes tfidf metrics: ')\n",
    "    print_metrics(testing.label.values, nb_idf)\n",
    "\n",
    "    print('\\nLogistic regression tfidf metrics: ')\n",
    "    print_metrics(testing.label.values, lr_idf)\n",
    "\n",
    "    print('\\nNaive bayes features metrics: ')\n",
    "    print_metrics(testing.label.values, nb_feat)\n",
    "\n",
    "    print('\\nLogistic regression features metrics: ')\n",
    "    print_metrics(testing.label.values, lr_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentiment classifier per each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books classifier\n",
      "\n",
      "\n",
      "getting training and testing data elapsed time: 32.29820418357849\n",
      "\n",
      "training shape:  (2000, 3)\n",
      "\n",
      "testing shape:  (4465, 3)\n",
      "\n",
      "vectorizer tf data elapsed time: 4.060862064361572\n",
      "\n",
      "vectorizer tfidf data elapsed time: 0.14737963676452637\n",
      "\n",
      "lexicon features matrix data elapsed time: 3.4545013904571533\n",
      "\n",
      "naive bayes prediction ft: 0.04754948616027832\n",
      "\n",
      "logistic regression prediction ft: 11.679904460906982\n",
      "\n",
      "naive bayes prediction ftidf: 0.05852341651916504\n",
      "\n",
      "logistic regression prediction ftidf: 5.159057855606079\n",
      "\n",
      "naive bayes prediction features matrix: 0.00766444206237793\n",
      "\n",
      "logistic regression prediction feature matrix: 0.040157318115234375\n",
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.8350342656009614\n",
      "Recall score:  0.8294270352538117\n",
      "F1 score:  0.8278642502163154\n",
      "Accuracy score:  0.8284434490481523\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.831096062202157\n",
      "Recall score:  0.8311416429730785\n",
      "F1 score:  0.8311106790147239\n",
      "Accuracy score:  0.8311310190369541\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.8431263645885044\n",
      "Recall score:  0.8309611315447685\n",
      "F1 score:  0.8282529553366651\n",
      "Accuracy score:  0.8295632698768197\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.8485922573551266\n",
      "Recall score:  0.8478830494651484\n",
      "F1 score:  0.8474384981718449\n",
      "Accuracy score:  0.8474804031354983\n",
      "\n",
      "Naive bayes features metrics: \n",
      "Precision score:  0.8208680216745905\n",
      "Recall score:  0.8163586700873198\n",
      "F1 score:  0.8149408983451537\n",
      "Accuracy score:  0.8154535274356103\n",
      "\n",
      "Logistic regression features metrics: \n",
      "Precision score:  0.8289145096235203\n",
      "Recall score:  0.8053363352347069\n",
      "F1 score:  0.8032498563793784\n",
      "Accuracy score:  0.8071668533034715\n"
     ]
    }
   ],
   "source": [
    "print('Books classifier\\n')\n",
    "category_classifier([books_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dvd classifier\n",
      "\n",
      "\n",
      "getting training and testing data elapsed time: 23.63008952140808\n",
      "\n",
      "training shape:  (2000, 3)\n",
      "\n",
      "testing shape:  (3586, 3)\n",
      "\n",
      "vectorizer tf data elapsed time: 4.0336689949035645\n",
      "\n",
      "vectorizer tfidf data elapsed time: 0.14813947677612305\n",
      "\n",
      "lexicon features matrix data elapsed time: 3.099989891052246\n",
      "\n",
      "naive bayes prediction ft: 0.059864044189453125\n",
      "\n",
      "logistic regression prediction ft: 9.384239673614502\n",
      "\n",
      "naive bayes prediction ftidf: 0.04687976837158203\n",
      "\n",
      "logistic regression prediction ftidf: 4.167803764343262\n",
      "\n",
      "naive bayes prediction features matrix: 0.0004978179931640625\n",
      "\n",
      "logistic regression prediction feature matrix: 0.0634918212890625\n",
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.8213267556139578\n",
      "Recall score:  0.8213102627250903\n",
      "F1 score:  0.8212489553338185\n",
      "Accuracy score:  0.8212493028443949\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.8258791154852412\n",
      "Recall score:  0.8252822310837282\n",
      "F1 score:  0.8253172610360089\n",
      "Accuracy score:  0.8254322364751813\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.8496766636478628\n",
      "Recall score:  0.8487972107720492\n",
      "F1 score:  0.8485079550910272\n",
      "Accuracy score:  0.8485778025655326\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.8456878503305423\n",
      "Recall score:  0.8454181524413367\n",
      "F1 score:  0.8454579682365828\n",
      "Accuracy score:  0.8455103179029559\n",
      "\n",
      "Naive bayes features metrics: \n",
      "Precision score:  0.805902240659099\n",
      "Recall score:  0.8059174660531012\n",
      "F1 score:  0.8059058421046247\n",
      "Accuracy score:  0.8059118795315114\n",
      "\n",
      "Logistic regression features metrics: \n",
      "Precision score:  0.7897970091574595\n",
      "Recall score:  0.7761030506247486\n",
      "F1 score:  0.7727385237639193\n",
      "Accuracy score:  0.7752370329057445\n"
     ]
    }
   ],
   "source": [
    "print('Dvd classifier\\n')\n",
    "category_classifier([dvd_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electronics classifier\n",
      "\n",
      "\n",
      "getting training and testing data elapsed time: 24.97767925262451\n",
      "\n",
      "training shape:  (2000, 3)\n",
      "\n",
      "testing shape:  (5681, 3)\n",
      "\n",
      "vectorizer tf data elapsed time: 4.075480222702026\n",
      "\n",
      "vectorizer tfidf data elapsed time: 0.09634923934936523\n",
      "\n",
      "lexicon features matrix data elapsed time: 3.1928648948669434\n",
      "\n",
      "naive bayes prediction ft: 0.04696822166442871\n",
      "\n",
      "logistic regression prediction ft: 6.6632301807403564\n",
      "\n",
      "naive bayes prediction ftidf: 0.029967069625854492\n",
      "\n",
      "logistic regression prediction ftidf: 1.8210182189941406\n",
      "\n",
      "naive bayes prediction features matrix: 0.008457422256469727\n",
      "\n",
      "logistic regression prediction feature matrix: 0.05720663070678711\n",
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.8545982734118966\n",
      "Recall score:  0.8546083819771726\n",
      "F1 score:  0.8546006795972588\n",
      "Accuracy score:  0.8546030628410491\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.8601032371353003\n",
      "Recall score:  0.8598116077900213\n",
      "F1 score:  0.85984125941143\n",
      "Accuracy score:  0.8598838232705509\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.8704260224170581\n",
      "Recall score:  0.869867732055158\n",
      "F1 score:  0.8697048071306086\n",
      "Accuracy score:  0.8697412427389544\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.8711584399348112\n",
      "Recall score:  0.870908736654963\n",
      "F1 score:  0.8709395732511002\n",
      "Accuracy score:  0.8709734201725048\n",
      "\n",
      "Naive bayes features metrics: \n",
      "Precision score:  0.8326503008310201\n",
      "Recall score:  0.8325605019627752\n",
      "F1 score:  0.8325772972882518\n",
      "Accuracy score:  0.8325998943847914\n",
      "\n",
      "Logistic regression features metrics: \n",
      "Precision score:  0.8104622183583279\n",
      "Recall score:  0.7921258580634414\n",
      "F1 score:  0.7884319937924829\n",
      "Accuracy score:  0.791409963034677\n"
     ]
    }
   ],
   "source": [
    "print('Electronics classifier\\n')\n",
    "category_classifier([electronics_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kitchen classifier\n",
      "\n",
      "\n",
      "getting training and testing data elapsed time: 19.120525360107422\n",
      "\n",
      "training shape:  (2000, 3)\n",
      "\n",
      "testing shape:  (5945, 3)\n",
      "\n",
      "vectorizer tf data elapsed time: 3.0594944953918457\n",
      "\n",
      "vectorizer tfidf data elapsed time: 0.06249809265136719\n",
      "\n",
      "lexicon features matrix data elapsed time: 2.0406932830810547\n",
      "\n",
      "naive bayes prediction ft: 0.015642404556274414\n",
      "\n",
      "logistic regression prediction ft: 3.996230363845825\n",
      "\n",
      "naive bayes prediction ftidf: 0.03127288818359375\n",
      "\n",
      "logistic regression prediction ftidf: 2.586960792541504\n",
      "\n",
      "naive bayes prediction features matrix: 0.0\n",
      "\n",
      "logistic regression prediction feature matrix: 0.07965326309204102\n",
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.8800992246016999\n",
      "Recall score:  0.8796351251905117\n",
      "F1 score:  0.8796785203566759\n",
      "Accuracy score:  0.87973086627418\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.8804134439810729\n",
      "Recall score:  0.8803854578857312\n",
      "F1 score:  0.8803952403083994\n",
      "Accuracy score:  0.8804037005887301\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.8826336796981646\n",
      "Recall score:  0.8821573046831761\n",
      "F1 score:  0.8822019169256725\n",
      "Accuracy score:  0.8822539949537427\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.8834154351395731\n",
      "Recall score:  0.8832021906387182\n",
      "F1 score:  0.8832358881671818\n",
      "Accuracy score:  0.8832632464255677\n",
      "\n",
      "Naive bayes features metrics: \n",
      "Precision score:  0.8601792637710846\n",
      "Recall score:  0.8599906014590827\n",
      "F1 score:  0.8600190903640753\n",
      "Accuracy score:  0.8600504625735913\n",
      "\n",
      "Logistic regression features metrics: \n",
      "Precision score:  0.8483000092401719\n",
      "Recall score:  0.8336209259690605\n",
      "F1 score:  0.8312960003127522\n",
      "Accuracy score:  0.8329688814129521\n"
     ]
    }
   ],
   "source": [
    "print('Kitchen classifier\\n')\n",
    "category_classifier([kitchen_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentiment classifier complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete classifier\n",
      "\n",
      "\n",
      "getting training and testing data elapsed time: 97.85711336135864\n",
      "\n",
      "training shape:  (8000, 3)\n",
      "\n",
      "testing shape:  (19677, 3)\n",
      "\n",
      "vectorizer tf data elapsed time: 14.482530355453491\n",
      "\n",
      "vectorizer tfidf data elapsed time: 0.573575496673584\n",
      "\n",
      "lexicon features matrix data elapsed time: 12.555979013442993\n",
      "\n",
      "naive bayes prediction ft: 0.13494586944580078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "logistic regression prediction ft: 30.15519070625305\n",
      "\n",
      "naive bayes prediction ftidf: 0.12407326698303223\n",
      "\n",
      "logistic regression prediction ftidf: 17.98591637611389\n",
      "\n",
      "naive bayes prediction features matrix: 0.0\n",
      "\n",
      "logistic regression prediction feature matrix: 0.22345662117004395\n",
      "\n",
      "Naive bayes tf metrics: \n",
      "Precision score:  0.8631942892515936\n",
      "Recall score:  0.8617613567508546\n",
      "F1 score:  0.8614964006712913\n",
      "Accuracy score:  0.8616150836001423\n",
      "\n",
      "Logistic regression tf metrics: \n",
      "Precision score:  0.8747372744108918\n",
      "Recall score:  0.8747142777887805\n",
      "F1 score:  0.8747212006666737\n",
      "Accuracy score:  0.8747268384408192\n",
      "\n",
      "Naive bayes tfidf metrics: \n",
      "Precision score:  0.8752871757223706\n",
      "Recall score:  0.8717473228506794\n",
      "F1 score:  0.8712479406019544\n",
      "Accuracy score:  0.8715251308634446\n",
      "\n",
      "Logistic regression tfidf metrics: \n",
      "Precision score:  0.8753261141773242\n",
      "Recall score:  0.8752285235301829\n",
      "F1 score:  0.8751795052716007\n",
      "Accuracy score:  0.875184225237587\n",
      "\n",
      "Naive bayes features metrics: \n",
      "Precision score:  0.8396858591306806\n",
      "Recall score:  0.838526155340522\n",
      "F1 score:  0.8382713331994831\n",
      "Accuracy score:  0.8383899984753773\n",
      "\n",
      "Logistic regression features metrics: \n",
      "Precision score:  0.8433604452926791\n",
      "Recall score:  0.8413263027460636\n",
      "F1 score:  0.841226304186909\n",
      "Accuracy score:  0.8414900645423591\n"
     ]
    }
   ],
   "source": [
    "print('complete classifier\\n')\n",
    "category_classifier([books_path, dvd_path, electronics_path, kitchen_path])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
