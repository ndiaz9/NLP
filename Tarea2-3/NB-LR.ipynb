{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from gensim.parsing.porter import PorterStemmer \n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(tmpdir,label,df):\n",
    "    for filename in sorted(os.listdir(tmpdir)):\n",
    "        with open(tmpdir+filename, encoding=\"utf8\", errors='ignore') as f:\n",
    "            lines = f.read()\n",
    "            df.loc[len(df)] = [label,lines]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject: A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject: A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>From: I3150101@dbstu1.rz.tu-bs.de (Benedikt Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject: R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>From: strom@Watson.Ibm.Com (Rob Strom)\\nSubjec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                          documents\n",
       "0      0  From: mathew <mathew@mantis.co.uk>\\nSubject: A...\n",
       "1      0  From: mathew <mathew@mantis.co.uk>\\nSubject: A...\n",
       "2      0  From: I3150101@dbstu1.rz.tu-bs.de (Benedikt Ro...\n",
       "3      0  From: mathew <mathew@mantis.co.uk>\\nSubject: R...\n",
       "4      0  From: strom@Watson.Ibm.Com (Rob Strom)\\nSubjec..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate corpus for 20N\n",
    "path = 'Datasets/20news-18828/'\n",
    "labels = {}\n",
    "df = df = pd.DataFrame({\"label\": [], \"documents\": [] })\n",
    "def get_senteces_from_path_20N(path)->list:\n",
    "    for dirs in sorted(os.listdir(path)):\n",
    "        labels[dirs] = len(labels)\n",
    "        tmpdir = path+dirs+'/'\n",
    "        if not dirs.startswith('.'):\n",
    "            get_documents(tmpdir,labels[dirs],df)\n",
    "\n",
    "get_senteces_from_path_20N(path)\n",
    "df.head()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document: str) -> list:\n",
    "    \"\"\"\n",
    "    clean data by removing non-latin characters\n",
    "    stem data sentences\n",
    "    remove stop words from a document\n",
    "    \"\"\"\n",
    "    document = document.lower()\n",
    "    document = remove_stopwords(document)\n",
    "    document = re.sub('\\n', ' ', document)\n",
    "    document = re.sub('[^a-zA-Z]|[0-9]', ' ', document)\n",
    "    document = re.sub('\\s+', ' ', document)\n",
    "    p = PorterStemmer()\n",
    "    document = p.stem_sentence(document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>documents</th>\n",
       "      <th>documents_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject: A...</td>\n",
       "      <td>from mathew mathew manti co uk subject alt ath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject: A...</td>\n",
       "      <td>from mathew mathew manti co uk subject alt ath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>From: I3150101@dbstu1.rz.tu-bs.de (Benedikt Ro...</td>\n",
       "      <td>from i dbstu rz tu bs de benedikt rosenau subj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject: R...</td>\n",
       "      <td>from mathew mathew manti co uk subject re univ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>From: strom@Watson.Ibm.Com (Rob Strom)\\nSubjec...</td>\n",
       "      <td>from strom watson ibm com rob strom subject re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18823</th>\n",
       "      <td>19</td>\n",
       "      <td>From: sbuckley@fraser.sfu.ca (Stephen Buckley)...</td>\n",
       "      <td>from sbucklei fraser sfu ca stephen bucklei su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18824</th>\n",
       "      <td>19</td>\n",
       "      <td>From: bakerj@gtephx.UUCP (Jon Baker)\\nSubject:...</td>\n",
       "      <td>from bakerj gtephx uucp jon baker subject re m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18825</th>\n",
       "      <td>19</td>\n",
       "      <td>From: pharvey@quack.kfu.com (Paul Harvey)\\nSub...</td>\n",
       "      <td>from pharvei quack kfu com paul harvei subject...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18826</th>\n",
       "      <td>19</td>\n",
       "      <td>From: &lt;KEVXU@CUNYVM.BITNET&gt;\\nSubject: Re: Info...</td>\n",
       "      <td>from kevxu cunyvm bitnet subject re info new a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18827</th>\n",
       "      <td>19</td>\n",
       "      <td>From: pharvey@quack.kfu.com (Paul Harvey)\\nSub...</td>\n",
       "      <td>from pharvei quack kfu com paul harvei subject...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18828 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                          documents  \\\n",
       "0          0  From: mathew <mathew@mantis.co.uk>\\nSubject: A...   \n",
       "1          0  From: mathew <mathew@mantis.co.uk>\\nSubject: A...   \n",
       "2          0  From: I3150101@dbstu1.rz.tu-bs.de (Benedikt Ro...   \n",
       "3          0  From: mathew <mathew@mantis.co.uk>\\nSubject: R...   \n",
       "4          0  From: strom@Watson.Ibm.Com (Rob Strom)\\nSubjec...   \n",
       "...      ...                                                ...   \n",
       "18823     19  From: sbuckley@fraser.sfu.ca (Stephen Buckley)...   \n",
       "18824     19  From: bakerj@gtephx.UUCP (Jon Baker)\\nSubject:...   \n",
       "18825     19  From: pharvey@quack.kfu.com (Paul Harvey)\\nSub...   \n",
       "18826     19  From: <KEVXU@CUNYVM.BITNET>\\nSubject: Re: Info...   \n",
       "18827     19  From: pharvey@quack.kfu.com (Paul Harvey)\\nSub...   \n",
       "\n",
       "                                     documents_processed  \n",
       "0      from mathew mathew manti co uk subject alt ath...  \n",
       "1      from mathew mathew manti co uk subject alt ath...  \n",
       "2      from i dbstu rz tu bs de benedikt rosenau subj...  \n",
       "3      from mathew mathew manti co uk subject re univ...  \n",
       "4      from strom watson ibm com rob strom subject re...  \n",
       "...                                                  ...  \n",
       "18823  from sbucklei fraser sfu ca stephen bucklei su...  \n",
       "18824  from bakerj gtephx uucp jon baker subject re m...  \n",
       "18825  from pharvei quack kfu com paul harvei subject...  \n",
       "18826  from kevxu cunyvm bitnet subject re info new a...  \n",
       "18827  from pharvei quack kfu com paul harvei subject...  \n",
       "\n",
       "[18828 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['documents_processed'] = df.documents.apply(preprocessing)\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: (18828, 3)\n",
      "train shape: (11296, 3)\n",
      "validation shape: (1883, 3)\n",
      "test shape: (5649, 3)\n"
     ]
    }
   ],
   "source": [
    "train, validation, test = np.split(df.sample(frac=1, random_state=42), \n",
    "                            [int(.6*len(df)), int(.7*len(df))])\n",
    "\n",
    "print(\"Original size: {}\".format(df.shape))\n",
    "print(\"train shape: {}\".format(train.shape))\n",
    "print(\"validation shape: {}\".format(validation.shape))\n",
    "print(\"test shape: {}\".format(test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector = CountVectorizer()\n",
    "training_data_tf = count_vector.fit_transform(train['documents_processed'].values)\n",
    "validation_data_tf = count_vector.transform(validation['documents_processed'].values)\n",
    "testing_data_tf = count_vector.transform(test['documents_processed'].values)\n",
    "\n",
    "training_validation_x_tf = np.concatenate((train['documents_processed'].values,validation['documents_processed'].values))\n",
    "cross_validation_x_tf = count_vector.transform(training_validation_x_tf)\n",
    "cross_validation_y_tf = np.concatenate((train['label'].values,validation['label'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector_tfidf = TfidfVectorizer()\n",
    "training_data_tfidf = count_vector_tfidf.fit_transform(train['documents_processed'].values)\n",
    "validation_data_tfidf = count_vector_tfidf.transform(validation['documents_processed'].values)\n",
    "testing_data_tfidf = count_vector_tfidf.transform(test['documents_processed'].values)\n",
    "\n",
    "training_validation_x_tfidf = np.concatenate((train['documents_processed'].values,validation['documents_processed'].values))\n",
    "cross_validation_x_tfidf = count_vector.transform(training_validation_x_tfidf)\n",
    "cross_validation_y_tfidf = np.concatenate((train['label'].values,validation['label'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.09873724, 0.10571623, 0.09873128, 0.10671091, 0.12266803,\n",
       "        0.09873509, 0.14361453, 0.10870862, 0.09476423, 0.14956546]),\n",
       " 'score_time': array([0.01296473, 0.01296043, 0.01299524, 0.02992153, 0.01495957,\n",
       "        0.01595807, 0.01994658, 0.01498842, 0.01394463, 0.0179522 ]),\n",
       " 'test_accuracy': array([0.85356601, 0.88391502, 0.84522003, 0.87101669, 0.85356601,\n",
       "        0.86418816, 0.86191199, 0.87025797, 0.87329287, 0.86028853]),\n",
       " 'test_precision_macro': array([0.86966458, 0.89371184, 0.85394464, 0.88582491, 0.86616487,\n",
       "        0.87887166, 0.88133039, 0.88018104, 0.88705615, 0.87112448]),\n",
       " 'test_recall_macro': array([0.84680464, 0.87620575, 0.83841742, 0.86517072, 0.84550489,\n",
       "        0.85616903, 0.85325946, 0.86471207, 0.86678354, 0.85269931]),\n",
       " 'test_f1_macro': array([0.84245793, 0.87479524, 0.83265917, 0.86478761, 0.83979475,\n",
       "        0.85544158, 0.84608133, 0.8622628 , 0.86262036, 0.84594671])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data_tf,train['label'].values)\n",
    "scores = cross_validate(naive_bayes, cross_validation_x_tf, cross_validation_y_tf, cv=10, scoring=('accuracy','precision_macro','recall_macro','f1_macro'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.8557266772880155\n",
      "Precision score:  0.87326708830818\n",
      "Recall score:  0.8534836602339386\n",
      "F1 score:  0.847581436359263\n"
     ]
    }
   ],
   "source": [
    "predictions = naive_bayes.predict(testing_data_tf)\n",
    "print('Accuracy score: ', format(accuracy_score(test['label'].values, predictions)))\n",
    "print('Precision score: ', format(precision_score(test['label'].values, predictions,average='macro')))\n",
    "print('Recall score: ', format(recall_score(test['label'].values, predictions,average='macro')))\n",
    "print('F1 score: ', format(f1_score(test['label'].values, predictions,average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.13364244, 0.09674072, 0.10671711, 0.0907495 , 0.09574413,\n",
       "        0.09973407, 0.09674501, 0.09674144, 0.09674168, 0.1266613 ]),\n",
       " 'score_time': array([0.01595783, 0.01495957, 0.01395988, 0.01696324, 0.01598883,\n",
       "        0.01496148, 0.01798391, 0.01399589, 0.01698756, 0.01695514]),\n",
       " 'test_accuracy': array([0.85356601, 0.88391502, 0.84522003, 0.87101669, 0.85356601,\n",
       "        0.86418816, 0.86191199, 0.87025797, 0.87329287, 0.86028853]),\n",
       " 'test_precision_macro': array([0.86966458, 0.89371184, 0.85394464, 0.88582491, 0.86616487,\n",
       "        0.87887166, 0.88133039, 0.88018104, 0.88705615, 0.87112448]),\n",
       " 'test_recall_macro': array([0.84680464, 0.87620575, 0.83841742, 0.86517072, 0.84550489,\n",
       "        0.85616903, 0.85325946, 0.86471207, 0.86678354, 0.85269931]),\n",
       " 'test_f1_macro': array([0.84245793, 0.87479524, 0.83265917, 0.86478761, 0.83979475,\n",
       "        0.85544158, 0.84608133, 0.8622628 , 0.86262036, 0.84594671])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes_2 = MultinomialNB()\n",
    "naive_bayes_2.fit(training_data_tfidf,train['label'].values)\n",
    "scores = cross_validate(naive_bayes, cross_validation_x_tfidf, cross_validation_y_tfidf, cv=10, scoring=('accuracy','precision_macro','recall_macro','f1_macro'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.8629845990440786\n",
      "Precision score:  0.8842414508084777\n",
      "Recall score:  0.8510266495885734\n",
      "F1 score:  0.8474723928859029\n"
     ]
    }
   ],
   "source": [
    "predictions = naive_bayes_2.predict(testing_data_tfidf)\n",
    "print('Accuracy score: ', format(accuracy_score(test['label'].values, predictions)))\n",
    "print('Precision score: ', format(precision_score(test['label'].values, predictions,average='macro')))\n",
    "print('Recall score: ', format(recall_score(test['label'].values, predictions,average='macro')))\n",
    "print('F1 score: ', format(f1_score(test['label'].values, predictions,average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([45.50137115, 46.14166117, 41.7095294 , 42.11690855, 40.38604426,\n",
       "        46.77610946, 43.41295457, 44.59412193, 45.69684792, 44.03130102]),\n",
       " 'score_time': array([0.01795316, 0.01695395, 0.01693082, 0.01995349, 0.02094388,\n",
       "        0.01994753, 0.01795101, 0.01496172, 0.02194381, 0.01495886]),\n",
       " 'test_accuracy': array([0.88543247, 0.88846737, 0.87253414, 0.88239757, 0.8801214 ,\n",
       "        0.89150228, 0.89301973, 0.87481032, 0.89757208, 0.89066059]),\n",
       " 'test_precision_macro': array([0.88755238, 0.89167206, 0.87205017, 0.88468477, 0.8773183 ,\n",
       "        0.89377206, 0.890412  , 0.87670491, 0.89851077, 0.89334661]),\n",
       " 'test_recall_macro': array([0.8823568 , 0.88456409, 0.86938008, 0.88150316, 0.87416059,\n",
       "        0.88889432, 0.88792547, 0.87392769, 0.8932437 , 0.88903774]),\n",
       " 'test_f1_macro': array([0.88421627, 0.88679451, 0.86938292, 0.88207696, 0.87470656,\n",
       "        0.88998523, 0.88863844, 0.87498501, 0.89366201, 0.89015005])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_log_0 = LogisticRegression(random_state=0,multi_class='multinomial').fit(training_data_tf, train['label'].values)\n",
    "scores_0 = cross_validate(clf_log_0, cross_validation_x_tf, cross_validation_y_tf, cv=10, scoring=('accuracy','precision_macro','recall_macro','f1_macro'))\n",
    "scores_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([ 93.45119953,  98.64232278,  91.14887762,  81.11816621,\n",
       "         91.78565168,  97.13934159,  87.05890322,  97.85941529,\n",
       "        100.65893221,  98.8966434 ]),\n",
       " 'score_time': array([0.01695466, 0.01894951, 0.02393508, 0.019943  , 0.01695299,\n",
       "        0.01795173, 0.01496053, 0.01496124, 0.01894903, 0.01495981]),\n",
       " 'test_accuracy': array([0.88694992, 0.88770865, 0.87253414, 0.88239757, 0.87936267,\n",
       "        0.89150228, 0.89301973, 0.87481032, 0.89757208, 0.88990129]),\n",
       " 'test_precision_macro': array([0.88885929, 0.89053554, 0.87191653, 0.88468477, 0.87632181,\n",
       "        0.89377206, 0.89035845, 0.87680551, 0.89851077, 0.89236727]),\n",
       " 'test_recall_macro': array([0.88425489, 0.88340129, 0.86938008, 0.88150316, 0.87297012,\n",
       "        0.88889432, 0.88805639, 0.87392769, 0.8932437 , 0.88823965]),\n",
       " 'test_f1_macro': array([0.88595803, 0.88549605, 0.86938499, 0.88207696, 0.8735194 ,\n",
       "        0.88998523, 0.88869109, 0.87502677, 0.89366201, 0.88921305])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_log = LogisticRegression(random_state=0, max_iter=250, multi_class='multinomial').fit(training_data_tf, train['label'].values)\n",
    "scores = cross_validate(clf_log, cross_validation_x_tf, cross_validation_y_tf, cv=10, scoring=('accuracy','precision_macro','recall_macro','f1_macro'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.8773234200743495\n",
      "Precision score:  0.8781323314007391\n",
      "Recall score:  0.8753214903344413\n",
      "F1 score:  0.8763249880558377\n"
     ]
    }
   ],
   "source": [
    "predictions = clf_log.predict(testing_data_tf)\n",
    "print('Accuracy score: ', format(accuracy_score(test['label'].values, predictions)))\n",
    "print('Precision score: ', format(precision_score(test['label'].values, predictions,average='macro')))\n",
    "print('Recall score: ', format(recall_score(test['label'].values, predictions,average='macro')))\n",
    "print('F1 score: ', format(f1_score(test['label'].values, predictions,average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([89.03699827, 95.82884336, 92.87672639, 83.81794858, 94.37820458,\n",
       "        98.33614302, 87.72251344, 96.61374426, 99.14449477, 99.78358293]),\n",
       " 'score_time': array([0.01496029, 0.01894927, 0.02294254, 0.01994562, 0.01994658,\n",
       "        0.02393508, 0.02094603, 0.01695538, 0.02393126, 0.01595926]),\n",
       " 'test_accuracy': array([0.88694992, 0.88770865, 0.87253414, 0.88239757, 0.87936267,\n",
       "        0.89150228, 0.89301973, 0.87481032, 0.89757208, 0.88990129]),\n",
       " 'test_precision_macro': array([0.88885929, 0.89053554, 0.87191653, 0.88468477, 0.87632181,\n",
       "        0.89377206, 0.89035845, 0.87680551, 0.89851077, 0.89236727]),\n",
       " 'test_recall_macro': array([0.88425489, 0.88340129, 0.86938008, 0.88150316, 0.87297012,\n",
       "        0.88889432, 0.88805639, 0.87392769, 0.8932437 , 0.88823965]),\n",
       " 'test_f1_macro': array([0.88595803, 0.88549605, 0.86938499, 0.88207696, 0.8735194 ,\n",
       "        0.88998523, 0.88869109, 0.87502677, 0.89366201, 0.88921305])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_log_2 = LogisticRegression(random_state=0, max_iter=250, multi_class='multinomial').fit(training_data_tfidf, train['label'].values)\n",
    "scores = cross_validate(clf_log, cross_validation_x_tfidf, cross_validation_y_tfidf, cv=10, scoring=('accuracy','precision_macro','recall_macro','f1_macro'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.8413878562577447\n",
      "Precision score:  0.8511797899772114\n",
      "Recall score:  0.8382191237793639\n",
      "F1 score:  0.8374518856871092\n"
     ]
    }
   ],
   "source": [
    "predictions = clf_log_2.predict(testing_data_tf)\n",
    "print('Accuracy score: ', format(accuracy_score(test['label'].values, predictions)))\n",
    "print('Precision score: ', format(precision_score(test['label'].values, predictions,average='macro')))\n",
    "print('Recall score: ', format(recall_score(test['label'].values, predictions,average='macro')))\n",
    "print('F1 score: ', format(f1_score(test['label'].values, predictions,average='macro')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7a79e9e75522a046d95171e373010a5dca4ce6e8605d007854b2218f1d88052"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
