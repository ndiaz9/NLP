{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **N-Gram Language Models Implementation**\n",
    "\n",
    "For the 20N and BAC datasets, perform the processing required to build two N-Gram Language Models:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I. Read the files and build two large consolidate files that are the union of all the documents in 20N and BAC.\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes a sentence by stem method.\n",
    "    \"\"\"\n",
    "    from gensim.parsing.porter import PorterStemmer \n",
    "\n",
    "    p=PorterStemmer()\n",
    "    sentences = p.stem_sentence(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for all characters\n",
    "\n",
    "def replace_number(text) -> str:\n",
    "    \"\"\"\n",
    "    Replaces all numeric characters with a NUM.\n",
    "    \"\"\"\n",
    "    sentences = re.sub(r'\\d+', 'NUM', text)\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revome_punctuation(text) -> str:\n",
    "    \"\"\"\n",
    "    Removes punctuation from a text.\n",
    "    \"\"\"\n",
    "    sentences = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    sentences = re.sub(r'_+', ' ', sentences)\n",
    "    sentences = re.sub(r'\"', ' ', sentences)\n",
    "    sentences = re.sub(r'-', ' ', sentences)\n",
    "    sentences = re.sub(r'[^\\x20-\\x7e]', '', sentences)\n",
    "    sentences = re.sub(r'\\s+', ' ', sentences)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_sentences(text) -> list:\n",
    "    \"\"\"\n",
    "    Splits a text into sentences.\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    sentences = [revome_punctuation(s.strip()) for s in sentences if s.strip() != \"\"]\n",
    "    sentences = [s for s in sentences if s.strip() != \"\"]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frequency(text:list) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates the frequency of each word in a text.\n",
    "    \"\"\"\n",
    "    frequency = {}\n",
    "    for sentence in text:\n",
    "        for word in sentence.strip().split(' '):\n",
    "            if word not in frequency:\n",
    "                frequency[word] = 1\n",
    "            else:\n",
    "                frequency[word] += 1\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_N20(path) -> list:\n",
    "    \"\"\"\n",
    "    Reads the N20 corpus and returns a list of sentences.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    lines = []\n",
    "    try:\n",
    "        with open(path, encoding=\"utf8\", errors='ignore') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            sentences_proc = split_sentences(replace_number(normalize(line)))\n",
    "            sentences = sentences+sentences_proc\n",
    "    except Exception as e:\n",
    "        print(f\"{path} {str(e)}\")\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "989441"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate corpus for 20N\n",
    "path = 'Datasets/20news-18828/'\n",
    "\n",
    "def get_senteces_from_path_20N(path)->list:\n",
    "    import os\n",
    "    sentences = []\n",
    "    for dirs in sorted(os.listdir(path)):\n",
    "        tmpdir = path+dirs+'/'\n",
    "        if not dirs.startswith('.'):\n",
    "            for filename in sorted(os.listdir(tmpdir)):                \n",
    "                sentences = sentences + get_sentences_N20(tmpdir+filename)    \n",
    "    return sentences\n",
    "    \n",
    "\n",
    "\n",
    "s_20N = get_senteces_from_path_20N(path)   \n",
    "len(s_20N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = calculate_frequency(s_20N)\n",
    "#sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_UNK(text:list, dic:dict) -> list:\n",
    "    \"\"\"\n",
    "    Replaces all words that are in the dictionary with frequency = 1 by UNK.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    for sentence in text:\n",
    "        words = sentence.strip().split(' ')\n",
    "        for word in words:\n",
    "            if word in dic and dic[word] <= 3:\n",
    "                words[words.index(word)] = 'UNK'\n",
    "        sentences.append(' '.join(words))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_20N=replace_UNK(s_20N, dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list in a file \n",
    "with open('salida/corpus_20N.txt', 'w') as f:\n",
    "    for item in s_20N:\n",
    "        if item != '':\n",
    "            f.write(\"<s> %s </s>\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file to list of sentences\n",
    "sentences = []\n",
    "with open('salida/corpus_20N.txt','r') as f:\n",
    "    for line in f:\n",
    "        sentences.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update frequency after replace UNK\n",
    "dic = calculate_frequency(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 'G02'\n",
    "\n",
    "count = len(sentences)\n",
    "# train test skitlearn random without replacement\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_N20, test_N20 = train_test_split(sentences, test_size=0.2, random_state=120)\n",
    "\n",
    "# save train list in a file\n",
    "with open(f'salida/20N_{group}_training.txt', 'w') as f:\n",
    "    for item in train_N20:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "# save test list in a file\n",
    "with open(f'salida/20N_{group}_test.txt', 'w') as f:\n",
    "    for item in test_N20:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities_unigram(dict_work: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates the probabilities of each word in the dictionary.\n",
    "    \"\"\"\n",
    "    probabilities = {}\n",
    "    total = sum(dict_work.values())\n",
    "    for word in dict_work:\n",
    "        probabilities[word] = dict_work[word]/total\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dic_train = calculate_frequency(train_N20)\n",
    "dic_train_prob = calculate_probabilities_unigram(dic_train)\n",
    "\n",
    "# save dic as json file\n",
    "with open(f'salida/20N_{group}_unigrams.json', 'w') as f:\n",
    "    json.dump(dic_train_prob, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "group = 'G02'\n",
    "\n",
    "# read unigram from json\n",
    "with open(f'salida/20N_{group}_unigrams.json', 'r') as f:\n",
    "    dic_train_prob = json.load(f)\n",
    "list_words = list(dic_train_prob.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_len = len(list_words)\n",
    "arr = np.ones((data_len, data_len), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "#np.array([ b for a in train_N20[:4] for b in a.split(' ')])\n",
    "\n",
    "res = np.array([f'{word} {sentence.split()[j + 1]}' for sentence in train_N20 \n",
    "       for j, word in enumerate(sentence.split()) if j < len(sentence.split()) - 1])\n",
    "bigram_exist = np.unique(res, return_counts=True)\n",
    "bigram_exist = dict(zip(bigram_exist[0], bigram_exist[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_prob(word1:str, word2:str, bigram_exist:dict, dic_train_prob:dict) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the probability of a bigram.\n",
    "    \"\"\"\n",
    "    list_words = list(dic_train_prob.keys())\n",
    "    vocabulary_size = len(list_words)\n",
    "    \n",
    "    if word1 in list_words and word2 in list_words:\n",
    "        if word1+' '+word2 in bigram_exist:\n",
    "            return (bigram_exist[word1+' '+word2]+1)/(dic_train_prob[word1]+vocabulary_size)\n",
    "        else:\n",
    "            return 1/(dic_train_prob[word1]+vocabulary_size)\n",
    "        \n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018984186520537457"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of get_bigram_prob\n",
    "get_bigram_prob('i', 'want', bigram_exist, dic_train_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save bigram as json file\n",
    "import json\n",
    "from numpyencoder import NumpyEncoder\n",
    "\n",
    "\n",
    "with open(f'salida/20N_{group}_bigrams.json', 'w') as f:\n",
    "    json.dump(bigram_exist, f,cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bigram from json\n",
    "import json\n",
    "\n",
    "with open(f'salida/20N_{group}_bigrams.json', 'r') as f:\n",
    "    bigram_exist = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.array([f'{word} {sentence.split()[j + 1]} {sentence.split()[j + 2]}' for sentence in train_N20\n",
    "       for j, word in enumerate(sentence.split()) if j < len(sentence.split()) - 2])\n",
    "\n",
    "trigram_exist = np.unique(res, return_counts=True)\n",
    "trigram_exist = dict(zip(trigram_exist[0], trigram_exist[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigram_prob(word1:str, word2:str, word3:str, trigram_exist:dict, dic_train_prob:dict) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the probability of a trigram.\n",
    "    \"\"\"\n",
    "    list_words = list(dic_train_prob.keys())\n",
    "    vocabulary_size = len(list_words)\n",
    "    if word1 in list_words and word2 in list_words and word3 in list_words:\n",
    "        if word1+' '+word2+' '+word3 in trigram_exist:\n",
    "            return (trigram_exist[word1+' '+word2+' '+word3]+1)/(dic_train_prob[word1]+vocabulary_size)\n",
    "        else:\n",
    "            return 1/(dic_train_prob[word1]+vocabulary_size)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012039908554847725"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_trigram_prob('i', 'want', 'to', trigram_exist, dic_train_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trigram as json file\n",
    "import json\n",
    "from numpyencoder import NumpyEncoder\n",
    "\n",
    "with open(f'salida/20N_{group}_trigrams.json', 'w') as f:\n",
    "    json.dump(trigram_exist, f,cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trigram from json\n",
    "import json\n",
    "\n",
    "with open(f'salida/20N_{group}_trigrams.json', 'r') as f:\n",
    "    trigram_exist = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_test = test_N20[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_probability_unigrams(sentence, unigram_probabilities):\n",
    "    sentence_probability = 1\n",
    "    for word in sentence.split(' '):\n",
    "        sentence_probability *= unigram_probabilities[word]\n",
    "    return sentence_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppu=get_sentence_probability_unigrams(sentence_test, dic_train_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_probability_bigrams(sentence, bigram_probabilities,dic_train_prob):\n",
    "    sentence_probability = 1\n",
    "    word_list = sentence.split(' ')\n",
    "    for i in range(1,len(word_list)):\n",
    "        if word_list[i-1] != '' and word_list[i] != '':\n",
    "            sentence_probability *= get_bigram_prob(word_list[i-1], word_list[i], bigram_probabilities, dic_train_prob)\n",
    "    return sentence_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppb=get_sentence_probability_bigrams(sentence_test, bigram_exist, dic_train_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_probability_trigrams(sentence, trigram_exist, dic_train_prob):\n",
    "    sentence_probability = 1\n",
    "    word_list = sentence.split(' ')\n",
    "    for i in range(2,len(word_list)):\n",
    "        if word_list[i-2] != '' and word_list[i-1] != '' and word_list[i] != '':\n",
    "            sentence_probability *= get_trigram_prob(word_list[i-2], word_list[i-1], word_list[i], trigram_exist, dic_train_prob)\n",
    "    return sentence_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt=get_sentence_probability_trigrams(sentence_test, trigram_exist, dic_train_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of unigrams: 9.651900697901276e-32\n",
      "Probability of bigrams: 2.393847391761494e-32\n",
      "Probability of trigrams: 4.200642259945541e-32\n"
     ]
    }
   ],
   "source": [
    "print(f'Probability of unigrams: {ppu}')\n",
    "print(f'Probability of bigrams: {ppb}')\n",
    "print(f'Probability of trigrams: {ppt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max probability of N-grams: 9.651900697901276e-32 is unigrams\n"
     ]
    }
   ],
   "source": [
    "# print max value of probability\n",
    "print(f'Max probability of N-grams: {max(ppu, ppb, ppt)} is unigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(word:str, bigram_exist, dic_train_prob:dict) -> str:\n",
    "    \"\"\"\n",
    "    Predicts the next word in a sentence.\n",
    "    \"\"\"\n",
    "    list_words = list(dic_train_prob.keys())\n",
    "    vocabulary_size = len(list_words)\n",
    "    if word in list_words:\n",
    "        return max(list_words, key=lambda x: get_bigram_prob(word, x, bigram_exist, dic_train_prob))\n",
    "    else:\n",
    "        return ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i m\n",
      "i m </s>\n",
      "i m </s> <s>\n",
      "i m </s> <s> NUM\n",
      "i m </s> <s> NUM </s>\n",
      "i m </s> <s> NUM </s> <s>\n",
      "i m </s> <s> NUM </s> <s> NUM\n",
      "i m </s> <s> NUM </s> <s> NUM </s>\n",
      "i m </s> <s> NUM </s> <s> NUM </s> <s>\n",
      "i m </s> <s> NUM </s> <s> NUM </s> <s> NUM\n"
     ]
    }
   ],
   "source": [
    "# create sentence of 10 words using predict_next_word\n",
    "sentence_test = 'i'\n",
    "for i in range(10):\n",
    "    sentence_test += ' ' + predict_next_word(sentence_test.split()[-1], bigram_exist, dic_train_prob)\n",
    "    print(sentence_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
