{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **N-Gram Language Models Implementation**\n",
    "\n",
    "For the 20N and BAC datasets, perform the processing required to build two N-Gram Language Models:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I. Read the files and build two large consolidate files that are the union of all the documents in 20N and BAC.\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes a sentence by stem method.\n",
    "    \"\"\"\n",
    "    from gensim.parsing.porter import PorterStemmer \n",
    "\n",
    "    p=PorterStemmer()\n",
    "    sentences = p.stem_sentence(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for all characters\n",
    "\n",
    "def replace_number(text) -> str:\n",
    "    \"\"\"\n",
    "    Replaces all numeric characters with a NUM.\n",
    "    \"\"\"\n",
    "    sentences = re.sub(r'\\d+', 'NUM', text)\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revome_punctuation(text) -> str:\n",
    "    \"\"\"\n",
    "    Removes punctuation from a text.\n",
    "    \"\"\"\n",
    "    sentences = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    sentences = re.sub(r'_+', ' ', sentences)\n",
    "    sentences = re.sub(r'\"', ' ', sentences)\n",
    "    sentences = re.sub(r'-', ' ', sentences)\n",
    "    sentences = re.sub(r'[^\\x20-\\x7e]', '', sentences)\n",
    "    sentences = re.sub(r'\\s+', ' ', sentences)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_sentences(text) -> list:\n",
    "    \"\"\"\n",
    "    Splits a text into sentences.\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    sentences = [revome_punctuation(s.strip()) for s in sentences if s.strip() != \"\"]\n",
    "    sentences = [s for s in sentences if s.strip() != \"\"]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frequency(text:list) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates the frequency of each word in a text.\n",
    "    \"\"\"\n",
    "    frequency = {}\n",
    "    for sentence in text:\n",
    "        for word in sentence.strip().split(' '):\n",
    "            if word not in frequency:\n",
    "                frequency[word] = 1\n",
    "            else:\n",
    "                frequency[word] += 1\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_N20(path) -> list:\n",
    "    \"\"\"\n",
    "    Reads the N20 corpus and returns a list of sentences.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    lines = []\n",
    "    try:\n",
    "        with open(path, encoding=\"utf8\", errors='ignore') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            sentences_proc = split_sentences(replace_number(normalize(line)))\n",
    "            sentences = sentences+sentences_proc\n",
    "    except Exception as e:\n",
    "        print(f\"{path} {str(e)}\")\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "989441"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate corpus for 20N\n",
    "path = 'Datasets/20news-18828/'\n",
    "\n",
    "def get_senteces_from_path_20N(path)->list:\n",
    "    import os\n",
    "    sentences = []\n",
    "    for dirs in sorted(os.listdir(path)):\n",
    "        tmpdir = path+dirs+'/'\n",
    "        if not dirs.startswith('.'):\n",
    "            for filename in sorted(os.listdir(tmpdir)):                \n",
    "                sentences = sentences + get_sentences_N20(tmpdir+filename)    \n",
    "    return sentences\n",
    "    \n",
    "\n",
    "\n",
    "s_20N = get_senteces_from_path_20N(path)   \n",
    "len(s_20N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wished', 1),\n",
       " ('charities', 1),\n",
       " ('conformism', 1),\n",
       " ('requiem', 1),\n",
       " ('robes', 1),\n",
       " ('unexcept', 1),\n",
       " ('unexceptional', 1),\n",
       " ('roper', 1),\n",
       " ('graphology', 1),\n",
       " ('iqcvagubkNUMajrxzxn', 1)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = calculate_frequency(s_20N)\n",
    "sorted(dic.items(), key=lambda x: x[1], reverse=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_UNK(text:list, dic:dict) -> list:\n",
    "    \"\"\"\n",
    "    Replaces all words that are in the dictionary with frequency = 1 by UNK.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    for sentence in text:\n",
    "        words = sentence.strip().split(' ')\n",
    "        for word in words:\n",
    "            if word in dic and dic[word] == 1:\n",
    "                words[words.index(word)] = 'UNK'\n",
    "        sentences.append(' '.join(words))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_20N=replace_UNK(s_20N, dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list in a file \n",
    "with open('salida/corpus_20N.txt', 'w') as f:\n",
    "    for item in s_20N:\n",
    "        if item != '':\n",
    "            f.write(\"<s> %s </s>\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file to list of sentences\n",
    "sentences = []\n",
    "with open('salida/corpus_20N.txt','r') as f:\n",
    "    for line in f:\n",
    "        sentences.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 'G02'\n",
    "\n",
    "count = len(sentences)\n",
    "# train test skitlearn random without replacement\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_N20, test_N20 = train_test_split(sentences, test_size=0.2, random_state=120)\n",
    "\n",
    "# save train list in a file\n",
    "with open(f'salida/20N_{group}_training.txt', 'w') as f:\n",
    "    for item in train_N20:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "# save test list in a file\n",
    "with open(f'salida/20N_{group}_test.txt', 'w') as f:\n",
    "    for item in test_N20:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities_unigram(dict_work: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates the probabilities of each word in the dictionary.\n",
    "    \"\"\"\n",
    "    probabilities = {}\n",
    "    total = sum(dict_work.values())\n",
    "    for word in dict_work:\n",
    "        probabilities[word] = dict_work[word]/total\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dic_train = calculate_frequency(train_N20)\n",
    "dic_train_prob = calculate_probabilities_unigram(dic_train)\n",
    "\n",
    "# save dic as json file\n",
    "with open(f'salida/20N_{group}_unigrams.json', 'w') as f:\n",
    "    json.dump(dic_train_prob, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "group = 'G02'\n",
    "\n",
    "# read unigram from json\n",
    "with open(f'salida/20N_{group}_unigrams.json', 'r') as f:\n",
    "    dic_train_prob = json.load(f)\n",
    "list_words = list(dic_train_prob.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_len = len(list_words)\n",
    "arr = np.zeros((data_len, data_len), dtype=np.uint32)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30764"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bigrams from train_N20\n",
    "from itertools import permutations\n",
    "\n",
    "def create_bigrams(text:list) -> dict:\n",
    "    \"\"\"\n",
    "    Creates bigrams from a list of sentences.\n",
    "    \"\"\"\n",
    "    bigrams = {}\n",
    "    uninques = []\n",
    "    for sentence in text:\n",
    "        words = sentence.strip().split(' ')\n",
    "        for i in range(len(words)-1):\n",
    "            if words[i] not in uninques:\n",
    "                uninques.append(words[i])\n",
    "            if words[i] not in bigrams:\n",
    "                bigrams[words[i]] = {}\n",
    "                bigrams[words[i]][words[i+1]] = 1\n",
    "            else:\n",
    "                if words[i+1] not in bigrams[words[i]]:\n",
    "                    bigrams[words[i]][words[i+1]] = 1\n",
    "                else:\n",
    "                    bigrams[words[i]][words[i+1]] += 1\n",
    "        if words[-1] not in uninques:\n",
    "            uninques.append(words[-1])\n",
    "    print(len(uninques))\n",
    "    # Laplace smoothing \n",
    "    combs = list(permutations(uninques, 2))\n",
    "    for w1, w2 in combs:\n",
    "        if w1 not in bigrams:\n",
    "            bigrams[w1] = {}\n",
    "            bigrams[w1][w2] = 1\n",
    "        else:\n",
    "            if w2 not in bigrams[w1]:\n",
    "                bigrams[w1][w2] = 1\n",
    "            else:\n",
    "                bigrams[w1][w2] += 1\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilidades_bigram(bigram:dict, unigram:dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates the probabilities of each bigram in the dictionary.\n",
    "    \"\"\"\n",
    "    vocabulario = len(unigram) \n",
    "    for w1 in bigram:\n",
    "        for w2 in bigram[w1]:\n",
    "            bigram[w1][w2] = bigram[w1][w2]/(unigram[w1] + vocabulario)\n",
    "    return bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save json file\n",
    "import json\n",
    "\n",
    "bg = create_bigrams(train_N20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_prob = probabilidades_bigram(bg, dic_train_prob)\n",
    "\n",
    "# save dic as json file\n",
    "with open(f'salida/20N_{group}_bigrams.json', 'w') as f:\n",
    "    json.dump(bg_prob, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
