{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **N-Gram Language Models Implementation**\n",
    "\n",
    "For the 20N and BAC datasets, perform the processing required to build two N-Gram Language Models:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I. Read the files and build two large consolidate files that are the union of all the documents in 20N and BAC.\n",
    "\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "def get_sentences_BAC(path) -> list:\n",
    "    \"\"\"\n",
    "    Reads the BAC corpus and returns a list of sentences.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    lines = []\n",
    "    try:\n",
    "        with open(path, encoding=\"utf8\", errors='ignore') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if not re.match( r'<.*>', line):\n",
    "                sentences_proc = split_sentences(replace_number(normalize(line)))\n",
    "                sentences = sentences+sentences_proc\n",
    "    except Exception as e:\n",
    "        print(f\"{path} {str(e)}\")\n",
    "        \n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes a sentence by stem method.\n",
    "    \"\"\"\n",
    "    from gensim.parsing.porter import PorterStemmer \n",
    "\n",
    "    p=PorterStemmer()\n",
    "    sentences = p.stem_sentence(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for all characters\n",
    "\n",
    "def replace_number(text) -> str:\n",
    "    \"\"\"\n",
    "    Replaces all numeric characters with a NUM.\n",
    "    \"\"\"\n",
    "    sentences = re.sub(r'\\d+', 'NUM', text)\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_sentences(text) -> list:\n",
    "    \"\"\"\n",
    "    Splits a text into sentences.\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    sentences = [f\"<s>{s.strip()}</s>\" for s in sentences if s.strip() != \"\"]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_senteces_from_path(path)->list:\n",
    "    import os\n",
    "    sentences = []\n",
    "    for filename in sorted(os.listdir(path)):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            sentences = sentences + get_sentences_BAC(path+filename)\n",
    "    return sentences\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Datasets/BAC/blogs/'\n",
    "\n",
    "s = get_senteces_from_path(path)\n",
    "print (len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list in a file \n",
    "with open('salida/corpus_BAC.txt', 'w') as f:\n",
    "    for item in s:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_N20(path) -> list:\n",
    "    \"\"\"\n",
    "    Reads the N20 corpus and returns a list of sentences.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    lines = []\n",
    "    try:\n",
    "        with open(path, encoding=\"utf8\", errors='ignore') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            sentences_proc = split_sentences(replace_number(normalize(line)))\n",
    "            sentences = sentences+sentences_proc\n",
    "    except Exception as e:\n",
    "        print(f\"{path} {str(e)}\")\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1046621"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate corpus for 20N\n",
    "path = 'Datasets/20news-18828/'\n",
    "\n",
    "\n",
    "def get_senteces_from_path_20N(path)->list:\n",
    "    import os\n",
    "    sentences = []\n",
    "    for dirs in sorted(os.listdir(path)):\n",
    "        tmpdir = path+dirs+'/'\n",
    "        if not dirs.startswith('.'):\n",
    "            for filename in sorted(os.listdir(tmpdir)):                \n",
    "                sentences = sentences + get_sentences_N20(tmpdir+filename)    \n",
    "    return sentences\n",
    "    \n",
    "\n",
    "\n",
    "s_20N = get_senteces_from_path_20N(path)   \n",
    "len(s_20N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list in a file \n",
    "with open('salida/corpus_20N.txt', 'w') as f:\n",
    "    for item in s_20N:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
