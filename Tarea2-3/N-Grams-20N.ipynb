{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **N-Gram Language Models Implementation**\n",
    "\n",
    "For the 20N and BAC datasets, perform the processing required to build two N-Gram Language Models:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I. Read the files and build two large consolidate files that are the union of all the documents in 20N and BAC.\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes a sentence by stem method.\n",
    "    \"\"\"\n",
    "    from gensim.parsing.porter import PorterStemmer \n",
    "\n",
    "    p=PorterStemmer()\n",
    "    sentences = p.stem_sentence(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for all characters\n",
    "\n",
    "def replace_number(text) -> str:\n",
    "    \"\"\"\n",
    "    Replaces all numeric characters with a NUM.\n",
    "    \"\"\"\n",
    "    sentences = re.sub(r'\\d+', 'NUM', text)\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revome_punctuation(text) -> str:\n",
    "    \"\"\"\n",
    "    Removes punctuation from a text.\n",
    "    \"\"\"\n",
    "    sentences = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    sentences = re.sub(r'_+', ' ', sentences)\n",
    "    sentences = re.sub(r'\"', ' ', sentences)\n",
    "    sentences = re.sub(r'-', ' ', sentences)\n",
    "    sentences = re.sub(r'[^\\x20-\\x7e]', '', sentences)\n",
    "    sentences = re.sub(r'\\s+', ' ', sentences)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_sentences(text) -> list:\n",
    "    \"\"\"\n",
    "    Splits a text into sentences.\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    sentences = [revome_punctuation(s.strip()) for s in sentences if s.strip() != \"\"]\n",
    "    sentences = [s for s in sentences if s.strip() != \"\"]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frequency(text:list) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates the frequency of each word in a text.\n",
    "    \"\"\"\n",
    "    frequency = {}\n",
    "    for sentence in text:\n",
    "        for word in sentence.strip().split(' '):\n",
    "            if word not in frequency:\n",
    "                frequency[word] = 1\n",
    "            else:\n",
    "                frequency[word] += 1\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_N20(path) -> list:\n",
    "    \"\"\"\n",
    "    Reads the N20 corpus and returns a list of sentences.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    lines = []\n",
    "    try:\n",
    "        with open(path, encoding=\"utf8\", errors='ignore') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            sentences_proc = split_sentences(replace_number(normalize(line)))\n",
    "            sentences = sentences+sentences_proc\n",
    "    except Exception as e:\n",
    "        print(f\"{path} {str(e)}\")\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "989441"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate corpus for 20N\n",
    "path = 'Datasets/20news-18828/'\n",
    "\n",
    "def get_senteces_from_path_20N(path)->list:\n",
    "    import os\n",
    "    sentences = []\n",
    "    for dirs in sorted(os.listdir(path)):\n",
    "        tmpdir = path+dirs+'/'\n",
    "        if not dirs.startswith('.'):\n",
    "            for filename in sorted(os.listdir(tmpdir)):                \n",
    "                sentences = sentences + get_sentences_N20(tmpdir+filename)    \n",
    "    return sentences\n",
    "    \n",
    "\n",
    "\n",
    "s_20N = get_senteces_from_path_20N(path)   \n",
    "len(s_20N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NUM', 253724),\n",
       " ('the', 238663),\n",
       " ('to', 120377),\n",
       " ('of', 107317),\n",
       " ('a', 106198),\n",
       " ('and', 95075),\n",
       " ('i', 87623),\n",
       " ('in', 80825),\n",
       " ('is', 69169),\n",
       " ('that', 65149)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = calculate_frequency(s_20N)\n",
    "sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_UNK(text:list, dic:dict) -> list:\n",
    "    \"\"\"\n",
    "    Replaces all words that are in the dictionary with frequency = 1 by UNK.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    for sentence in text:\n",
    "        words = sentence.strip().split(' ')\n",
    "        for word in words:\n",
    "            if word in dic and dic[word] == 1:\n",
    "                words[words.index(word)] = 'UNK'\n",
    "        sentences.append(' '.join(words))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_20N=replace_UNK(s_20N, dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list in a file \n",
    "with open('salida/corpus_20N.txt', 'w') as f:\n",
    "    for item in s_20N:\n",
    "        if item != '':\n",
    "            f.write(\"<s> %s </s>\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file to list of sentences\n",
    "sentences = []\n",
    "with open('salida/corpus_20N.txt','r') as f:\n",
    "    for line in f:\n",
    "        sentences.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 'G02'\n",
    "\n",
    "count = len(sentences)\n",
    "# train test skitlearn random without replacement\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_N20, test_N20 = train_test_split(sentences, test_size=0.2, random_state=120)\n",
    "\n",
    "# save train list in a file\n",
    "with open(f'salida/20N_{group}_training.txt', 'w') as f:\n",
    "    for item in train_N20:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "# save test list in a file\n",
    "with open(f'salida/20N_{group}_test.txt', 'w') as f:\n",
    "    for item in test_N20:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities_unigram(dict_work: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates the probabilities of each word in the dictionary.\n",
    "    \"\"\"\n",
    "    probabilities = {}\n",
    "    total = sum(dict_work.values())\n",
    "    for word in dict_work:\n",
    "        probabilities[word] = dict_work[word]/total\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dic_train = calculate_frequency(train_N20)\n",
    "dic_train_prob = calculate_probabilities_unigram(dic_train)\n",
    "\n",
    "# save dic as json file\n",
    "with open(f'salida/20N_{group}_unigrams.json', 'w') as f:\n",
    "    json.dump(dic_train_prob, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "group = 'G02'\n",
    "\n",
    "# read unigram from json\n",
    "with open(f'salida/20N_{group}_unigrams.json', 'r') as f:\n",
    "    dic_train_prob = json.load(f)\n",
    "list_words = list(dic_train_prob.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_len = len(list_words)\n",
    "arr = np.ones((data_len, data_len), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in train_N20:\n",
    "    words = sentence.strip().split(' ')\n",
    "    for i in range(len(words)-1):\n",
    "        arr[list_words.index(words[i]), list_words.index(words[i+1])] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = len(list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(data_len):\n",
    "    for j in range(data_len):\n",
    "        arr[i, j] = arr[i, j]/(dic_train_prob[list_words[i]]+voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'salida/20N_{group}_bigrams_np.txt',arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bigrams from train_N20\n",
    "from itertools import permutations\n",
    "\n",
    "def create_bigrams(text:list) -> dict:\n",
    "    \"\"\"\n",
    "    Creates bigrams from a list of sentences.\n",
    "    \"\"\"\n",
    "    bigrams = {}\n",
    "    uninques = []\n",
    "    for sentence in text:\n",
    "        words = sentence.strip().split(' ')\n",
    "        for i in range(len(words)-1):\n",
    "            if words[i] not in uninques:\n",
    "                uninques.append(words[i])\n",
    "            if words[i] not in bigrams:\n",
    "                bigrams[words[i]] = {}\n",
    "                bigrams[words[i]][words[i+1]] = 1\n",
    "            else:\n",
    "                if words[i+1] not in bigrams[words[i]]:\n",
    "                    bigrams[words[i]][words[i+1]] = 1\n",
    "                else:\n",
    "                    bigrams[words[i]][words[i+1]] += 1\n",
    "        if words[-1] not in uninques:\n",
    "            uninques.append(words[-1])\n",
    "    print(len(uninques))\n",
    "    # Laplace smoothing \n",
    "    combs = list(permutations(uninques, 2))\n",
    "    for w1, w2 in combs:\n",
    "        if w1 not in bigrams:\n",
    "            bigrams[w1] = {}\n",
    "            bigrams[w1][w2] = 1\n",
    "        else:\n",
    "            if w2 not in bigrams[w1]:\n",
    "                bigrams[w1][w2] = 1\n",
    "            else:\n",
    "                bigrams[w1][w2] += 1\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilidades_bigram(bigram:dict, unigram:dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates the probabilities of each bigram in the dictionary.\n",
    "    \"\"\"\n",
    "    vocabulario = len(unigram) \n",
    "    for w1 in bigram:\n",
    "        for w2 in bigram[w1]:\n",
    "            bigram[w1][w2] = bigram[w1][w2]/(unigram[w1] + vocabulario)\n",
    "    return bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save json file\n",
    "import json\n",
    "bg = create_bigrams(train_N20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_prob = probabilidades_bigram(bg, dic_train_prob)\n",
    "\n",
    "# save dic as json file\n",
    "with open(f'salida/20N_{group}_bigrams.json', 'w') as f:\n",
    "    json.dump(bg_prob, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trigram(sentences, unigram,list_word):\n",
    "\n",
    "\ttrigram = np.ones((len(list_word), len(list_word), len(list_word)), dtype=np.float32)\n",
    "\tfor sentence in sentences:\n",
    "\t\twords = sentence.split()\n",
    "\t\tfor i in range(len(words) - 2):\n",
    "\t\t\ttrigram[list_word.index(words[i]), list_word.index(words[i+1]), list_word.index(words[i+2])] += 1\n",
    "\tvoc = len(list_word)\n",
    "\tfor i in range(len(list_word)):\n",
    "\t\tfor j in range(len(list_word)):\n",
    "\t\t\tfor k in range(len(list_word)):\n",
    "\t\t\t\ttrigram[i, j, k] /= (unigram[list_word[i]] + voc)\n",
    "\treturn trigram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = calculate_trigram(sentences, dic_train_prob,list_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigram_value(trigram,list_word, word1, word2, word3):\n",
    "\treturn trigram[list_word.index(word1), list_word.index(word2), list_word.index(word3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_trigram_value(trigram,list_words,'c','d','</s>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_probability_unigrams(sentence, unigram_probabilities):\n",
    "    sentence_probability = 1\n",
    "    for word in sentence.split(' '):\n",
    "        sentence_probability *= unigram_probabilities[word]\n",
    "    return sentence_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.913427259649652e-18"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_probability_unigrams(sentences[0],dic_train_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_probability_bigrams(sentence, bigram_probabilities):\n",
    "    sentence_probability = 1\n",
    "    word_list = sentence.split(' ')\n",
    "    for i in range(1,len(word_list)):\n",
    "        if word_list[i-1] != '' and word_list[i] != '':\n",
    "                bigram = word_list[i-1] + ' ' + word_list[i]\n",
    "        sentence_probability *= bigram_probabilities[bigram]\n",
    "    return sentence_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentence_probability_bigrams(sentences[0],bg_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_probability_trigrams(sentence, trigram_probabilities):\n",
    "    sentence_probability = 1\n",
    "    word_list = sentence.split(' ')\n",
    "    for i in range(2,len(word_list)):\n",
    "        if word_list[i-2] != '' and word_list[i-1] != '' and word_list[i] != '':\n",
    "                trigram = word_list[i-2] + ' ' + word_list[i-1] + ' ' + word_list[i]\n",
    "        sentence_probability *= trigram_probabilities[trigram]\n",
    "    return sentence_probability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
