{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import gensim\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# remove warnings\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126646, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_character_text                                       spoken_words\n",
       "0              Miss Hoover  No, actually, it was a little of both. Sometim...\n",
       "1             Lisa Simpson                             Where's Mr. Bergstrom?\n",
       "2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n",
       "3             Lisa Simpson                         That life is worth living.\n",
       "4  Edna Krabappel-Flanders  The polls will be open from now until the end ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documents = pd.read_csv('./datos/simpsons_dataset.csv').dropna().drop_duplicates()\n",
    "documents = documents.reset_index(drop=True)\n",
    "print(documents.shape)\n",
    "display(documents.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60610, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bart Simpson</td>\n",
       "      <td>Victory party under the slide!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Mr. Bergstrom! Mr. Bergstrom!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Do you know where I could find him?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   raw_character_text                         spoken_words\n",
       "1        Lisa Simpson               Where's Mr. Bergstrom?\n",
       "3        Lisa Simpson           That life is worth living.\n",
       "7        Bart Simpson       Victory party under the slide!\n",
       "8        Lisa Simpson        Mr. Bergstrom! Mr. Bergstrom!\n",
       "10       Lisa Simpson  Do you know where I could find him?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main_characters =  [\"Lisa Simpson\", \"Bart Simpson\", \"Homer Simpson\", \"Marge Simpson\"]\n",
    "documents = documents[documents[\"raw_character_text\"].isin(main_characters)]\n",
    "print(documents.shape)\n",
    "display(documents.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sublist(list, n):\n",
    "    return [list[i:i+n] for i in range(0, len(list), n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_characters(characters: pd.DataFrame) -> pd.DataFrame:\n",
    "    sentences_big = get_sublist(characters['spoken_words'].tolist(),5)\n",
    "    df = pd.DataFrame({ 'spoken_words': sentences_big})\n",
    "    for index,dialogue in df['spoken_words'].items():\n",
    "        df.loc[index,\"spoken_words_concatenated\"] = ' '.join(dialogue)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_dialogue_concatenated = documents.groupby(\"raw_character_text\")  \\\n",
    "                                    .apply(lambda x: get_vectors_characters(x)) \\\n",
    "                                    .reset_index(\"raw_character_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(label):\n",
    "    if label == \"bart simpson\":\n",
    "        return 1\n",
    "    elif label == \"lisa simpson\":\n",
    "        return 2\n",
    "    elif label == \"homer simpson\":\n",
    "        return 3\n",
    "    else : \n",
    "        return 4\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>spoken_words_concatenated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[Victory party under the slide!, Hey, thanks f...</td>\n",
       "      <td>victory party under the slide! hey, thanks for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Somebody must have voted., Uh oh., I demand a...</td>\n",
       "      <td>somebody must have voted. uh oh. i demand a re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[Ah, Dad, if just me, Milhouse and Lewis had v...</td>\n",
       "      <td>ah, dad, if just me, milhouse and lewis had vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[Please Dad., What?, Yes sir., They're fightin...</td>\n",
       "      <td>please dad. what? yes sir. they're fighting in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[Dad, I have as much respect for you as I ever...</td>\n",
       "      <td>dad, i have as much respect for you as i ever ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   raw_character_text                                       spoken_words  \\\n",
       "0                   1  [Victory party under the slide!, Hey, thanks f...   \n",
       "1                   1  [Somebody must have voted., Uh oh., I demand a...   \n",
       "2                   1  [Ah, Dad, if just me, Milhouse and Lewis had v...   \n",
       "3                   1  [Please Dad., What?, Yes sir., They're fightin...   \n",
       "4                   1  [Dad, I have as much respect for you as I ever...   \n",
       "\n",
       "                           spoken_words_concatenated  \n",
       "0  victory party under the slide! hey, thanks for...  \n",
       "1  somebody must have voted. uh oh. i demand a re...  \n",
       "2  ah, dad, if just me, milhouse and lewis had vo...  \n",
       "3  please dad. what? yes sir. they're fighting in...  \n",
       "4  dad, i have as much respect for you as i ever ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_dialogue_concatenated[\"raw_character_text\"] = documents_dialogue_concatenated[\"raw_character_text\"].apply(lambda x: x.lower())\n",
    "documents_dialogue_concatenated[\"raw_character_text\"] = documents_dialogue_concatenated[\"raw_character_text\"].apply(convert_label)\n",
    "documents_dialogue_concatenated[\"spoken_words_concatenated\"] = documents_dialogue_concatenated[\"spoken_words_concatenated\"].apply(lambda x: x.lower())\n",
    "documents_dialogue_concatenated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = documents_dialogue_concatenated['spoken_words_concatenated'].values\n",
    "y = documents_dialogue_concatenated['raw_character_text'].values\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.2, random_state=1)\n",
    "\n",
    "y_train_categorical = tf.one_hot(y_train,4)\n",
    "y_test_categorical = tf.one_hot(y_test,4)\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((sentences_train,y_train_categorical))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((sentences_test,y_test_categorical))\n",
    "\n",
    "text_dataset = tf.data.Dataset.from_tensor_slices((sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the total set: 12123\n",
      "Number of rows of bart class: 0\n",
      "Number of rows of lisa class: 2414\n",
      "Number of rows of homer class: 2029\n",
      "Number of rows of marge class: 5195\n",
      "---------------------------------------------\n",
      "Number of rows in the training set: 9698\n",
      "Number of rows of bart class: 0\n",
      "Number of rows of lisa class: 1913\n",
      "Number of rows of homer class: 1598\n",
      "Number of rows of marge class: 4171\n",
      "---------------------------------------------\n",
      "Number of rows in the test set: 2425\n",
      "Number of rows of bart class: 0\n",
      "Number of rows of lisa class: 501\n",
      "Number of rows of homer class: 431\n",
      "Number of rows of marge class: 1024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Number of rows in the total set: {}'.format(sentences.shape[0]))\n",
    "print('Number of rows of bart class: {}'.format(y[y == 0].shape[0]))\n",
    "print('Number of rows of lisa class: {}'.format(y[y == 1].shape[0]))\n",
    "print('Number of rows of homer class: {}'.format(y[y == 2].shape[0]))\n",
    "print('Number of rows of marge class: {}'.format(y[y == 3].shape[0]))\n",
    "print('---------------------------------------------')\n",
    "\n",
    "print('Number of rows in the training set: {}'.format(sentences_train.shape[0]))\n",
    "print('Number of rows of bart class: {}'.format(y_train[y_train == 0].shape[0]))\n",
    "print('Number of rows of lisa class: {}'.format(y_train[y_train == 1].shape[0]))\n",
    "print('Number of rows of homer class: {}'.format(y_train[y_train == 2].shape[0]))\n",
    "print('Number of rows of marge class: {}'.format(y_train[y_train == 3].shape[0]))\n",
    "print('---------------------------------------------')\n",
    "\n",
    "print('Number of rows in the test set: {}'.format(sentences_test.shape[0]))\n",
    "\n",
    "print('Number of rows of bart class: {}'.format(y_test[y_test == 0].shape[0]))\n",
    "print('Number of rows of lisa class: {}'.format(y_test[y_test == 1].shape[0]))\n",
    "print('Number of rows of homer class: {}'.format(y_test[y_test == 2].shape[0]))\n",
    "print('Number of rows of marge class: {}'.format(y_test[y_test == 3].shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Layer Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_binary = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=1000, vocabulary=None,\n",
    "    output_mode='binary', output_sequence_length=None, pad_to_max_tokens=True,\n",
    ")\n",
    "\n",
    "vectorize_layer_binary.adapt(text_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vectorize_layer_binary.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsequ1 = Sequential()\n",
    "modelsequ1.add(vectorize_layer_binary)\n",
    "modelsequ1.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ1.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ1.add(layers.Dense(50, activation='relu', name=\"Hidden_layer_2\"))\n",
    "modelsequ1.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc', 'mse']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 0.8537 - acc: 0.3732 - mse: 0.1496 - val_loss: 0.9788 - val_acc: 0.2066 - val_mse: 0.1674\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 0.8594 - acc: 0.3676 - mse: 0.1506 - val_loss: 1.0177 - val_acc: 0.2066 - val_mse: 0.1787\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 0.8633 - acc: 0.3647 - mse: 0.1512 - val_loss: 1.0509 - val_acc: 0.2066 - val_mse: 0.1888\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 0.8683 - acc: 0.3660 - mse: 0.1522 - val_loss: 1.0346 - val_acc: 0.2066 - val_mse: 0.1865\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 0.8758 - acc: 0.3666 - mse: 0.1536 - val_loss: 0.9945 - val_acc: 0.2066 - val_mse: 0.1758\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 0.8825 - acc: 0.3604 - mse: 0.1550 - val_loss: 0.9616 - val_acc: 0.2066 - val_mse: 0.1648\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 0.8855 - acc: 0.3559 - mse: 0.1557 - val_loss: 0.9292 - val_acc: 0.2066 - val_mse: 0.1577\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 2s 23ms/step - loss: 0.8818 - acc: 0.3538 - mse: 0.1554 - val_loss: 0.8660 - val_acc: 0.2066 - val_mse: 0.1480\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 0.8623 - acc: 0.3751 - mse: 0.1518 - val_loss: 0.8534 - val_acc: 0.4223 - val_mse: 0.1463\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 0.8702 - acc: 0.3628 - mse: 0.1532 - val_loss: 0.8765 - val_acc: 0.4223 - val_mse: 0.1490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a99fb5fd60>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 10\n",
    "\n",
    "# Fit the model using the train and test datasets.\n",
    "modelsequ1.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (TextV  (None, 1000)             0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 1000, 10)          10000     \n",
      "                                                                 \n",
      " global_average_pooling1d_2   (None, 10)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 50)                550       \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,754\n",
      "Trainable params: 10,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelsequ1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 4ms/step - loss: 0.8765 - acc: 0.4223 - mse: 0.1490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8765268325805664, 0.4222680330276489, 0.14895105361938477]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsequ1.evaluate(sentences_test,y_test_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Layer count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_count = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=100, vocabulary=None,\n",
    "    output_mode='count', output_sequence_length=None, pad_to_max_tokens=True,\n",
    ")\n",
    "\n",
    "vectorize_layer_count.adapt(text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_15 (Text  (None, 100)              0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 50)                5050      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,254\n",
      "Trainable params: 5,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelsequ2 = Sequential()\n",
    "modelsequ2.add(vectorize_layer_count)\n",
    "modelsequ1.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ1.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ2.add(layers.Dense(50, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ2.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ2.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc', 'mse']) \n",
    "modelsequ2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "97/97 [==============================] - 3s 15ms/step - loss: 1.3935 - acc: 0.3822 - mse: 0.1849 - val_loss: 1.3477 - val_acc: 0.4004 - val_mse: 0.1806\n",
      "Epoch 2/20\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3263 - acc: 0.4150 - mse: 0.1775 - val_loss: 1.3143 - val_acc: 0.4165 - val_mse: 0.1766\n",
      "Epoch 3/20\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.2947 - acc: 0.4297 - mse: 0.1739 - val_loss: 1.2899 - val_acc: 0.4214 - val_mse: 0.1736\n",
      "Epoch 4/20\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.2699 - acc: 0.4384 - mse: 0.1708 - val_loss: 1.2696 - val_acc: 0.4359 - val_mse: 0.1710\n",
      "Epoch 5/20\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.2483 - acc: 0.4494 - mse: 0.1680 - val_loss: 1.2502 - val_acc: 0.4445 - val_mse: 0.1684\n",
      "Epoch 6/20\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.2293 - acc: 0.4602 - mse: 0.1655 - val_loss: 1.2331 - val_acc: 0.4540 - val_mse: 0.1660\n",
      "Epoch 7/20\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.2109 - acc: 0.4732 - mse: 0.1631 - val_loss: 1.2166 - val_acc: 0.4697 - val_mse: 0.1636\n",
      "Epoch 8/20\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.1932 - acc: 0.4837 - mse: 0.1607 - val_loss: 1.1994 - val_acc: 0.4800 - val_mse: 0.1613\n",
      "Epoch 9/20\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.1760 - acc: 0.4945 - mse: 0.1584 - val_loss: 1.1829 - val_acc: 0.4903 - val_mse: 0.1590\n",
      "Epoch 10/20\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.1600 - acc: 0.5047 - mse: 0.1563 - val_loss: 1.1683 - val_acc: 0.5035 - val_mse: 0.1570\n",
      "Epoch 11/20\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.1448 - acc: 0.5150 - mse: 0.1543 - val_loss: 1.1543 - val_acc: 0.5126 - val_mse: 0.1551\n",
      "Epoch 12/20\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.1306 - acc: 0.5226 - mse: 0.1524 - val_loss: 1.1418 - val_acc: 0.5138 - val_mse: 0.1534\n",
      "Epoch 13/20\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.1174 - acc: 0.5282 - mse: 0.1507 - val_loss: 1.1297 - val_acc: 0.5229 - val_mse: 0.1518\n",
      "Epoch 14/20\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.1056 - acc: 0.5341 - mse: 0.1491 - val_loss: 1.1187 - val_acc: 0.5299 - val_mse: 0.1503\n",
      "Epoch 15/20\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.0949 - acc: 0.5416 - mse: 0.1478 - val_loss: 1.1083 - val_acc: 0.5373 - val_mse: 0.1489\n",
      "Epoch 16/20\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.0851 - acc: 0.5474 - mse: 0.1465 - val_loss: 1.0991 - val_acc: 0.5410 - val_mse: 0.1477\n",
      "Epoch 17/20\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.0766 - acc: 0.5514 - mse: 0.1454 - val_loss: 1.0907 - val_acc: 0.5439 - val_mse: 0.1466\n",
      "Epoch 18/20\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.0689 - acc: 0.5552 - mse: 0.1444 - val_loss: 1.0830 - val_acc: 0.5489 - val_mse: 0.1456\n",
      "Epoch 19/20\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.0618 - acc: 0.5581 - mse: 0.1435 - val_loss: 1.0763 - val_acc: 0.5493 - val_mse: 0.1447\n",
      "Epoch 20/20\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.0555 - acc: 0.5610 - mse: 0.1426 - val_loss: 1.0702 - val_acc: 0.5522 - val_mse: 0.1440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dad9a796a0>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "modelsequ2.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize layer tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_tfidf = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=100, vocabulary=None,\n",
    "    output_mode='tf-idf', output_sequence_length=None, pad_to_max_tokens=True,\n",
    ")\n",
    "\n",
    "vectorize_layer_tfidf.adapt(text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_17 (Text  (None, 100)              1         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 50)                5050      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,255\n",
      "Trainable params: 5,254\n",
      "Non-trainable params: 1\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelsequ3 = Sequential()\n",
    "modelsequ3.add(vectorize_layer_tfidf)\n",
    "modelsequ1.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ1.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ3.add(layers.Dense(50, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ3.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ3.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc', 'mse']) \n",
    "modelsequ3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.0495 - acc: 0.5648 - mse: 0.1418 - val_loss: 1.0655 - val_acc: 0.5538 - val_mse: 0.1434\n",
      "Epoch 2/20\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.0441 - acc: 0.5676 - mse: 0.1411 - val_loss: 1.0612 - val_acc: 0.5538 - val_mse: 0.1428\n",
      "Epoch 3/20\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.0394 - acc: 0.5691 - mse: 0.1405 - val_loss: 1.0574 - val_acc: 0.5559 - val_mse: 0.1423\n",
      "Epoch 4/20\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.0347 - acc: 0.5705 - mse: 0.1399 - val_loss: 1.0543 - val_acc: 0.5575 - val_mse: 0.1420\n",
      "Epoch 5/20\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.0303 - acc: 0.5724 - mse: 0.1393 - val_loss: 1.0513 - val_acc: 0.5575 - val_mse: 0.1416\n",
      "Epoch 6/20\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.0263 - acc: 0.5760 - mse: 0.1388 - val_loss: 1.0487 - val_acc: 0.5579 - val_mse: 0.1413\n",
      "Epoch 7/20\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.0229 - acc: 0.5785 - mse: 0.1383 - val_loss: 1.0463 - val_acc: 0.5604 - val_mse: 0.1410\n",
      "Epoch 8/20\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.0191 - acc: 0.5795 - mse: 0.1378 - val_loss: 1.0440 - val_acc: 0.5645 - val_mse: 0.1407\n",
      "Epoch 9/20\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.0158 - acc: 0.5818 - mse: 0.1374 - val_loss: 1.0416 - val_acc: 0.5645 - val_mse: 0.1404\n",
      "Epoch 10/20\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.0126 - acc: 0.5826 - mse: 0.1369 - val_loss: 1.0398 - val_acc: 0.5670 - val_mse: 0.1402\n",
      "Epoch 11/20\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.0095 - acc: 0.5846 - mse: 0.1365 - val_loss: 1.0381 - val_acc: 0.5670 - val_mse: 0.1400\n",
      "Epoch 12/20\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.0060 - acc: 0.5849 - mse: 0.1361 - val_loss: 1.0361 - val_acc: 0.5649 - val_mse: 0.1398\n",
      "Epoch 13/20\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.0026 - acc: 0.5873 - mse: 0.1356 - val_loss: 1.0346 - val_acc: 0.5666 - val_mse: 0.1396\n",
      "Epoch 14/20\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.9996 - acc: 0.5891 - mse: 0.1352 - val_loss: 1.0335 - val_acc: 0.5687 - val_mse: 0.1395\n",
      "Epoch 15/20\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 0.9974 - acc: 0.5898 - mse: 0.1350 - val_loss: 1.0321 - val_acc: 0.5682 - val_mse: 0.1393\n",
      "Epoch 16/20\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 0.9950 - acc: 0.5910 - mse: 0.1346 - val_loss: 1.0309 - val_acc: 0.5682 - val_mse: 0.1392\n",
      "Epoch 17/20\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 0.9927 - acc: 0.5918 - mse: 0.1343 - val_loss: 1.0298 - val_acc: 0.5682 - val_mse: 0.1391\n",
      "Epoch 18/20\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 0.9905 - acc: 0.5925 - mse: 0.1341 - val_loss: 1.0287 - val_acc: 0.5695 - val_mse: 0.1390\n",
      "Epoch 19/20\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 0.9888 - acc: 0.5936 - mse: 0.1338 - val_loss: 1.0276 - val_acc: 0.5707 - val_mse: 0.1388\n",
      "Epoch 20/20\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 0.9868 - acc: 0.5940 - mse: 0.1336 - val_loss: 1.0264 - val_acc: 0.5711 - val_mse: 0.1387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dadbdfc790>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "modelsequ2.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7a79e9e75522a046d95171e373010a5dca4ce6e8605d007854b2218f1d88052"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
