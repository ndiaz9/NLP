{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import gensim\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# remove warnings\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126646, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_character_text                                       spoken_words\n",
       "0              Miss Hoover  No, actually, it was a little of both. Sometim...\n",
       "1             Lisa Simpson                             Where's Mr. Bergstrom?\n",
       "2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n",
       "3             Lisa Simpson                         That life is worth living.\n",
       "4  Edna Krabappel-Flanders  The polls will be open from now until the end ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lectura de la data y eliminaci√≥n de registros repetidos y nulos\n",
    "documents = pd.read_csv('./datos/simpsons_dataset.csv').dropna().drop_duplicates()\n",
    "documents = documents.reset_index(drop=True)\n",
    "print(documents.shape)\n",
    "display(documents.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60610, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bart Simpson</td>\n",
       "      <td>Victory party under the slide!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Mr. Bergstrom! Mr. Bergstrom!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Do you know where I could find him?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   raw_character_text                         spoken_words\n",
       "1        Lisa Simpson               Where's Mr. Bergstrom?\n",
       "3        Lisa Simpson           That life is worth living.\n",
       "7        Bart Simpson       Victory party under the slide!\n",
       "8        Lisa Simpson        Mr. Bergstrom! Mr. Bergstrom!\n",
       "10       Lisa Simpson  Do you know where I could find him?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filtrado por personajes principales\n",
    "main_characters =  [\"Lisa Simpson\", \"Bart Simpson\", \"Homer Simpson\", \"Marge Simpson\"]\n",
    "documents = documents[documents[\"raw_character_text\"].isin(main_characters)]\n",
    "print(documents.shape)\n",
    "display(documents.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sublist(list:list, n:int) -> list:\n",
    "    \"\"\"\n",
    "    Obtener sublistas de una lista\n",
    "    \"\"\"\n",
    "    return [list[i:i+n] for i in range(0, len(list), n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_characters(characters: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extraer dialogos para todos los personajes en grupos de 5 oraciones\n",
    "    \"\"\"\n",
    "    sentences_big = get_sublist(characters['spoken_words'].tolist(),5)\n",
    "    df = pd.DataFrame({ 'spoken_words': sentences_big})\n",
    "    for index,dialogue in df['spoken_words'].items():\n",
    "        df.loc[index,\"spoken_words_concatenated\"] = ' '.join(dialogue)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_dialogue_concatenated = documents.groupby(\"raw_character_text\")  \\\n",
    "                                    .apply(lambda x: get_vectors_characters(x)) \\\n",
    "                                    .reset_index(\"raw_character_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(label:str) -> int:\n",
    "    \"\"\" \n",
    "    Categoriza los labels\n",
    "    \"\"\"\n",
    "    if label == \"bart simpson\":\n",
    "        return 0\n",
    "    elif label == \"lisa simpson\":\n",
    "        return 1\n",
    "    elif label == \"homer simpson\":\n",
    "        return 2\n",
    "    else : \n",
    "        return 3\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>spoken_words_concatenated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[Victory party under the slide!, Hey, thanks f...</td>\n",
       "      <td>victory party under the slide! hey, thanks for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Somebody must have voted., Uh oh., I demand a...</td>\n",
       "      <td>somebody must have voted. uh oh. i demand a re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[Ah, Dad, if just me, Milhouse and Lewis had v...</td>\n",
       "      <td>ah, dad, if just me, milhouse and lewis had vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[Please Dad., What?, Yes sir., They're fightin...</td>\n",
       "      <td>please dad. what? yes sir. they're fighting in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[Dad, I have as much respect for you as I ever...</td>\n",
       "      <td>dad, i have as much respect for you as i ever ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   raw_character_text                                       spoken_words  \\\n",
       "0                   1  [Victory party under the slide!, Hey, thanks f...   \n",
       "1                   1  [Somebody must have voted., Uh oh., I demand a...   \n",
       "2                   1  [Ah, Dad, if just me, Milhouse and Lewis had v...   \n",
       "3                   1  [Please Dad., What?, Yes sir., They're fightin...   \n",
       "4                   1  [Dad, I have as much respect for you as I ever...   \n",
       "\n",
       "                           spoken_words_concatenated  \n",
       "0  victory party under the slide! hey, thanks for...  \n",
       "1  somebody must have voted. uh oh. i demand a re...  \n",
       "2  ah, dad, if just me, milhouse and lewis had vo...  \n",
       "3  please dad. what? yes sir. they're fighting in...  \n",
       "4  dad, i have as much respect for you as i ever ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_dialogue_concatenated[\"raw_character_text\"] = documents_dialogue_concatenated[\"raw_character_text\"].apply(lambda x: x.lower())\n",
    "documents_dialogue_concatenated[\"raw_character_text\"] = documents_dialogue_concatenated[\"raw_character_text\"].apply(convert_label)\n",
    "documents_dialogue_concatenated[\"spoken_words_concatenated\"] = documents_dialogue_concatenated[\"spoken_words_concatenated\"].apply(lambda x: x.lower())\n",
    "documents_dialogue_concatenated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = documents_dialogue_concatenated['spoken_words_concatenated'].values\n",
    "y = documents_dialogue_concatenated['raw_character_text'].values\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.2, random_state=1)\n",
    "\n",
    "y_train_categorical = tf.one_hot(y_train,4)\n",
    "y_test_categorical = tf.one_hot(y_test,4)\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((sentences_train,y_train_categorical))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((sentences_test,y_test_categorical))\n",
    "\n",
    "text_dataset = tf.data.Dataset.from_tensor_slices((sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the total set: 12123\n",
      "Number of rows of bart class: 2414\n",
      "Number of rows of lisa class: 2029\n",
      "Number of rows of homer class: 5195\n",
      "Number of rows of marge class: 2485\n",
      "---------------------------------------------\n",
      "Number of rows in the training set: 9698\n",
      "Number of rows of bart class: 1913\n",
      "Number of rows of lisa class: 1598\n",
      "Number of rows of homer class: 4171\n",
      "Number of rows of marge class: 2016\n",
      "---------------------------------------------\n",
      "Number of rows in the test set: 2425\n",
      "Number of rows of bart class: 501\n",
      "Number of rows of lisa class: 431\n",
      "Number of rows of homer class: 1024\n",
      "Number of rows of marge class: 469\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Number of rows in the total set: {}'.format(sentences.shape[0]))\n",
    "print('Number of rows of bart class: {}'.format(y[y == 0].shape[0]))\n",
    "print('Number of rows of lisa class: {}'.format(y[y == 1].shape[0]))\n",
    "print('Number of rows of homer class: {}'.format(y[y == 2].shape[0]))\n",
    "print('Number of rows of marge class: {}'.format(y[y == 3].shape[0]))\n",
    "print('---------------------------------------------')\n",
    "\n",
    "print('Number of rows in the training set: {}'.format(sentences_train.shape[0]))\n",
    "print('Number of rows of bart class: {}'.format(y_train[y_train == 0].shape[0]))\n",
    "print('Number of rows of lisa class: {}'.format(y_train[y_train == 1].shape[0]))\n",
    "print('Number of rows of homer class: {}'.format(y_train[y_train == 2].shape[0]))\n",
    "print('Number of rows of marge class: {}'.format(y_train[y_train == 3].shape[0]))\n",
    "print('---------------------------------------------')\n",
    "\n",
    "print('Number of rows in the test set: {}'.format(sentences_test.shape[0]))\n",
    "\n",
    "print('Number of rows of bart class: {}'.format(y_test[y_test == 0].shape[0]))\n",
    "print('Number of rows of lisa class: {}'.format(y_test[y_test == 1].shape[0]))\n",
    "print('Number of rows of homer class: {}'.format(y_test[y_test == 2].shape[0]))\n",
    "print('Number of rows of marge class: {}'.format(y_test[y_test == 3].shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Layer Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_binary = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=200, vocabulary=None,\n",
    "    output_mode='binary', output_sequence_length=None, pad_to_max_tokens=True,\n",
    ")\n",
    "\n",
    "vectorize_layer_binary.adapt(text_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vectorize_layer_binary.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsequ1 = Sequential()\n",
    "modelsequ1.add(vectorize_layer_binary)\n",
    "modelsequ1.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ1.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ1.add(layers.Dense(50, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ1.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc', 'mse']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 4s 23ms/step - loss: 0.9290 - acc: 0.4227 - mse: 0.1447 - val_loss: 0.8613 - val_acc: 0.4223 - val_mse: 0.1430\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 0.8075 - acc: 0.4301 - mse: 0.1402 - val_loss: 0.8279 - val_acc: 0.4223 - val_mse: 0.1427\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 0.8047 - acc: 0.4301 - mse: 0.1405 - val_loss: 0.8287 - val_acc: 0.4223 - val_mse: 0.1436\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 0.8088 - acc: 0.4291 - mse: 0.1413 - val_loss: 0.8642 - val_acc: 0.4223 - val_mse: 0.1505\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 0.8258 - acc: 0.4065 - mse: 0.1450 - val_loss: 0.8912 - val_acc: 0.2066 - val_mse: 0.1533\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 0.8316 - acc: 0.3946 - mse: 0.1457 - val_loss: 0.9167 - val_acc: 0.2066 - val_mse: 0.1558\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 0.8534 - acc: 0.3740 - mse: 0.1496 - val_loss: 1.0003 - val_acc: 0.2066 - val_mse: 0.1765\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 0.8637 - acc: 0.3727 - mse: 0.1517 - val_loss: 0.8727 - val_acc: 0.4231 - val_mse: 0.1484\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 2s 21ms/step - loss: 0.8794 - acc: 0.3614 - mse: 0.1550 - val_loss: 0.8931 - val_acc: 0.4223 - val_mse: 0.1505\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 0.9364 - acc: 0.3467 - mse: 0.1637 - val_loss: 0.9053 - val_acc: 0.4223 - val_mse: 0.1528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x229fad44e20>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 10\n",
    "\n",
    "# Fit the model using the train and test datasets.\n",
    "modelsequ1.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, 200)              0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 100)          20000     \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 100)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 50)                5050      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,254\n",
      "Trainable params: 25,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelsequ1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 5ms/step - loss: 0.9053 - acc: 0.4223 - mse: 0.1528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9053424000740051, 0.4222680330276489, 0.15278129279613495]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsequ1.evaluate(sentences_test,y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 10ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       469\n",
      "           1       0.00      0.00      0.00       501\n",
      "           2       0.00      0.00      0.00       431\n",
      "           3       0.42      1.00      0.59      1024\n",
      "\n",
      "    accuracy                           0.42      2425\n",
      "   macro avg       0.11      0.25      0.15      2425\n",
      "weighted avg       0.18      0.42      0.25      2425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = modelsequ1.predict(sentences_test, batch_size=100, verbose=1) \n",
    "predicted = np.argmax(pred, axis=1)  \n",
    "report = classification_report(np.argmax(y_test_categorical, axis=1), predicted)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Layer count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_count = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=200, vocabulary=None,\n",
    "    output_mode='count', output_sequence_length=None, pad_to_max_tokens=True,\n",
    ")\n",
    "\n",
    "vectorize_layer_count.adapt(text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = len(vectorize_layer_count.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsequ2 = Sequential()\n",
    "modelsequ2.add(vectorize_layer_count)\n",
    "modelsequ2.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ2.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ2.add(layers.Dense(50, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ2.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ2.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc', 'mse']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 4s 28ms/step - loss: 1.0375 - acc: 0.4066 - mse: 0.1541 - val_loss: 1.0105 - val_acc: 0.4223 - val_mse: 0.1503\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 3s 27ms/step - loss: 0.9539 - acc: 0.4301 - mse: 0.1445 - val_loss: 0.9515 - val_acc: 0.4223 - val_mse: 0.1444\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 3s 26ms/step - loss: 0.9077 - acc: 0.4301 - mse: 0.1404 - val_loss: 0.9190 - val_acc: 0.4223 - val_mse: 0.1422\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 2s 25ms/step - loss: 0.8811 - acc: 0.4301 - mse: 0.1389 - val_loss: 0.8990 - val_acc: 0.4223 - val_mse: 0.1415\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 2s 24ms/step - loss: 0.8643 - acc: 0.4301 - mse: 0.1383 - val_loss: 0.8854 - val_acc: 0.4223 - val_mse: 0.1412\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 2s 22ms/step - loss: 0.8526 - acc: 0.4301 - mse: 0.1381 - val_loss: 0.8753 - val_acc: 0.4223 - val_mse: 0.1411\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 2s 24ms/step - loss: 0.8438 - acc: 0.4301 - mse: 0.1381 - val_loss: 0.8674 - val_acc: 0.4223 - val_mse: 0.1410\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 3s 26ms/step - loss: 0.8370 - acc: 0.4301 - mse: 0.1381 - val_loss: 0.8610 - val_acc: 0.4223 - val_mse: 0.1410\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 3s 26ms/step - loss: 0.8314 - acc: 0.4301 - mse: 0.1381 - val_loss: 0.8559 - val_acc: 0.4223 - val_mse: 0.1410\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 3s 26ms/step - loss: 0.8273 - acc: 0.4301 - mse: 0.1381 - val_loss: 0.8523 - val_acc: 0.4223 - val_mse: 0.1410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2298005e190>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "\n",
    "modelsequ2.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (TextV  (None, 200)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 100)          20000     \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 100)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 50)                5050      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,254\n",
      "Trainable params: 25,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelsequ2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 6ms/step - loss: 0.8523 - acc: 0.4223 - mse: 0.1410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8523479700088501, 0.4222680330276489, 0.14097552001476288]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsequ2.evaluate(sentences_test,y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 10ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       469\n",
      "           1       0.00      0.00      0.00       501\n",
      "           2       0.00      0.00      0.00       431\n",
      "           3       0.42      1.00      0.59      1024\n",
      "\n",
      "    accuracy                           0.42      2425\n",
      "   macro avg       0.11      0.25      0.15      2425\n",
      "weighted avg       0.18      0.42      0.25      2425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = modelsequ2.predict(sentences_test, batch_size=100, verbose=1) \n",
    "predicted = np.argmax(pred, axis=1)  \n",
    "report = classification_report(np.argmax(y_test_categorical, axis=1), predicted)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize layer int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_int = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=200, vocabulary=None,\n",
    "    output_mode='int', output_sequence_length=None,\n",
    ")\n",
    "\n",
    "vectorize_layer_int.adapt(text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = len(vectorize_layer_int.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsequ3 = Sequential()\n",
    "modelsequ3.add(vectorize_layer_int)\n",
    "modelsequ3.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ3.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ3.add(layers.Dense(50, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ3.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ3.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc', 'mse']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 3s 19ms/step - loss: 1.0193 - acc: 0.4289 - mse: 0.1520 - val_loss: 0.9894 - val_acc: 0.4223 - val_mse: 0.1482\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 0.9341 - acc: 0.4301 - mse: 0.1429 - val_loss: 0.9352 - val_acc: 0.4223 - val_mse: 0.1434\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 0.8932 - acc: 0.4301 - mse: 0.1398 - val_loss: 0.9073 - val_acc: 0.4223 - val_mse: 0.1419\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 0.8710 - acc: 0.4301 - mse: 0.1388 - val_loss: 0.8910 - val_acc: 0.4223 - val_mse: 0.1415\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 0.8575 - acc: 0.4301 - mse: 0.1385 - val_loss: 0.8800 - val_acc: 0.4223 - val_mse: 0.1414\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 0.8481 - acc: 0.4301 - mse: 0.1384 - val_loss: 0.8721 - val_acc: 0.4223 - val_mse: 0.1413\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 0.8417 - acc: 0.4301 - mse: 0.1383 - val_loss: 0.8664 - val_acc: 0.4223 - val_mse: 0.1413\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 0.8369 - acc: 0.4301 - mse: 0.1383 - val_loss: 0.8620 - val_acc: 0.4223 - val_mse: 0.1412\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 0.8332 - acc: 0.4301 - mse: 0.1383 - val_loss: 0.8585 - val_acc: 0.4223 - val_mse: 0.1412\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 0.8305 - acc: 0.4301 - mse: 0.1384 - val_loss: 0.8562 - val_acc: 0.4223 - val_mse: 0.1412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22981194af0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "modelsequ3.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_2 (TextV  (None, None)             0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         20000     \n",
      "                                                                 \n",
      " global_average_pooling1d_2   (None, 100)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 50)                5050      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,254\n",
      "Trainable params: 25,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelsequ3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 4ms/step - loss: 0.8587 - acc: 0.4223 - mse: 0.1412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8586507439613342, 0.4222680330276489, 0.14121271669864655]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsequ3.evaluate(sentences_test,y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 7ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       469\n",
      "           1       0.00      0.00      0.00       501\n",
      "           2       0.00      0.00      0.00       431\n",
      "           3       0.42      1.00      0.59      1024\n",
      "\n",
      "    accuracy                           0.42      2425\n",
      "   macro avg       0.11      0.25      0.15      2425\n",
      "weighted avg       0.18      0.42      0.25      2425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = modelsequ3.predict(sentences_test, batch_size=100, verbose=1) \n",
    "predicted = np.argmax(pred, axis=1)  \n",
    "report = classification_report(np.argmax(y_test_categorical, axis=1), predicted)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Layer Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_binary = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=200, vocabulary=None,\n",
    "    output_mode='binary', output_sequence_length=None, pad_to_max_tokens=True,\n",
    ")\n",
    "\n",
    "vectorize_layer_binary.adapt(text_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vectorize_layer_binary.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsequ1 = Sequential()\n",
    "modelsequ1.add(vectorize_layer_binary)\n",
    "modelsequ1.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ1.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ1.add(layers.Dense(50, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ1.add(layers.Dense(50, activation='relu', name=\"Hidden_layer_2\"))\n",
    "modelsequ1.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc', 'mse']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 4s 24ms/step - loss: 0.8940 - acc: 0.4295 - mse: 0.1438 - val_loss: 0.8269 - val_acc: 0.4223 - val_mse: 0.1432\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 2s 21ms/step - loss: 0.8058 - acc: 0.4301 - mse: 0.1414 - val_loss: 0.8625 - val_acc: 0.4223 - val_mse: 0.1482\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 0.8670 - acc: 0.3788 - mse: 0.1521 - val_loss: 0.9076 - val_acc: 0.4223 - val_mse: 0.1531\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 1.4629 - acc: 0.3180 - mse: 0.2084 - val_loss: 1.4051 - val_acc: 0.2066 - val_mse: 0.2061\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 2.9974 - acc: 0.3023 - mse: 0.2530 - val_loss: 3.2271 - val_acc: 0.4223 - val_mse: 0.2343\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 13.0034 - acc: 0.3121 - mse: 0.2821 - val_loss: 15.3277 - val_acc: 0.2066 - val_mse: 0.3484\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 16.4163 - acc: 0.2941 - mse: 0.2969 - val_loss: 26.1277 - val_acc: 0.2066 - val_mse: 0.3484\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 21.8722 - acc: 0.3062 - mse: 0.2902 - val_loss: 54.5234 - val_acc: 0.4223 - val_mse: 0.2405\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 40.2443 - acc: 0.3131 - mse: 0.2889 - val_loss: 21.3995 - val_acc: 0.1777 - val_mse: 0.3628\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 30.4273 - acc: 0.3093 - mse: 0.2891 - val_loss: 46.1363 - val_acc: 0.4223 - val_mse: 0.2405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2298242ce80>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 10\n",
    "\n",
    "# Fit the model using the train and test datasets.\n",
    "modelsequ1.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_3 (TextV  (None, 200)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 100)          20000     \n",
      "                                                                 \n",
      " global_average_pooling1d_3   (None, 100)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 50)                5050      \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 50)                2550      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,804\n",
      "Trainable params: 27,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelsequ1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 1s 5ms/step - loss: 46.1362 - acc: 0.4223 - mse: 0.2405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[46.13624572753906, 0.4222680330276489, 0.24051547050476074]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsequ1.evaluate(sentences_test,y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       469\n",
      "           1       0.00      0.00      0.00       501\n",
      "           2       0.00      0.00      0.00       431\n",
      "           3       0.42      1.00      0.59      1024\n",
      "\n",
      "    accuracy                           0.42      2425\n",
      "   macro avg       0.11      0.25      0.15      2425\n",
      "weighted avg       0.18      0.42      0.25      2425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = modelsequ1.predict(sentences_test, batch_size=100, verbose=1) \n",
    "predicted = np.argmax(pred, axis=1)  \n",
    "report = classification_report(np.argmax(y_test_categorical, axis=1), predicted)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Layer count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_count = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=200, vocabulary=None,\n",
    "    output_mode='count', output_sequence_length=None, pad_to_max_tokens=True,\n",
    ")\n",
    "\n",
    "vectorize_layer_count.adapt(text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = len(vectorize_layer_count.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsequ2 = Sequential()\n",
    "modelsequ2.add(vectorize_layer_count)\n",
    "modelsequ2.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ2.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ2.add(layers.Dense(50, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ2.add(layers.Dense(50, activation='relu', name=\"Hidden_layer_2\"))\n",
    "modelsequ2.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ2.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc', 'mse']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 4s 27ms/step - loss: 1.0164 - acc: 0.4280 - mse: 0.1515 - val_loss: 0.9785 - val_acc: 0.4223 - val_mse: 0.1466\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 2s 24ms/step - loss: 0.9212 - acc: 0.4301 - mse: 0.1412 - val_loss: 0.9264 - val_acc: 0.4223 - val_mse: 0.1427\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 2s 24ms/step - loss: 0.8847 - acc: 0.4301 - mse: 0.1392 - val_loss: 0.9045 - val_acc: 0.4223 - val_mse: 0.1422\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 3s 30ms/step - loss: 0.8670 - acc: 0.4301 - mse: 0.1388 - val_loss: 0.8901 - val_acc: 0.4223 - val_mse: 0.1420\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 3s 32ms/step - loss: 0.8536 - acc: 0.4301 - mse: 0.1386 - val_loss: 0.8767 - val_acc: 0.4223 - val_mse: 0.1417\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 0.8418 - acc: 0.4301 - mse: 0.1384 - val_loss: 0.8648 - val_acc: 0.4223 - val_mse: 0.1414\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 3s 30ms/step - loss: 0.8318 - acc: 0.4301 - mse: 0.1383 - val_loss: 0.8548 - val_acc: 0.4223 - val_mse: 0.1411\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 3s 32ms/step - loss: 0.8271 - acc: 0.4301 - mse: 0.1384 - val_loss: 0.8600 - val_acc: 0.4223 - val_mse: 0.1411\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 4s 40ms/step - loss: 2.4011 - acc: 0.3708 - mse: 0.1887 - val_loss: 20.2456 - val_acc: 0.4223 - val_mse: 0.2405\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 4s 38ms/step - loss: 2212.3484 - acc: 0.3066 - mse: 0.2946 - val_loss: 1.0368 - val_acc: 0.4223 - val_mse: 0.1544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22983594940>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "\n",
    "modelsequ2.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_4 (TextV  (None, 200)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 100)          20000     \n",
      "                                                                 \n",
      " global_average_pooling1d_4   (None, 100)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 50)                5050      \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 50)                2550      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,804\n",
      "Trainable params: 27,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelsequ2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 1s 6ms/step - loss: 1.0368 - acc: 0.4223 - mse: 0.1544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0368092060089111, 0.4222680330276489, 0.1543927937746048]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsequ2.evaluate(sentences_test,y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 11ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       469\n",
      "           1       0.00      0.00      0.00       501\n",
      "           2       0.00      0.00      0.00       431\n",
      "           3       0.42      1.00      0.59      1024\n",
      "\n",
      "    accuracy                           0.42      2425\n",
      "   macro avg       0.11      0.25      0.15      2425\n",
      "weighted avg       0.18      0.42      0.25      2425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = modelsequ2.predict(sentences_test, batch_size=100, verbose=1) \n",
    "predicted = np.argmax(pred, axis=1)  \n",
    "report = classification_report(np.argmax(y_test_categorical, axis=1), predicted)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize layer int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_int = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=200, vocabulary=None,\n",
    "    output_mode='int', output_sequence_length=None,\n",
    ")\n",
    "\n",
    "vectorize_layer_int.adapt(text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = len(vectorize_layer_int.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsequ3 = Sequential()\n",
    "modelsequ3.add(vectorize_layer_int)\n",
    "modelsequ3.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ3.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ3.add(layers.Dense(50, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ3.add(layers.Dense(50, activation='relu', name=\"Hidden_layer_2\"))\n",
    "modelsequ3.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ3.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc', 'mse']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 3s 24ms/step - loss: 1.0426 - acc: 0.4172 - mse: 0.1548 - val_loss: 1.0164 - val_acc: 0.4223 - val_mse: 0.1512\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 3s 27ms/step - loss: 0.9608 - acc: 0.4301 - mse: 0.1455 - val_loss: 0.9562 - val_acc: 0.4223 - val_mse: 0.1451\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 3s 30ms/step - loss: 0.9114 - acc: 0.4301 - mse: 0.1410 - val_loss: 0.9189 - val_acc: 0.4223 - val_mse: 0.1423\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 2s 23ms/step - loss: 0.8801 - acc: 0.4301 - mse: 0.1390 - val_loss: 0.8946 - val_acc: 0.4223 - val_mse: 0.1412\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 3s 27ms/step - loss: 0.8594 - acc: 0.4301 - mse: 0.1382 - val_loss: 0.8778 - val_acc: 0.4223 - val_mse: 0.1408\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 3s 26ms/step - loss: 0.8447 - acc: 0.4301 - mse: 0.1379 - val_loss: 0.8652 - val_acc: 0.4223 - val_mse: 0.1407\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 2s 23ms/step - loss: 0.8335 - acc: 0.4301 - mse: 0.1378 - val_loss: 0.8553 - val_acc: 0.4223 - val_mse: 0.1406\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 0.8249 - acc: 0.4301 - mse: 0.1378 - val_loss: 0.8479 - val_acc: 0.4223 - val_mse: 0.1407\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 0.8190 - acc: 0.4301 - mse: 0.1379 - val_loss: 0.8438 - val_acc: 0.4223 - val_mse: 0.1406\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 0.8185 - acc: 0.4301 - mse: 0.1379 - val_loss: 0.8502 - val_acc: 0.4223 - val_mse: 0.1405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x229836a6ac0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "modelsequ3.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_5 (TextV  (None, None)             0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         20000     \n",
      "                                                                 \n",
      " global_average_pooling1d_5   (None, 100)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 50)                5050      \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 50)                2550      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,804\n",
      "Trainable params: 27,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelsequ3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 4ms/step - loss: 0.8509 - acc: 0.4223 - mse: 0.1404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8509414196014404, 0.4222680330276489, 0.14042586088180542]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsequ3.evaluate(sentences_test,y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 7ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       469\n",
      "           1       0.00      0.00      0.00       501\n",
      "           2       0.00      0.00      0.00       431\n",
      "           3       0.42      1.00      0.59      1024\n",
      "\n",
      "    accuracy                           0.42      2425\n",
      "   macro avg       0.11      0.25      0.15      2425\n",
      "weighted avg       0.18      0.42      0.25      2425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = modelsequ3.predict(sentences_test, batch_size=100, verbose=1) \n",
    "predicted = np.argmax(pred, axis=1)  \n",
    "report = classification_report(np.argmax(y_test_categorical, axis=1), predicted)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Layer Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_binary = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=200, vocabulary=None,\n",
    "    output_mode='binary', output_sequence_length=None, pad_to_max_tokens=True,\n",
    ")\n",
    "\n",
    "vectorize_layer_binary.adapt(text_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vectorize_layer_binary.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsequ1 = Sequential()\n",
    "modelsequ1.add(vectorize_layer_binary)\n",
    "modelsequ1.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ1.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ1.add(layers.Dense(100, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ1.add(layers.Dense(50, activation='relu', name=\"Hidden_layer_2\"))\n",
    "modelsequ1.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc', 'mse']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 3s 23ms/step - loss: 0.8693 - acc: 0.4263 - mse: 0.1423 - val_loss: 0.8318 - val_acc: 0.4223 - val_mse: 0.1419\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 0.8211 - acc: 0.4072 - mse: 0.1438 - val_loss: 1.0520 - val_acc: 0.2066 - val_mse: 0.1891\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 1.1361 - acc: 0.3348 - mse: 0.1797 - val_loss: 1.4425 - val_acc: 0.4223 - val_mse: 0.2108\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 4.3957 - acc: 0.3127 - mse: 0.2583 - val_loss: 3.4620 - val_acc: 0.1777 - val_mse: 0.3583\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 10.4776 - acc: 0.3056 - mse: 0.2874 - val_loss: 19.5853 - val_acc: 0.4223 - val_mse: 0.2405\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 16.9414 - acc: 0.3049 - mse: 0.2873 - val_loss: 15.8044 - val_acc: 0.2066 - val_mse: 0.3484\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 36.6586 - acc: 0.3028 - mse: 0.2956 - val_loss: 86.7227 - val_acc: 0.2066 - val_mse: 0.3484\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 51.1171 - acc: 0.3161 - mse: 0.2914 - val_loss: 60.2224 - val_acc: 0.4223 - val_mse: 0.2405\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 66.8316 - acc: 0.3029 - mse: 0.2973 - val_loss: 108.6534 - val_acc: 0.2066 - val_mse: 0.3484\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 114.1056 - acc: 0.3112 - mse: 0.2896 - val_loss: 230.6767 - val_acc: 0.4223 - val_mse: 0.2405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22983a13670>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 10\n",
    "\n",
    "# Fit the model using the train and test datasets.\n",
    "modelsequ1.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_6 (TextV  (None, 200)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 100)          20000     \n",
      "                                                                 \n",
      " global_average_pooling1d_6   (None, 100)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 100)               10100     \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 50)                5050      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,354\n",
      "Trainable params: 35,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelsequ1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 5ms/step - loss: 230.6767 - acc: 0.4223 - mse: 0.2405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[230.67665100097656, 0.4222680330276489, 0.24051547050476074]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsequ1.evaluate(sentences_test,y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       469\n",
      "           1       0.00      0.00      0.00       501\n",
      "           2       0.00      0.00      0.00       431\n",
      "           3       0.42      1.00      0.59      1024\n",
      "\n",
      "    accuracy                           0.42      2425\n",
      "   macro avg       0.11      0.25      0.15      2425\n",
      "weighted avg       0.18      0.42      0.25      2425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = modelsequ1.predict(sentences_test, batch_size=100, verbose=1) \n",
    "predicted = np.argmax(pred, axis=1)  \n",
    "report = classification_report(np.argmax(y_test_categorical, axis=1), predicted)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Layer count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_count = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=200, vocabulary=None,\n",
    "    output_mode='count', output_sequence_length=None, pad_to_max_tokens=True,\n",
    ")\n",
    "\n",
    "vectorize_layer_count.adapt(text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = len(vectorize_layer_count.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsequ2 = Sequential()\n",
    "modelsequ2.add(vectorize_layer_count)\n",
    "modelsequ2.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ2.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ2.add(layers.Dense(100, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ2.add(layers.Dense(50, activation='relu', name=\"Hidden_layer_2\"))\n",
    "modelsequ2.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ2.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc', 'mse']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 4s 30ms/step - loss: 1.0422 - acc: 0.4225 - mse: 0.1546 - val_loss: 1.0183 - val_acc: 0.4223 - val_mse: 0.1512\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 3s 27ms/step - loss: 0.9621 - acc: 0.4301 - mse: 0.1454 - val_loss: 0.9575 - val_acc: 0.4223 - val_mse: 0.1449\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 2s 25ms/step - loss: 0.9124 - acc: 0.4301 - mse: 0.1408 - val_loss: 0.9215 - val_acc: 0.4223 - val_mse: 0.1423\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 3s 27ms/step - loss: 0.8825 - acc: 0.4301 - mse: 0.1389 - val_loss: 0.8984 - val_acc: 0.4223 - val_mse: 0.1414\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 2s 25ms/step - loss: 0.8626 - acc: 0.4301 - mse: 0.1383 - val_loss: 0.8824 - val_acc: 0.4223 - val_mse: 0.1411\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 3s 27ms/step - loss: 0.8487 - acc: 0.4301 - mse: 0.1381 - val_loss: 0.8704 - val_acc: 0.4223 - val_mse: 0.1410\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 2s 26ms/step - loss: 0.8383 - acc: 0.4301 - mse: 0.1380 - val_loss: 0.8610 - val_acc: 0.4223 - val_mse: 0.1409\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 2s 25ms/step - loss: 0.8302 - acc: 0.4301 - mse: 0.1381 - val_loss: 0.8536 - val_acc: 0.4223 - val_mse: 0.1409\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 2s 25ms/step - loss: 0.8244 - acc: 0.4301 - mse: 0.1382 - val_loss: 0.8507 - val_acc: 0.4223 - val_mse: 0.1409\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 2s 25ms/step - loss: 0.8297 - acc: 0.4301 - mse: 0.1384 - val_loss: 0.8867 - val_acc: 0.4223 - val_mse: 0.1439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x229822db880>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "\n",
    "modelsequ2.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_7 (TextV  (None, 200)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 100)          20000     \n",
      "                                                                 \n",
      " global_average_pooling1d_7   (None, 100)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 100)               10100     \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 50)                5050      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,354\n",
      "Trainable params: 35,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelsequ2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 5ms/step - loss: 0.8867 - acc: 0.4223 - mse: 0.1439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8866713047027588, 0.4222680330276489, 0.1439012885093689]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsequ2.evaluate(sentences_test,y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 10ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       469\n",
      "           1       0.00      0.00      0.00       501\n",
      "           2       0.00      0.00      0.00       431\n",
      "           3       0.42      1.00      0.59      1024\n",
      "\n",
      "    accuracy                           0.42      2425\n",
      "   macro avg       0.11      0.25      0.15      2425\n",
      "weighted avg       0.18      0.42      0.25      2425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = modelsequ2.predict(sentences_test, batch_size=100, verbose=1) \n",
    "predicted = np.argmax(pred, axis=1)  \n",
    "report = classification_report(np.argmax(y_test_categorical, axis=1), predicted)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize layer int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_int = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=200, vocabulary=None,\n",
    "    output_mode='int', output_sequence_length=None,\n",
    ")\n",
    "\n",
    "vectorize_layer_int.adapt(text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = len(vectorize_layer_int.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsequ3 = Sequential()\n",
    "modelsequ3.add(vectorize_layer_int)\n",
    "modelsequ3.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ3.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ3.add(layers.Dense(100, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ3.add(layers.Dense(50, activation='relu', name=\"Hidden_layer_2\"))\n",
    "modelsequ3.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ3.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc', 'mse']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 3s 20ms/step - loss: 1.0171 - acc: 0.4279 - mse: 0.1516 - val_loss: 0.9839 - val_acc: 0.4223 - val_mse: 0.1474\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 0.9286 - acc: 0.4301 - mse: 0.1420 - val_loss: 0.9323 - val_acc: 0.4223 - val_mse: 0.1430\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 0.8909 - acc: 0.4301 - mse: 0.1394 - val_loss: 0.9078 - val_acc: 0.4223 - val_mse: 0.1420\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 0.8709 - acc: 0.4301 - mse: 0.1387 - val_loss: 0.8923 - val_acc: 0.4223 - val_mse: 0.1417\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 0.8575 - acc: 0.4301 - mse: 0.1384 - val_loss: 0.8804 - val_acc: 0.4223 - val_mse: 0.1415\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 0.8469 - acc: 0.4301 - mse: 0.1383 - val_loss: 0.8704 - val_acc: 0.4223 - val_mse: 0.1413\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 0.8381 - acc: 0.4301 - mse: 0.1382 - val_loss: 0.8616 - val_acc: 0.4223 - val_mse: 0.1412\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 0.8310 - acc: 0.4301 - mse: 0.1383 - val_loss: 0.8558 - val_acc: 0.4223 - val_mse: 0.1411\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 0.8284 - acc: 0.4301 - mse: 0.1383 - val_loss: 0.8571 - val_acc: 0.4223 - val_mse: 0.1411\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 0.8354 - acc: 0.4301 - mse: 0.1386 - val_loss: 0.8820 - val_acc: 0.4223 - val_mse: 0.1430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22983cdc3a0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "modelsequ3.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_8 (TextV  (None, None)             0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         20000     \n",
      "                                                                 \n",
      " global_average_pooling1d_8   (None, 100)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 100)               10100     \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 50)                5050      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,354\n",
      "Trainable params: 35,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelsequ3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 4ms/step - loss: 0.8835 - acc: 0.4223 - mse: 0.1429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8835266828536987, 0.4222680330276489, 0.1429128348827362]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsequ3.evaluate(sentences_test,y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 8ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       469\n",
      "           1       0.00      0.00      0.00       501\n",
      "           2       0.00      0.00      0.00       431\n",
      "           3       0.42      1.00      0.59      1024\n",
      "\n",
      "    accuracy                           0.42      2425\n",
      "   macro avg       0.11      0.25      0.15      2425\n",
      "weighted avg       0.18      0.42      0.25      2425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = modelsequ3.predict(sentences_test, batch_size=100, verbose=1) \n",
    "predicted = np.argmax(pred, axis=1)  \n",
    "report = classification_report(np.argmax(y_test_categorical, axis=1), predicted)  \n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7a79e9e75522a046d95171e373010a5dca4ce6e8605d007854b2218f1d88052"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
