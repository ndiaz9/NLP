{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import gensim\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# remove warnings\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126646, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_character_text                                       spoken_words\n",
       "0              Miss Hoover  No, actually, it was a little of both. Sometim...\n",
       "1             Lisa Simpson                             Where's Mr. Bergstrom?\n",
       "2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n",
       "3             Lisa Simpson                         That life is worth living.\n",
       "4  Edna Krabappel-Flanders  The polls will be open from now until the end ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lectura de la data y eliminaci√≥n de registros repetidos y nulos\n",
    "documents = pd.read_csv('./datos/simpsons_dataset.csv').dropna().drop_duplicates()\n",
    "documents = documents.reset_index(drop=True)\n",
    "print(documents.shape)\n",
    "display(documents.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60610, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bart Simpson</td>\n",
       "      <td>Victory party under the slide!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Mr. Bergstrom! Mr. Bergstrom!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Do you know where I could find him?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   raw_character_text                         spoken_words\n",
       "1        Lisa Simpson               Where's Mr. Bergstrom?\n",
       "3        Lisa Simpson           That life is worth living.\n",
       "7        Bart Simpson       Victory party under the slide!\n",
       "8        Lisa Simpson        Mr. Bergstrom! Mr. Bergstrom!\n",
       "10       Lisa Simpson  Do you know where I could find him?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filtrado por personajes principales\n",
    "main_characters =  [\"Lisa Simpson\", \"Bart Simpson\", \"Homer Simpson\", \"Marge Simpson\"]\n",
    "documents = documents[documents[\"raw_character_text\"].isin(main_characters)]\n",
    "print(documents.shape)\n",
    "display(documents.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sublist(list:list, n:int) -> list:\n",
    "    \"\"\"\n",
    "    Obtener sublistas de una lista\n",
    "    \"\"\"\n",
    "    return [list[i:i+n] for i in range(0, len(list), n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_characters(characters: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extraer dialogos para todos los personajes en grupos de 5 oraciones\n",
    "    \"\"\"\n",
    "    sentences_big = get_sublist(characters['spoken_words'].tolist(),5)\n",
    "    df = pd.DataFrame({ 'spoken_words': sentences_big})\n",
    "    for index,dialogue in df['spoken_words'].items():\n",
    "        df.loc[index,\"spoken_words_concatenated\"] = ' '.join(dialogue)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_dialogue_concatenated = documents.groupby(\"raw_character_text\")  \\\n",
    "                                    .apply(lambda x: get_vectors_characters(x)) \\\n",
    "                                    .reset_index(\"raw_character_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(label:str) -> int:\n",
    "    \"\"\" \n",
    "    Categoriza los labels\n",
    "    \"\"\"\n",
    "    if label == \"bart simpson\":\n",
    "        return 0\n",
    "    elif label == \"lisa simpson\":\n",
    "        return 1\n",
    "    elif label == \"homer simpson\":\n",
    "        return 2\n",
    "    else : \n",
    "        return 3\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>spoken_words_concatenated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Victory party under the slide!, Hey, thanks f...</td>\n",
       "      <td>victory party under the slide! hey, thanks for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[Somebody must have voted., Uh oh., I demand a...</td>\n",
       "      <td>somebody must have voted. uh oh. i demand a re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[Ah, Dad, if just me, Milhouse and Lewis had v...</td>\n",
       "      <td>ah, dad, if just me, milhouse and lewis had vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[Please Dad., What?, Yes sir., They're fightin...</td>\n",
       "      <td>please dad. what? yes sir. they're fighting in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[Dad, I have as much respect for you as I ever...</td>\n",
       "      <td>dad, i have as much respect for you as i ever ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   raw_character_text                                       spoken_words  \\\n",
       "0                   0  [Victory party under the slide!, Hey, thanks f...   \n",
       "1                   0  [Somebody must have voted., Uh oh., I demand a...   \n",
       "2                   0  [Ah, Dad, if just me, Milhouse and Lewis had v...   \n",
       "3                   0  [Please Dad., What?, Yes sir., They're fightin...   \n",
       "4                   0  [Dad, I have as much respect for you as I ever...   \n",
       "\n",
       "                           spoken_words_concatenated  \n",
       "0  victory party under the slide! hey, thanks for...  \n",
       "1  somebody must have voted. uh oh. i demand a re...  \n",
       "2  ah, dad, if just me, milhouse and lewis had vo...  \n",
       "3  please dad. what? yes sir. they're fighting in...  \n",
       "4  dad, i have as much respect for you as i ever ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_dialogue_concatenated[\"raw_character_text\"] = documents_dialogue_concatenated[\"raw_character_text\"].apply(lambda x: x.lower())\n",
    "documents_dialogue_concatenated[\"raw_character_text\"] = documents_dialogue_concatenated[\"raw_character_text\"].apply(convert_label)\n",
    "documents_dialogue_concatenated[\"spoken_words_concatenated\"] = documents_dialogue_concatenated[\"spoken_words_concatenated\"].apply(lambda x: x.lower())\n",
    "documents_dialogue_concatenated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = documents_dialogue_concatenated['spoken_words_concatenated'].values\n",
    "y = documents_dialogue_concatenated['raw_character_text'].values\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.2, random_state=1)\n",
    "\n",
    "y_train_categorical = tf.one_hot(y_train,4)\n",
    "y_test_categorical = tf.one_hot(y_test,4)\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((sentences_train,y_train_categorical))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((sentences_test,y_test_categorical))\n",
    "\n",
    "text_dataset = tf.data.Dataset.from_tensor_slices((sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the total set: 12123\n",
      "Number of rows of bart class: 2414\n",
      "Number of rows of lisa class: 2029\n",
      "Number of rows of homer class: 5195\n",
      "Number of rows of marge class: 2485\n",
      "---------------------------------------------\n",
      "Number of rows in the training set: 9698\n",
      "Number of rows of bart class: 1913\n",
      "Number of rows of lisa class: 1598\n",
      "Number of rows of homer class: 4171\n",
      "Number of rows of marge class: 2016\n",
      "---------------------------------------------\n",
      "Number of rows in the test set: 2425\n",
      "Number of rows of bart class: 501\n",
      "Number of rows of lisa class: 431\n",
      "Number of rows of homer class: 1024\n",
      "Number of rows of marge class: 469\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Number of rows in the total set: {}'.format(sentences.shape[0]))\n",
    "print('Number of rows of bart class: {}'.format(y[y == 0].shape[0]))\n",
    "print('Number of rows of lisa class: {}'.format(y[y == 1].shape[0]))\n",
    "print('Number of rows of homer class: {}'.format(y[y == 2].shape[0]))\n",
    "print('Number of rows of marge class: {}'.format(y[y == 3].shape[0]))\n",
    "print('---------------------------------------------')\n",
    "\n",
    "print('Number of rows in the training set: {}'.format(sentences_train.shape[0]))\n",
    "print('Number of rows of bart class: {}'.format(y_train[y_train == 0].shape[0]))\n",
    "print('Number of rows of lisa class: {}'.format(y_train[y_train == 1].shape[0]))\n",
    "print('Number of rows of homer class: {}'.format(y_train[y_train == 2].shape[0]))\n",
    "print('Number of rows of marge class: {}'.format(y_train[y_train == 3].shape[0]))\n",
    "print('---------------------------------------------')\n",
    "\n",
    "print('Number of rows in the test set: {}'.format(sentences_test.shape[0]))\n",
    "\n",
    "print('Number of rows of bart class: {}'.format(y_test[y_test == 0].shape[0]))\n",
    "print('Number of rows of lisa class: {}'.format(y_test[y_test == 1].shape[0]))\n",
    "print('Number of rows of homer class: {}'.format(y_test[y_test == 2].shape[0]))\n",
    "print('Number of rows of marge class: {}'.format(y_test[y_test == 3].shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Layer Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_binary = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=200, vocabulary=None,\n",
    "    output_mode='binary', output_sequence_length=None, pad_to_max_tokens=True,\n",
    ")\n",
    "\n",
    "vectorize_layer_binary.adapt(text_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vectorize_layer_binary.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsequ1 = Sequential()\n",
    "modelsequ1.add(vectorize_layer_binary)\n",
    "modelsequ1.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ1.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ1.add(layers.Dense(50, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ1.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc', 'mse']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 3s 22ms/step - loss: 1.3216 - acc: 0.4269 - mse: 0.1786 - val_loss: 1.3158 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 1.3072 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3149 - val_acc: 0.4223 - val_mse: 0.1775\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 1.3060 - acc: 0.4301 - mse: 0.1764 - val_loss: 1.3135 - val_acc: 0.4223 - val_mse: 0.1773\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 1.3044 - acc: 0.4301 - mse: 0.1762 - val_loss: 1.3114 - val_acc: 0.4223 - val_mse: 0.1771\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 1.3020 - acc: 0.4301 - mse: 0.1759 - val_loss: 1.3096 - val_acc: 0.4223 - val_mse: 0.1768\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 1.2994 - acc: 0.4301 - mse: 0.1756 - val_loss: 1.3087 - val_acc: 0.4223 - val_mse: 0.1767\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.2978 - acc: 0.4302 - mse: 0.1754 - val_loss: 1.3081 - val_acc: 0.4223 - val_mse: 0.1766\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 1.2969 - acc: 0.4302 - mse: 0.1753 - val_loss: 1.3079 - val_acc: 0.4223 - val_mse: 0.1766\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 1.2964 - acc: 0.4299 - mse: 0.1753 - val_loss: 1.3076 - val_acc: 0.4223 - val_mse: 0.1766\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 1.2960 - acc: 0.4299 - mse: 0.1752 - val_loss: 1.3072 - val_acc: 0.4223 - val_mse: 0.1765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25c6114d640>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 10\n",
    "\n",
    "# Fit the model using the train and test datasets.\n",
    "modelsequ1.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, 200)              0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 100)          20000     \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 100)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 50)                5050      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,254\n",
      "Trainable params: 25,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelsequ1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 1s 5ms/step - loss: 1.3072 - acc: 0.4223 - mse: 0.1765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3071997165679932, 0.4222680330276489, 0.1765207052230835]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsequ1.evaluate(sentences_test,y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       501\n",
      "           1       0.00      0.00      0.00       431\n",
      "           2       0.42      1.00      0.59      1024\n",
      "           3       0.00      0.00      0.00       469\n",
      "\n",
      "    accuracy                           0.42      2425\n",
      "   macro avg       0.11      0.25      0.15      2425\n",
      "weighted avg       0.18      0.42      0.25      2425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = modelsequ1.predict(sentences_test, batch_size=100, verbose=1) \n",
    "predicted = np.argmax(pred, axis=1)  \n",
    "report = classification_report(np.argmax(y_test_categorical, axis=1), predicted)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Layer count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_count = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=200, vocabulary=None,\n",
    "    output_mode='count', output_sequence_length=None, pad_to_max_tokens=True,\n",
    ")\n",
    "\n",
    "vectorize_layer_count.adapt(text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = len(vectorize_layer_count.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsequ2 = Sequential()\n",
    "modelsequ2.add(vectorize_layer_count)\n",
    "modelsequ2.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ2.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ2.add(layers.Dense(50, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ2.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ2.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc', 'mse']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 4s 29ms/step - loss: 1.3561 - acc: 0.4184 - mse: 0.1835 - val_loss: 1.3392 - val_acc: 0.4223 - val_mse: 0.1812\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 3s 27ms/step - loss: 1.3248 - acc: 0.4301 - mse: 0.1791 - val_loss: 1.3233 - val_acc: 0.4223 - val_mse: 0.1788\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 3s 27ms/step - loss: 1.3134 - acc: 0.4301 - mse: 0.1774 - val_loss: 1.3182 - val_acc: 0.4223 - val_mse: 0.1780\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 3s 28ms/step - loss: 1.3095 - acc: 0.4301 - mse: 0.1768 - val_loss: 1.3168 - val_acc: 0.4223 - val_mse: 0.1778\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 3s 28ms/step - loss: 1.3081 - acc: 0.4301 - mse: 0.1766 - val_loss: 1.3164 - val_acc: 0.4223 - val_mse: 0.1777\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 4s 42ms/step - loss: 1.3076 - acc: 0.4301 - mse: 0.1766 - val_loss: 1.3164 - val_acc: 0.4223 - val_mse: 0.1777\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 4s 37ms/step - loss: 1.3075 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3164 - val_acc: 0.4223 - val_mse: 0.1777\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 3s 31ms/step - loss: 1.3074 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3164 - val_acc: 0.4223 - val_mse: 0.1777\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 2s 25ms/step - loss: 1.3073 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3164 - val_acc: 0.4223 - val_mse: 0.1777\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 3s 29ms/step - loss: 1.3073 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3164 - val_acc: 0.4223 - val_mse: 0.1777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25c62167e20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "\n",
    "modelsequ2.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (TextV  (None, 200)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 100)          20000     \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 100)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 50)                5050      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,254\n",
      "Trainable params: 25,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelsequ2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 6ms/step - loss: 1.3164 - acc: 0.4223 - mse: 0.1777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3163774013519287, 0.4222680330276489, 0.17770437896251678]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsequ2.evaluate(sentences_test,y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 13ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       501\n",
      "           1       0.00      0.00      0.00       431\n",
      "           2       0.42      1.00      0.59      1024\n",
      "           3       0.00      0.00      0.00       469\n",
      "\n",
      "    accuracy                           0.42      2425\n",
      "   macro avg       0.11      0.25      0.15      2425\n",
      "weighted avg       0.18      0.42      0.25      2425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = modelsequ2.predict(sentences_test, batch_size=100, verbose=1) \n",
    "predicted = np.argmax(pred, axis=1)  \n",
    "report = classification_report(np.argmax(y_test_categorical, axis=1), predicted)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize layer int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_int = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=200, vocabulary=None,\n",
    "    output_mode='int', output_sequence_length=None,\n",
    ")\n",
    "\n",
    "vectorize_layer_int.adapt(text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = len(vectorize_layer_int.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsequ3 = Sequential()\n",
    "modelsequ3.add(vectorize_layer_int)\n",
    "modelsequ3.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ3.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ3.add(layers.Dense(50, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ3.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ3.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc', 'mse']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 5s 30ms/step - loss: 1.3567 - acc: 0.4192 - mse: 0.1836 - val_loss: 1.3385 - val_acc: 0.4223 - val_mse: 0.1811\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 3s 28ms/step - loss: 1.3240 - acc: 0.4301 - mse: 0.1790 - val_loss: 1.3223 - val_acc: 0.4223 - val_mse: 0.1787\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 3s 26ms/step - loss: 1.3129 - acc: 0.4301 - mse: 0.1773 - val_loss: 1.3177 - val_acc: 0.4223 - val_mse: 0.1779\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 3s 29ms/step - loss: 1.3093 - acc: 0.4301 - mse: 0.1768 - val_loss: 1.3164 - val_acc: 0.4223 - val_mse: 0.1777\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 2s 22ms/step - loss: 1.3080 - acc: 0.4301 - mse: 0.1766 - val_loss: 1.3160 - val_acc: 0.4223 - val_mse: 0.1777\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 2s 21ms/step - loss: 1.3074 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3157 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 1.3071 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3155 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 3s 26ms/step - loss: 1.3068 - acc: 0.4301 - mse: 0.1764 - val_loss: 1.3153 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 3s 26ms/step - loss: 1.3065 - acc: 0.4301 - mse: 0.1764 - val_loss: 1.3151 - val_acc: 0.4223 - val_mse: 0.1775\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 3s 26ms/step - loss: 1.3063 - acc: 0.4301 - mse: 0.1764 - val_loss: 1.3149 - val_acc: 0.4223 - val_mse: 0.1775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25c62311760>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "modelsequ3.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_2 (TextV  (None, None)             0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         20000     \n",
      "                                                                 \n",
      " global_average_pooling1d_2   (None, 100)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 50)                5050      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,254\n",
      "Trainable params: 25,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelsequ3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 1s 4ms/step - loss: 1.3150 - acc: 0.4223 - mse: 0.1775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.314957857131958, 0.4222680330276489, 0.177505761384964]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsequ3.evaluate(sentences_test,y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 8ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       501\n",
      "           1       0.00      0.00      0.00       431\n",
      "           2       0.42      1.00      0.59      1024\n",
      "           3       0.00      0.00      0.00       469\n",
      "\n",
      "    accuracy                           0.42      2425\n",
      "   macro avg       0.11      0.25      0.15      2425\n",
      "weighted avg       0.18      0.42      0.25      2425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = modelsequ3.predict(sentences_test, batch_size=100, verbose=1) \n",
    "predicted = np.argmax(pred, axis=1)  \n",
    "report = classification_report(np.argmax(y_test_categorical, axis=1), predicted)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Layer Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_binary = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=200, vocabulary=None,\n",
    "    output_mode='binary', output_sequence_length=None, pad_to_max_tokens=True,\n",
    ")\n",
    "\n",
    "vectorize_layer_binary.adapt(text_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vectorize_layer_binary.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsequ1 = Sequential()\n",
    "modelsequ1.add(vectorize_layer_binary)\n",
    "modelsequ1.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ1.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ1.add(layers.Dense(50, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ1.add(layers.Dense(50, activation='relu', name=\"Hidden_layer_2\"))\n",
    "modelsequ1.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc', 'mse']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 4s 26ms/step - loss: 1.3205 - acc: 0.4253 - mse: 0.1784 - val_loss: 1.3158 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 2s 22ms/step - loss: 1.3073 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3149 - val_acc: 0.4223 - val_mse: 0.1775\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 2s 26ms/step - loss: 1.3062 - acc: 0.4301 - mse: 0.1764 - val_loss: 1.3136 - val_acc: 0.4223 - val_mse: 0.1773\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 2s 23ms/step - loss: 1.3039 - acc: 0.4301 - mse: 0.1761 - val_loss: 1.3141 - val_acc: 0.4223 - val_mse: 0.1773\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 2s 21ms/step - loss: 1.3007 - acc: 0.4301 - mse: 0.1757 - val_loss: 1.3151 - val_acc: 0.4223 - val_mse: 0.1774\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 1.2985 - acc: 0.4301 - mse: 0.1755 - val_loss: 1.3149 - val_acc: 0.4223 - val_mse: 0.1774\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 2s 23ms/step - loss: 1.2975 - acc: 0.4301 - mse: 0.1754 - val_loss: 1.3156 - val_acc: 0.4223 - val_mse: 0.1774\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 2s 23ms/step - loss: 1.2972 - acc: 0.4301 - mse: 0.1754 - val_loss: 1.3121 - val_acc: 0.4223 - val_mse: 0.1770\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 2s 23ms/step - loss: 1.2965 - acc: 0.4301 - mse: 0.1753 - val_loss: 1.3111 - val_acc: 0.4223 - val_mse: 0.1769\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 2s 21ms/step - loss: 1.2960 - acc: 0.4301 - mse: 0.1752 - val_loss: 1.3105 - val_acc: 0.4223 - val_mse: 0.1768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25c67cdfb50>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 10\n",
    "\n",
    "# Fit the model using the train and test datasets.\n",
    "modelsequ1.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_3 (TextV  (None, 200)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 100)          20000     \n",
      "                                                                 \n",
      " global_average_pooling1d_3   (None, 100)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 50)                5050      \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 50)                2550      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,804\n",
      "Trainable params: 27,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelsequ1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 5ms/step - loss: 1.3105 - acc: 0.4223 - mse: 0.1768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3105006217956543, 0.4222680330276489, 0.1768207550048828]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsequ1.evaluate(sentences_test,y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 10ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       501\n",
      "           1       0.00      0.00      0.00       431\n",
      "           2       0.42      1.00      0.59      1024\n",
      "           3       0.00      0.00      0.00       469\n",
      "\n",
      "    accuracy                           0.42      2425\n",
      "   macro avg       0.11      0.25      0.15      2425\n",
      "weighted avg       0.18      0.42      0.25      2425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = modelsequ1.predict(sentences_test, batch_size=100, verbose=1) \n",
    "predicted = np.argmax(pred, axis=1)  \n",
    "report = classification_report(np.argmax(y_test_categorical, axis=1), predicted)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Layer count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_count = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=200, vocabulary=None,\n",
    "    output_mode='count', output_sequence_length=None, pad_to_max_tokens=True,\n",
    ")\n",
    "\n",
    "vectorize_layer_count.adapt(text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = len(vectorize_layer_count.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsequ2 = Sequential()\n",
    "modelsequ2.add(vectorize_layer_count)\n",
    "modelsequ2.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ2.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ2.add(layers.Dense(50, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ2.add(layers.Dense(50, activation='relu', name=\"Hidden_layer_2\"))\n",
    "modelsequ2.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ2.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc', 'mse']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 5s 37ms/step - loss: 1.3518 - acc: 0.4301 - mse: 0.1829 - val_loss: 1.3343 - val_acc: 0.4223 - val_mse: 0.1805\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 4s 39ms/step - loss: 1.3207 - acc: 0.4301 - mse: 0.1785 - val_loss: 1.3196 - val_acc: 0.4223 - val_mse: 0.1782\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 3s 30ms/step - loss: 1.3109 - acc: 0.4301 - mse: 0.1770 - val_loss: 1.3161 - val_acc: 0.4223 - val_mse: 0.1777\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 3s 36ms/step - loss: 1.3082 - acc: 0.4301 - mse: 0.1766 - val_loss: 1.3156 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 4s 39ms/step - loss: 1.3075 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3157 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 3s 33ms/step - loss: 1.3073 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3158 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 4s 37ms/step - loss: 1.3072 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3159 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 3s 30ms/step - loss: 1.3071 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3160 - val_acc: 0.4223 - val_mse: 0.1777\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 3s 32ms/step - loss: 1.3071 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3160 - val_acc: 0.4223 - val_mse: 0.1777\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 4s 41ms/step - loss: 1.3071 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3161 - val_acc: 0.4223 - val_mse: 0.1777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25c69e4aa60>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "\n",
    "modelsequ2.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_4 (TextV  (None, 200)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 100)          20000     \n",
      "                                                                 \n",
      " global_average_pooling1d_4   (None, 100)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 50)                5050      \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 50)                2550      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,804\n",
      "Trainable params: 27,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelsequ2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 1s 6ms/step - loss: 1.3161 - acc: 0.4223 - mse: 0.1777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3160678148269653, 0.4222680330276489, 0.17766143381595612]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsequ2.evaluate(sentences_test,y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 11ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       501\n",
      "           1       0.00      0.00      0.00       431\n",
      "           2       0.42      1.00      0.59      1024\n",
      "           3       0.00      0.00      0.00       469\n",
      "\n",
      "    accuracy                           0.42      2425\n",
      "   macro avg       0.11      0.25      0.15      2425\n",
      "weighted avg       0.18      0.42      0.25      2425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = modelsequ2.predict(sentences_test, batch_size=100, verbose=1) \n",
    "predicted = np.argmax(pred, axis=1)  \n",
    "report = classification_report(np.argmax(y_test_categorical, axis=1), predicted)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize layer int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_int = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=200, vocabulary=None,\n",
    "    output_mode='int', output_sequence_length=None,\n",
    ")\n",
    "\n",
    "vectorize_layer_int.adapt(text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = len(vectorize_layer_int.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsequ3 = Sequential()\n",
    "modelsequ3.add(vectorize_layer_int)\n",
    "modelsequ3.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ3.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ3.add(layers.Dense(50, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ3.add(layers.Dense(50, activation='relu', name=\"Hidden_layer_2\"))\n",
    "modelsequ3.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ3.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc', 'mse']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 4s 25ms/step - loss: 1.3535 - acc: 0.4239 - mse: 0.1832 - val_loss: 1.3356 - val_acc: 0.4223 - val_mse: 0.1807\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 1.3205 - acc: 0.4301 - mse: 0.1786 - val_loss: 1.3209 - val_acc: 0.4223 - val_mse: 0.1785\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 2s 23ms/step - loss: 1.3108 - acc: 0.4301 - mse: 0.1771 - val_loss: 1.3174 - val_acc: 0.4223 - val_mse: 0.1779\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 2s 22ms/step - loss: 1.3082 - acc: 0.4301 - mse: 0.1767 - val_loss: 1.3166 - val_acc: 0.4223 - val_mse: 0.1778\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 2s 21ms/step - loss: 1.3075 - acc: 0.4301 - mse: 0.1766 - val_loss: 1.3163 - val_acc: 0.4223 - val_mse: 0.1777\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 2s 21ms/step - loss: 1.3073 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3162 - val_acc: 0.4223 - val_mse: 0.1777\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 1.3071 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3160 - val_acc: 0.4223 - val_mse: 0.1777\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 2s 23ms/step - loss: 1.3069 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3159 - val_acc: 0.4223 - val_mse: 0.1777\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 1.3068 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3157 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 2s 22ms/step - loss: 1.3067 - acc: 0.4301 - mse: 0.1764 - val_loss: 1.3156 - val_acc: 0.4223 - val_mse: 0.1776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25c6afcf5b0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "modelsequ3.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_5 (TextV  (None, None)             0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         20000     \n",
      "                                                                 \n",
      " global_average_pooling1d_5   (None, 100)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 50)                5050      \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 50)                2550      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,804\n",
      "Trainable params: 27,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelsequ3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 5ms/step - loss: 1.3156 - acc: 0.4223 - mse: 0.1776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3156100511550903, 0.4222680330276489, 0.17761465907096863]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsequ3.evaluate(sentences_test,y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 10ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       501\n",
      "           1       0.00      0.00      0.00       431\n",
      "           2       0.42      1.00      0.59      1024\n",
      "           3       0.00      0.00      0.00       469\n",
      "\n",
      "    accuracy                           0.42      2425\n",
      "   macro avg       0.11      0.25      0.15      2425\n",
      "weighted avg       0.18      0.42      0.25      2425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = modelsequ3.predict(sentences_test, batch_size=100, verbose=1) \n",
    "predicted = np.argmax(pred, axis=1)  \n",
    "report = classification_report(np.argmax(y_test_categorical, axis=1), predicted)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Layer Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_binary = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=200, vocabulary=None,\n",
    "    output_mode='binary', output_sequence_length=None, pad_to_max_tokens=True,\n",
    ")\n",
    "\n",
    "vectorize_layer_binary.adapt(text_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vectorize_layer_binary.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsequ1 = Sequential()\n",
    "modelsequ1.add(vectorize_layer_binary)\n",
    "modelsequ1.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ1.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ1.add(layers.Dense(100, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ1.add(layers.Dense(50, activation='relu', name=\"Hidden_layer_2\"))\n",
    "modelsequ1.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc', 'mse']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 4s 26ms/step - loss: 1.3171 - acc: 0.4301 - mse: 0.1779 - val_loss: 1.3158 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 2s 23ms/step - loss: 1.3075 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3148 - val_acc: 0.4223 - val_mse: 0.1775\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 2s 22ms/step - loss: 1.3063 - acc: 0.4301 - mse: 0.1764 - val_loss: 1.3134 - val_acc: 0.4223 - val_mse: 0.1773\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 2s 21ms/step - loss: 1.3043 - acc: 0.4301 - mse: 0.1761 - val_loss: 1.3120 - val_acc: 0.4223 - val_mse: 0.1771\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 2s 21ms/step - loss: 1.3008 - acc: 0.4301 - mse: 0.1757 - val_loss: 1.3140 - val_acc: 0.4223 - val_mse: 0.1773\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 1.2981 - acc: 0.4301 - mse: 0.1755 - val_loss: 1.3158 - val_acc: 0.4223 - val_mse: 0.1774\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 2s 21ms/step - loss: 1.2974 - acc: 0.4301 - mse: 0.1754 - val_loss: 1.3141 - val_acc: 0.4223 - val_mse: 0.1772\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 1.2967 - acc: 0.4301 - mse: 0.1753 - val_loss: 1.3127 - val_acc: 0.4223 - val_mse: 0.1770\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 1.2962 - acc: 0.4301 - mse: 0.1752 - val_loss: 1.3121 - val_acc: 0.4223 - val_mse: 0.1770\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 2s 21ms/step - loss: 1.2959 - acc: 0.4301 - mse: 0.1752 - val_loss: 1.3101 - val_acc: 0.4223 - val_mse: 0.1768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25c6248c640>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 10\n",
    "\n",
    "# Fit the model using the train and test datasets.\n",
    "modelsequ1.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_6 (TextV  (None, 200)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 100)          20000     \n",
      "                                                                 \n",
      " global_average_pooling1d_6   (None, 100)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 100)               10100     \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 50)                5050      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,354\n",
      "Trainable params: 35,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelsequ1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 5ms/step - loss: 1.3101 - acc: 0.4223 - mse: 0.1768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3101179599761963, 0.4222680330276489, 0.17675313353538513]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsequ1.evaluate(sentences_test,y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       501\n",
      "           1       0.00      0.00      0.00       431\n",
      "           2       0.42      1.00      0.59      1024\n",
      "           3       0.00      0.00      0.00       469\n",
      "\n",
      "    accuracy                           0.42      2425\n",
      "   macro avg       0.11      0.25      0.15      2425\n",
      "weighted avg       0.18      0.42      0.25      2425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = modelsequ1.predict(sentences_test, batch_size=100, verbose=1) \n",
    "predicted = np.argmax(pred, axis=1)  \n",
    "report = classification_report(np.argmax(y_test_categorical, axis=1), predicted)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Layer count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_count = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=200, vocabulary=None,\n",
    "    output_mode='count', output_sequence_length=None, pad_to_max_tokens=True,\n",
    ")\n",
    "\n",
    "vectorize_layer_count.adapt(text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = len(vectorize_layer_count.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsequ2 = Sequential()\n",
    "modelsequ2.add(vectorize_layer_count)\n",
    "modelsequ2.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ2.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ2.add(layers.Dense(100, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ2.add(layers.Dense(50, activation='relu', name=\"Hidden_layer_2\"))\n",
    "modelsequ2.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ2.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc', 'mse']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 5s 35ms/step - loss: 1.3435 - acc: 0.4301 - mse: 0.1818 - val_loss: 1.3254 - val_acc: 0.4223 - val_mse: 0.1792\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 5s 51ms/step - loss: 1.3151 - acc: 0.4301 - mse: 0.1776 - val_loss: 1.3161 - val_acc: 0.4223 - val_mse: 0.1777\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 5s 51ms/step - loss: 1.3093 - acc: 0.4301 - mse: 0.1768 - val_loss: 1.3150 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 4s 40ms/step - loss: 1.3079 - acc: 0.4301 - mse: 0.1766 - val_loss: 1.3151 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 4s 38ms/step - loss: 1.3074 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3154 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 5s 53ms/step - loss: 1.3072 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3156 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 3s 32ms/step - loss: 1.3071 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3158 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 4s 41ms/step - loss: 1.3071 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3159 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 5s 50ms/step - loss: 1.3071 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3160 - val_acc: 0.4223 - val_mse: 0.1777\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 4s 36ms/step - loss: 1.3071 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3160 - val_acc: 0.4223 - val_mse: 0.1777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25c6256db20>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "\n",
    "modelsequ2.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_7 (TextV  (None, 200)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 100)          20000     \n",
      "                                                                 \n",
      " global_average_pooling1d_7   (None, 100)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 100)               10100     \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 50)                5050      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,354\n",
      "Trainable params: 35,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelsequ2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 1s 6ms/step - loss: 1.3160 - acc: 0.4223 - mse: 0.1777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3160150051116943, 0.4222680330276489, 0.17766331136226654]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsequ2.evaluate(sentences_test,y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       501\n",
      "           1       0.00      0.00      0.00       431\n",
      "           2       0.42      1.00      0.59      1024\n",
      "           3       0.00      0.00      0.00       469\n",
      "\n",
      "    accuracy                           0.42      2425\n",
      "   macro avg       0.11      0.25      0.15      2425\n",
      "weighted avg       0.18      0.42      0.25      2425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = modelsequ2.predict(sentences_test, batch_size=100, verbose=1) \n",
    "predicted = np.argmax(pred, axis=1)  \n",
    "report = classification_report(np.argmax(y_test_categorical, axis=1), predicted)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize layer int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_int = TextVectorization(\n",
    "    ngrams=None, \n",
    "    max_tokens=200, vocabulary=None,\n",
    "    output_mode='int', output_sequence_length=None,\n",
    ")\n",
    "\n",
    "vectorize_layer_int.adapt(text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = len(vectorize_layer_int.get_vocabulary())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsequ3 = Sequential()\n",
    "modelsequ3.add(vectorize_layer_int)\n",
    "modelsequ3.add(layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"))\n",
    "modelsequ3.add(layers.GlobalAveragePooling1D())\n",
    "modelsequ3.add(layers.Dense(100, activation='relu', name=\"Hidden_layer\"))\n",
    "modelsequ3.add(layers.Dense(50, activation='relu', name=\"Hidden_layer_2\"))\n",
    "modelsequ3.add(layers.Dense(4, activation='softmax', name=\"Output_layer\"))\n",
    "modelsequ3.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc', 'mse']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 4s 32ms/step - loss: 1.3497 - acc: 0.4220 - mse: 0.1826 - val_loss: 1.3311 - val_acc: 0.4223 - val_mse: 0.1800\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 3s 29ms/step - loss: 1.3180 - acc: 0.4301 - mse: 0.1781 - val_loss: 1.3189 - val_acc: 0.4223 - val_mse: 0.1781\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 2s 22ms/step - loss: 1.3103 - acc: 0.4301 - mse: 0.1769 - val_loss: 1.3165 - val_acc: 0.4223 - val_mse: 0.1777\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 2s 24ms/step - loss: 1.3082 - acc: 0.4301 - mse: 0.1766 - val_loss: 1.3159 - val_acc: 0.4223 - val_mse: 0.1777\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 2s 22ms/step - loss: 1.3075 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3157 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 2s 24ms/step - loss: 1.3071 - acc: 0.4301 - mse: 0.1765 - val_loss: 1.3155 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 2s 24ms/step - loss: 1.3068 - acc: 0.4301 - mse: 0.1764 - val_loss: 1.3154 - val_acc: 0.4223 - val_mse: 0.1776\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 3s 26ms/step - loss: 1.3066 - acc: 0.4301 - mse: 0.1764 - val_loss: 1.3152 - val_acc: 0.4223 - val_mse: 0.1775\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 3s 30ms/step - loss: 1.3064 - acc: 0.4301 - mse: 0.1764 - val_loss: 1.3151 - val_acc: 0.4223 - val_mse: 0.1775\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 2s 24ms/step - loss: 1.3062 - acc: 0.4301 - mse: 0.1764 - val_loss: 1.3149 - val_acc: 0.4223 - val_mse: 0.1775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25c6e49ac70>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "modelsequ3.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    validation_data=test_dataset.batch(batch_size),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_8 (TextV  (None, None)             0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         20000     \n",
      "                                                                 \n",
      " global_average_pooling1d_8   (None, 100)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " Hidden_layer (Dense)        (None, 100)               10100     \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 50)                5050      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,354\n",
      "Trainable params: 35,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelsequ3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 5ms/step - loss: 1.3148 - acc: 0.4223 - mse: 0.1775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3148270845413208, 0.4222680330276489, 0.17748606204986572]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsequ3.evaluate(sentences_test,y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       501\n",
      "           1       0.00      0.00      0.00       431\n",
      "           2       0.42      1.00      0.59      1024\n",
      "           3       0.00      0.00      0.00       469\n",
      "\n",
      "    accuracy                           0.42      2425\n",
      "   macro avg       0.11      0.25      0.15      2425\n",
      "weighted avg       0.18      0.42      0.25      2425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\camilo\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = modelsequ3.predict(sentences_test, batch_size=100, verbose=1) \n",
    "predicted = np.argmax(pred, axis=1)  \n",
    "report = classification_report(np.argmax(y_test_categorical, axis=1), predicted)  \n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7a79e9e75522a046d95171e373010a5dca4ce6e8605d007854b2218f1d88052"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
