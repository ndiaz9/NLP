{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to model files and data\n",
    "dataset_path = 'simpsons_dataset.csv'\n",
    "models_path = './Modelos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all embedding models\n",
    "model_50 = gensim.models.Word2Vec.load(f'{models_path}/Simpsons_50_02.model')\n",
    "model_100 = gensim.models.Word2Vec.load(f'{models_path}/Simpsons_100_02.model')\n",
    "model_200 = gensim.models.Word2Vec.load(f'{models_path}/Simpsons_200_02.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158309</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I'm back.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158310</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>You see, class, my Lyme disease turned out to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158311</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Psy-cho-so-ma-tic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158312</th>\n",
       "      <td>Ralph Wiggum</td>\n",
       "      <td>Does that mean you were crazy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158313</th>\n",
       "      <td>JANEY</td>\n",
       "      <td>No, that means she was faking it.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             raw_character_text  \\\n",
       "0                   Miss Hoover   \n",
       "1                  Lisa Simpson   \n",
       "2                   Miss Hoover   \n",
       "3                  Lisa Simpson   \n",
       "4       Edna Krabappel-Flanders   \n",
       "...                         ...   \n",
       "158309              Miss Hoover   \n",
       "158310              Miss Hoover   \n",
       "158311              Miss Hoover   \n",
       "158312             Ralph Wiggum   \n",
       "158313                    JANEY   \n",
       "\n",
       "                                             spoken_words  \n",
       "0       No, actually, it was a little of both. Sometim...  \n",
       "1                                  Where's Mr. Bergstrom?  \n",
       "2       I don't know. Although I'd sure like to talk t...  \n",
       "3                              That life is worth living.  \n",
       "4       The polls will be open from now until the end ...  \n",
       "...                                                   ...  \n",
       "158309                                          I'm back.  \n",
       "158310  You see, class, my Lyme disease turned out to ...  \n",
       "158311                                 Psy-cho-so-ma-tic.  \n",
       "158312                     Does that mean you were crazy?  \n",
       "158313                  No, that means she was faking it.  \n",
       "\n",
       "[158314 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV data\n",
    "data_raw = pd.read_csv(dataset_path)\n",
    "data_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(document: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace all abbreviations with their corresponding expansion\n",
    "    \"\"\"\n",
    "    document = re.sub(r\"'cause\", \"because\", document)\n",
    "    document = re.sub(r\"o'clock\", \"of the clock\", document)\n",
    "    document = re.sub(r\"won\\'t\", \"will not\", document)\n",
    "    document = re.sub(r\"can\\'t\", \"can not\", document)\n",
    "    document = re.sub(r\"n\\'t\", \" not\", document)\n",
    "    document = re.sub(r\"\\'re\", \" are\", document)\n",
    "    document = re.sub(r\"\\'s\", \" is\", document)\n",
    "    document = re.sub(r\"\\'d\", \" would\", document)\n",
    "    document = re.sub(r\"\\'ll\", \" will\", document)\n",
    "    document = re.sub(r\"\\'t\", \" not\", document)\n",
    "    document = re.sub(r\"\\'ve\", \" have\", document)\n",
    "    document = re.sub(r\"\\'m\", \" am\", document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_numbers(document: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace number appearances with 'number'\n",
    "    \"\"\"\n",
    "    # Case 1: Combination of numbers and letters (Eg. 2nd -> number)\n",
    "    document = re.sub('[a-zA-Z]+[0-9]+[a-zA-Z]+', 'number', document)\n",
    "    document = re.sub('[0-9]+[a-zA-Z]+|[a-zA-Z]+[0-9]+', 'number', document)\n",
    "    # Case 2: Decimal numbers (Eg. 2.1 -> number)\n",
    "    document = re.sub('[0-9]+\\.+[0-9]+', 'number', document)\n",
    "    # Case 3: Numbers between spaces (Eg. 220 888 -> number)\n",
    "    document = re.sub('([0-9]+\\s)*[0-9]+', 'number', document)\n",
    "    # Case 4: One or more of the previous cases (Eg. number number -> number)\n",
    "    document = re.sub('((number)+\\s)*(number)+', 'number', document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document: str) -> list:\n",
    "    \"\"\"\n",
    "    iterate over all words in document identifing the word and frecuency\n",
    "    remove all the problematic characters over the word\n",
    "    and return a dictionary with the word as the key and the frecuency as the value\n",
    "    \"\"\"\n",
    "    document = document.lower()\n",
    "    document = expand_contractions(document)\n",
    "    document = replace_numbers(document)\n",
    "    document = re.sub('[^A-Za-z]+', ' ', document)\n",
    "    document = document.split()\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_sentences(data: pd.DataFrame, characters: list, sentences_per_group: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Groups sentences from the same character. Returns a compressed DataFrame consisting\n",
    "    of sentences_per_group concatenated sentences for each character\n",
    "    \"\"\"\n",
    "    out_df = pd.DataFrame()\n",
    "    for character in characters:\n",
    "        sentences_subset = data[data['raw_character_text'] == character]\n",
    "        groups = []\n",
    "        for i in range(int(np.ceil(sentences_subset.shape[0]/sentences_per_group))):\n",
    "            groups.append(' '.join(sentences_subset[(sentences_per_group*i):(sentences_per_group*(i+1))]['spoken_words']))\n",
    "        out_df = pd.concat([out_df, pd.DataFrame({'character': character, 'sentences': groups})])\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_in_embeddings(sentence: str):\n",
    "    \"\"\"\n",
    "    Returns True if all words in a sentence have an embedding representation, False if not \n",
    "    \"\"\"\n",
    "    for word in preprocessing(sentence):\n",
    "        if word not in model_50.wv.key_to_index:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158309</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I'm back.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158310</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>You see, class, my Lyme disease turned out to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158311</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Psy-cho-so-ma-tic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158312</th>\n",
       "      <td>Ralph Wiggum</td>\n",
       "      <td>Does that mean you were crazy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158313</th>\n",
       "      <td>JANEY</td>\n",
       "      <td>No, that means she was faking it.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126646 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             raw_character_text  \\\n",
       "0                   Miss Hoover   \n",
       "1                  Lisa Simpson   \n",
       "2                   Miss Hoover   \n",
       "3                  Lisa Simpson   \n",
       "4       Edna Krabappel-Flanders   \n",
       "...                         ...   \n",
       "158309              Miss Hoover   \n",
       "158310              Miss Hoover   \n",
       "158311              Miss Hoover   \n",
       "158312             Ralph Wiggum   \n",
       "158313                    JANEY   \n",
       "\n",
       "                                             spoken_words  \n",
       "0       No, actually, it was a little of both. Sometim...  \n",
       "1                                  Where's Mr. Bergstrom?  \n",
       "2       I don't know. Although I'd sure like to talk t...  \n",
       "3                              That life is worth living.  \n",
       "4       The polls will be open from now until the end ...  \n",
       "...                                                   ...  \n",
       "158309                                          I'm back.  \n",
       "158310  You see, class, my Lyme disease turned out to ...  \n",
       "158311                                 Psy-cho-so-ma-tic.  \n",
       "158312                     Does that mean you were crazy?  \n",
       "158313                  No, that means she was faking it.  \n",
       "\n",
       "[126646 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove null values and duplicate rows\n",
    "data = data_raw.dropna().drop_duplicates()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158308</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Good morning, Lisa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158309</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I'm back.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158310</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>You see, class, my Lyme disease turned out to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158312</th>\n",
       "      <td>Ralph Wiggum</td>\n",
       "      <td>Does that mean you were crazy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158313</th>\n",
       "      <td>JANEY</td>\n",
       "      <td>No, that means she was faking it.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106114 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             raw_character_text  \\\n",
       "0                   Miss Hoover   \n",
       "1                  Lisa Simpson   \n",
       "2                   Miss Hoover   \n",
       "3                  Lisa Simpson   \n",
       "4       Edna Krabappel-Flanders   \n",
       "...                         ...   \n",
       "158308              Miss Hoover   \n",
       "158309              Miss Hoover   \n",
       "158310              Miss Hoover   \n",
       "158312             Ralph Wiggum   \n",
       "158313                    JANEY   \n",
       "\n",
       "                                             spoken_words  \n",
       "0       No, actually, it was a little of both. Sometim...  \n",
       "1                                  Where's Mr. Bergstrom?  \n",
       "2       I don't know. Although I'd sure like to talk t...  \n",
       "3                              That life is worth living.  \n",
       "4       The polls will be open from now until the end ...  \n",
       "...                                                   ...  \n",
       "158308                                Good morning, Lisa.  \n",
       "158309                                          I'm back.  \n",
       "158310  You see, class, my Lyme disease turned out to ...  \n",
       "158312                     Does that mean you were crazy?  \n",
       "158313                  No, that means she was faking it.  \n",
       "\n",
       "[106114 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove sentences without vector representation\n",
    "data_filtered = data[data['spoken_words'].apply(words_in_embeddings)]\n",
    "data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Homer Simpson         22745\n",
       "Marge Simpson         11015\n",
       "Bart Simpson          10768\n",
       "Lisa Simpson           8645\n",
       "Moe Szyslak            2241\n",
       "                      ...  \n",
       "SPOILED 2-YEAR-OLD        1\n",
       "Sun                       1\n",
       "Applicants                1\n",
       "Statue of Liberty         1\n",
       "6th Graders               1\n",
       "Name: raw_character_text, Length: 5548, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get most common characters\n",
    "data_filtered['raw_character_text'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bart Simpson</td>\n",
       "      <td>Victory party under the slide!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Mr. Bergstrom! Mr. Bergstrom!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Do you know where I could find him?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158299</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Can we have wine?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158301</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Can I have wine?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158303</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Does Bart have to be there?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158305</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Can we do it this week?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158307</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Mr. Bergstrom, we request the pleasure of your...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53173 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       raw_character_text                                       spoken_words\n",
       "1            Lisa Simpson                             Where's Mr. Bergstrom?\n",
       "3            Lisa Simpson                         That life is worth living.\n",
       "7            Bart Simpson                     Victory party under the slide!\n",
       "9            Lisa Simpson                      Mr. Bergstrom! Mr. Bergstrom!\n",
       "11           Lisa Simpson                Do you know where I could find him?\n",
       "...                   ...                                                ...\n",
       "158299       Lisa Simpson                                  Can we have wine?\n",
       "158301       Lisa Simpson                                   Can I have wine?\n",
       "158303       Lisa Simpson                        Does Bart have to be there?\n",
       "158305       Lisa Simpson                            Can we do it this week?\n",
       "158307       Lisa Simpson  Mr. Bergstrom, we request the pleasure of your...\n",
       "\n",
       "[53173 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter dataset leaving the four most common characters\n",
    "characters = ['Homer Simpson','Marge Simpson','Bart Simpson','Lisa Simpson']\n",
    "data_filtered = data_filtered.query('raw_character_text in @characters')\n",
    "data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>Never thrown a party? What about that big bash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>I don't think you realize what you're saying. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>No, no, no. I just wish I knew what to say. Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>Come on, you're holding out on me. Think nothi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>And is this Martin guy going to get to do anyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>No, no. You don't understand. When Mr. Bergstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Yes! Yes, Mr. Bergstrom? No. Homework's not my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>When? Mr. Bergstrom! Ewww. Gross. Oh Lord. He ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Oh, Mom, that's wonderful. Can I find out his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Can we have wine? Can I have wine? Does Bart h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10635 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          character                                          sentences\n",
       "0     Homer Simpson  Never thrown a party? What about that big bash...\n",
       "1     Homer Simpson  I don't think you realize what you're saying. ...\n",
       "2     Homer Simpson  No, no, no. I just wish I knew what to say. Al...\n",
       "3     Homer Simpson  Come on, you're holding out on me. Think nothi...\n",
       "4     Homer Simpson  And is this Martin guy going to get to do anyt...\n",
       "...             ...                                                ...\n",
       "1724   Lisa Simpson  No, no. You don't understand. When Mr. Bergstr...\n",
       "1725   Lisa Simpson  Yes! Yes, Mr. Bergstrom? No. Homework's not my...\n",
       "1726   Lisa Simpson  When? Mr. Bergstrom! Ewww. Gross. Oh Lord. He ...\n",
       "1727   Lisa Simpson  Oh, Mom, that's wonderful. Can I find out his ...\n",
       "1728   Lisa Simpson  Can we have wine? Can I have wine? Does Bart h...\n",
       "\n",
       "[10635 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new dataset with groups of five sentences\n",
    "data_grouped = group_sentences(data_filtered, characters, 5)\n",
    "data_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data in training, test and validation sets\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(data_grouped['sentences'], data_grouped['character'], test_size=0.2, random_state=0)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Homer Simpson</th>\n",
       "      <th>Marge Simpson</th>\n",
       "      <th>Bart Simpson</th>\n",
       "      <th>Lisa Simpson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>2948</td>\n",
       "      <td>1416</td>\n",
       "      <td>1358</td>\n",
       "      <td>1084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>720</td>\n",
       "      <td>359</td>\n",
       "      <td>344</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>881</td>\n",
       "      <td>428</td>\n",
       "      <td>452</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Homer Simpson  Marge Simpson  Bart Simpson  Lisa Simpson\n",
       "train                2948           1416          1358          1084\n",
       "validation            720            359           344           279\n",
       "test                  881            428           452           366"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count each characters' appearance in each set \n",
    "summary = pd.DataFrame([y_train.value_counts(), y_val.value_counts(), y_test.value_counts()], index=['train', 'validation', 'test'])\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size 50 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_embedding_50(sentence: str):\n",
    "    \"\"\"\n",
    "    Returns the element-wise mean of the embeddings that represent each word in a sentence\n",
    "    \"\"\"\n",
    "    token_list = preprocessing(sentence)\n",
    "    return np.mean(model_50.wv[token_list], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1133    [0.08307874, -0.34154454, -0.21555215, -0.6717...\n",
       "877     [0.0988574, -0.113601886, -0.051913228, -1.012...\n",
       "3829    [0.29998192, -0.33769158, -0.27057227, -0.2832...\n",
       "1182    [0.3900361, -0.3936474, -0.17824244, -0.492518...\n",
       "1518    [0.24746819, -0.43653318, -0.015143045, -0.550...\n",
       "                              ...                        \n",
       "3035    [0.44571173, -0.38923872, -0.15169472, -0.2831...\n",
       "3604    [0.2916982, -0.0005521466, -0.17822812, -0.224...\n",
       "3425    [0.22261555, -0.087218, -0.025895413, -0.42048...\n",
       "3717    [0.40869293, -0.290884, -0.4478682, -0.7300225...\n",
       "3266    [0.18771069, 0.02331811, -0.011993099, -0.3344...\n",
       "Name: sentences, Length: 6806, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform sentences into vectors\n",
    "x_train_50 = x_train.apply(sentence_to_embedding_50)\n",
    "x_val_50 = x_val.apply(sentence_to_embedding_50)\n",
    "x_test_50 = x_test.apply(sentence_to_embedding_50)\n",
    "x_train_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-Hot encode labels\n",
    "encoder = LabelBinarizer()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_val_encoded = encoder.transform(y_val)\n",
    "y_test_encoded = encoder.transform(y_test)\n",
    "y_train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classification model\n",
    "model = Sequential()\n",
    "model.add(Dense(50,  activation='relu', name=\"Input_layer\"))\n",
    "model.add(Dense(25, activation='relu', name=\"Hidden_layer_1\"))\n",
    "model.add(Dense(25, activation='relu', name=\"Hidden_layer_2\"))\n",
    "model.add(Dense(4, activation='softmax', name=\"Output_layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 1.3194 - acc: 0.4146 - val_loss: 1.3108 - val_acc: 0.4230\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2985 - acc: 0.4329 - val_loss: 1.3032 - val_acc: 0.4230\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2892 - acc: 0.4329 - val_loss: 1.2943 - val_acc: 0.4230\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2793 - acc: 0.4329 - val_loss: 1.2856 - val_acc: 0.4230\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2680 - acc: 0.4340 - val_loss: 1.2763 - val_acc: 0.4242\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2561 - acc: 0.4373 - val_loss: 1.2654 - val_acc: 0.4342\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2438 - acc: 0.4456 - val_loss: 1.2585 - val_acc: 0.4360\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2311 - acc: 0.4521 - val_loss: 1.2476 - val_acc: 0.4407\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2204 - acc: 0.4555 - val_loss: 1.2448 - val_acc: 0.4442\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2107 - acc: 0.4630 - val_loss: 1.2344 - val_acc: 0.4489\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2030 - acc: 0.4684 - val_loss: 1.2278 - val_acc: 0.4536\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1950 - acc: 0.4737 - val_loss: 1.2266 - val_acc: 0.4647\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1860 - acc: 0.4831 - val_loss: 1.2159 - val_acc: 0.4647\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1786 - acc: 0.4819 - val_loss: 1.2159 - val_acc: 0.4812\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1707 - acc: 0.4952 - val_loss: 1.2056 - val_acc: 0.4771\n",
      "Epoch 16/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1642 - acc: 0.4975 - val_loss: 1.1951 - val_acc: 0.4841\n",
      "Epoch 17/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1577 - acc: 0.5018 - val_loss: 1.2038 - val_acc: 0.4783\n",
      "Epoch 18/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1516 - acc: 0.5032 - val_loss: 1.1868 - val_acc: 0.4853\n",
      "Epoch 19/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1496 - acc: 0.5109 - val_loss: 1.1809 - val_acc: 0.4906\n",
      "Epoch 20/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1432 - acc: 0.5056 - val_loss: 1.1896 - val_acc: 0.4812\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(x=np.asarray(x_train_50.to_list()).astype('float32'),\n",
    "                    y=y_train_encoded,\n",
    "                    batch_size=32, epochs=20,\n",
    "                    validation_data=(np.asarray(x_val_50.to_list()).astype('float32'), y_val_encoded),\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (Dense)         (None, 50)                2550      \n",
      "                                                                 \n",
      " Hidden_layer_1 (Dense)      (None, 25)                1275      \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 25)                650       \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 104       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,579\n",
      "Trainable params: 4,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 0s 711us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return predictions - the chosen class is the one with the highest probability\n",
    "y_pred_50 = model.predict(np.asarray(x_test_50.to_list()).astype('float32'))\n",
    "y_pred_50 = to_categorical(np.argmax(y_pred_50, axis=1), 4)\n",
    "y_pred_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      " Bart Simpson       0.39      0.05      0.09       452\n",
      "Homer Simpson       0.49      0.87      0.63       881\n",
      " Lisa Simpson       0.51      0.16      0.24       366\n",
      "Marge Simpson       0.40      0.38      0.39       428\n",
      "\n",
      "     accuracy                           0.47      2127\n",
      "    macro avg       0.45      0.36      0.34      2127\n",
      " weighted avg       0.46      0.47      0.40      2127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, encoder.inverse_transform(y_pred_50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classification model\n",
    "model = Sequential()\n",
    "model.add(Dense(50,  activation='relu', name=\"Input_layer\"))\n",
    "model.add(Dense(50, activation='relu', name=\"Hidden_layer_1\"))\n",
    "model.add(Dense(50, activation='relu', name=\"Hidden_layer_2\"))\n",
    "model.add(Dense(25, activation='relu', name=\"Hidden_layer_3\"))\n",
    "model.add(Dense(4, activation='softmax', name=\"Output_layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 1.3256 - acc: 0.4079 - val_loss: 1.3169 - val_acc: 0.4230\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.3048 - acc: 0.4331 - val_loss: 1.3149 - val_acc: 0.4230\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.3022 - acc: 0.4331 - val_loss: 1.3128 - val_acc: 0.4230\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2990 - acc: 0.4331 - val_loss: 1.3121 - val_acc: 0.4230\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2966 - acc: 0.4331 - val_loss: 1.3084 - val_acc: 0.4230\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2935 - acc: 0.4331 - val_loss: 1.3051 - val_acc: 0.4230\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2898 - acc: 0.4331 - val_loss: 1.3014 - val_acc: 0.4230\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2853 - acc: 0.4331 - val_loss: 1.2996 - val_acc: 0.4230\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2801 - acc: 0.4331 - val_loss: 1.2940 - val_acc: 0.4230\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2739 - acc: 0.4331 - val_loss: 1.2893 - val_acc: 0.4230\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2656 - acc: 0.4330 - val_loss: 1.2797 - val_acc: 0.4230\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2548 - acc: 0.4333 - val_loss: 1.2700 - val_acc: 0.4230\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2432 - acc: 0.4351 - val_loss: 1.2589 - val_acc: 0.4242\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2277 - acc: 0.4439 - val_loss: 1.2468 - val_acc: 0.4412\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2118 - acc: 0.4603 - val_loss: 1.2318 - val_acc: 0.4512\n",
      "Epoch 16/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1946 - acc: 0.4747 - val_loss: 1.2196 - val_acc: 0.4689\n",
      "Epoch 17/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1810 - acc: 0.4828 - val_loss: 1.2118 - val_acc: 0.4536\n",
      "Epoch 18/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1673 - acc: 0.4891 - val_loss: 1.2107 - val_acc: 0.4612\n",
      "Epoch 19/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1594 - acc: 0.4971 - val_loss: 1.1955 - val_acc: 0.4865\n",
      "Epoch 20/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1517 - acc: 0.5037 - val_loss: 1.1988 - val_acc: 0.4800\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(x=np.asarray(x_train_50.to_list()).astype('float32'),\n",
    "                    y=y_train_encoded,\n",
    "                    batch_size=32, epochs=20,\n",
    "                    validation_data=(np.asarray(x_val_50.to_list()).astype('float32'), y_val_encoded),\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (Dense)         (None, 50)                2550      \n",
      "                                                                 \n",
      " Hidden_layer_1 (Dense)      (None, 50)                2550      \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 50)                2550      \n",
      "                                                                 \n",
      " Hidden_layer_3 (Dense)      (None, 25)                1275      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 104       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,029\n",
      "Trainable params: 9,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 0s 705us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return predictions - the chosen class is the one with the highest probability\n",
    "y_pred_50 = model.predict(np.asarray(x_test_50.to_list()).astype('float32'))\n",
    "y_pred_50 = to_categorical(np.argmax(y_pred_50, axis=1), 4)\n",
    "y_pred_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      " Bart Simpson       0.36      0.16      0.22       452\n",
      "Homer Simpson       0.50      0.83      0.62       881\n",
      " Lisa Simpson       0.39      0.32      0.35       366\n",
      "Marge Simpson       0.53      0.18      0.27       428\n",
      "\n",
      "     accuracy                           0.47      2127\n",
      "    macro avg       0.45      0.37      0.37      2127\n",
      " weighted avg       0.46      0.47      0.42      2127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, encoder.inverse_transform(y_pred_50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classification model\n",
    "model = Sequential()\n",
    "model.add(Dense(50,  activation='relu', name=\"Input_layer\"))\n",
    "model.add(Dense(40, activation='relu', name=\"Hidden_layer_1\"))\n",
    "model.add(Dense(40, activation='relu', name=\"Hidden_layer_2\"))\n",
    "model.add(Dense(25, activation='relu', name=\"Hidden_layer_3\"))\n",
    "model.add(Dense(4, activation='softmax', name=\"Output_layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 1.3126 - acc: 0.4226 - val_loss: 1.3098 - val_acc: 0.4230\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2984 - acc: 0.4331 - val_loss: 1.3046 - val_acc: 0.4230\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2919 - acc: 0.4331 - val_loss: 1.2970 - val_acc: 0.4230\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2841 - acc: 0.4331 - val_loss: 1.2900 - val_acc: 0.4230\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2739 - acc: 0.4331 - val_loss: 1.2802 - val_acc: 0.4230\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2624 - acc: 0.4331 - val_loss: 1.2702 - val_acc: 0.4230\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2487 - acc: 0.4362 - val_loss: 1.2577 - val_acc: 0.4389\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2347 - acc: 0.4480 - val_loss: 1.2461 - val_acc: 0.4477\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2220 - acc: 0.4575 - val_loss: 1.2390 - val_acc: 0.4553\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2116 - acc: 0.4647 - val_loss: 1.2331 - val_acc: 0.4677\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2017 - acc: 0.4658 - val_loss: 1.2230 - val_acc: 0.4653\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1949 - acc: 0.4706 - val_loss: 1.2229 - val_acc: 0.4671\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1853 - acc: 0.4771 - val_loss: 1.2257 - val_acc: 0.4618\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1802 - acc: 0.4844 - val_loss: 1.2198 - val_acc: 0.4659\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1740 - acc: 0.4852 - val_loss: 1.2007 - val_acc: 0.4759\n",
      "Epoch 16/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1656 - acc: 0.4900 - val_loss: 1.2388 - val_acc: 0.4559\n",
      "Epoch 17/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1612 - acc: 0.4969 - val_loss: 1.1889 - val_acc: 0.4777\n",
      "Epoch 18/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1560 - acc: 0.4963 - val_loss: 1.1929 - val_acc: 0.4753\n",
      "Epoch 19/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1500 - acc: 0.4997 - val_loss: 1.1918 - val_acc: 0.4794\n",
      "Epoch 20/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1490 - acc: 0.5057 - val_loss: 1.1761 - val_acc: 0.4871\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(x=np.asarray(x_train_50.to_list()).astype('float32'),\n",
    "                    y=y_train_encoded,\n",
    "                    batch_size=32, epochs=20,\n",
    "                    validation_data=(np.asarray(x_val_50.to_list()).astype('float32'), y_val_encoded),\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (Dense)         (None, 50)                2550      \n",
      "                                                                 \n",
      " Hidden_layer_1 (Dense)      (None, 40)                2040      \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 40)                1640      \n",
      "                                                                 \n",
      " Hidden_layer_3 (Dense)      (None, 25)                1025      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 104       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,359\n",
      "Trainable params: 7,359\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 0s 690us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return predictions - the chosen class is the one with the highest probability\n",
    "y_pred_50 = model.predict(np.asarray(x_test_50.to_list()).astype('float32'))\n",
    "y_pred_50 = to_categorical(np.argmax(y_pred_50, axis=1), 4)\n",
    "y_pred_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      " Bart Simpson       0.38      0.14      0.20       452\n",
      "Homer Simpson       0.51      0.80      0.62       881\n",
      " Lisa Simpson       0.42      0.22      0.29       366\n",
      "Marge Simpson       0.42      0.38      0.40       428\n",
      "\n",
      "     accuracy                           0.47      2127\n",
      "    macro avg       0.43      0.38      0.38      2127\n",
      " weighted avg       0.45      0.47      0.43      2127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, encoder.inverse_transform(y_pred_50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size 100 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_embedding_100(sentence: str):\n",
    "    \"\"\"\n",
    "    Returns the element-wise mean of the embeddings that represent each word in a sentence\n",
    "    \"\"\"\n",
    "    token_list = preprocessing(sentence)\n",
    "    return np.mean(model_100.wv[token_list], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1133    [-0.19050829, 0.39607593, 0.18970492, 0.443213...\n",
       "877     [-0.290738, 0.28118527, -0.20843536, 0.365131,...\n",
       "3829    [-0.52621406, 0.64666843, 0.0261572, 0.3445141...\n",
       "1182    [-0.3504744, 0.44219318, -0.16405015, 0.423895...\n",
       "1518    [-0.13211192, 0.5161154, -0.07311637, 0.383568...\n",
       "                              ...                        \n",
       "3035    [0.039870866, 0.3191851, 0.14636753, 0.4339915...\n",
       "3604    [-0.20343174, 0.56542456, -0.06081393, 0.50534...\n",
       "3425    [-0.16546537, 0.32568675, 0.16236827, 0.265110...\n",
       "3717    [-0.04070326, 0.59277475, -0.0049013845, 0.101...\n",
       "3266    [-0.096700296, 0.23148116, -0.015666625, 0.432...\n",
       "Name: sentences, Length: 6806, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform sentences into vectors\n",
    "x_train_100 = x_train.apply(sentence_to_embedding_100)\n",
    "x_val_100 = x_val.apply(sentence_to_embedding_100)\n",
    "x_test_100 = x_test.apply(sentence_to_embedding_100)\n",
    "x_train_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classification model\n",
    "model = Sequential()\n",
    "model.add(Dense(100,  activation='relu', name=\"Input_layer\"))\n",
    "model.add(Dense(50, activation='relu', name=\"Hidden_layer_1\"))\n",
    "model.add(Dense(50, activation='relu', name=\"Hidden_layer_2\"))\n",
    "model.add(Dense(4, activation='softmax', name=\"Output_layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 1.3134 - acc: 0.4331 - val_loss: 1.3090 - val_acc: 0.4230\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2965 - acc: 0.4330 - val_loss: 1.3015 - val_acc: 0.4230\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2852 - acc: 0.4330 - val_loss: 1.2925 - val_acc: 0.4230\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2727 - acc: 0.4330 - val_loss: 1.2791 - val_acc: 0.4236\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2578 - acc: 0.4333 - val_loss: 1.2651 - val_acc: 0.4289\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2402 - acc: 0.4427 - val_loss: 1.2491 - val_acc: 0.4336\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2185 - acc: 0.4603 - val_loss: 1.2374 - val_acc: 0.4377\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1962 - acc: 0.4752 - val_loss: 1.2137 - val_acc: 0.4677\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1745 - acc: 0.4860 - val_loss: 1.1933 - val_acc: 0.4835\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1539 - acc: 0.5046 - val_loss: 1.1810 - val_acc: 0.4853\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1347 - acc: 0.5129 - val_loss: 1.1862 - val_acc: 0.4818\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1169 - acc: 0.5239 - val_loss: 1.1696 - val_acc: 0.5012\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1014 - acc: 0.5373 - val_loss: 1.1515 - val_acc: 0.5176\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0869 - acc: 0.5442 - val_loss: 1.1344 - val_acc: 0.5159\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0749 - acc: 0.5516 - val_loss: 1.1097 - val_acc: 0.5400\n",
      "Epoch 16/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0605 - acc: 0.5567 - val_loss: 1.0985 - val_acc: 0.5364\n",
      "Epoch 17/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0534 - acc: 0.5627 - val_loss: 1.0928 - val_acc: 0.5429\n",
      "Epoch 18/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0452 - acc: 0.5623 - val_loss: 1.1120 - val_acc: 0.5264\n",
      "Epoch 19/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0391 - acc: 0.5676 - val_loss: 1.0839 - val_acc: 0.5405\n",
      "Epoch 20/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0316 - acc: 0.5671 - val_loss: 1.0892 - val_acc: 0.5476\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(x=np.asarray(x_train_100.to_list()).astype('float32'),\n",
    "                    y=y_train_encoded,\n",
    "                    batch_size=32, epochs=20,\n",
    "                    validation_data=(np.asarray(x_val_100.to_list()).astype('float32'), y_val_encoded),\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (Dense)         (None, 100)               10100     \n",
      "                                                                 \n",
      " Hidden_layer_1 (Dense)      (None, 50)                5050      \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 50)                2550      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,904\n",
      "Trainable params: 17,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 0s 701us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return predictions - the chosen class is the one with the highest probability\n",
    "y_pred_100 = model.predict(np.asarray(x_test_100.to_list()).astype('float32'))\n",
    "y_pred_100 = to_categorical(np.argmax(y_pred_100, axis=1), 4)\n",
    "y_pred_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      " Bart Simpson       0.44      0.42      0.43       452\n",
      "Homer Simpson       0.60      0.71      0.65       881\n",
      " Lisa Simpson       0.44      0.43      0.44       366\n",
      "Marge Simpson       0.54      0.38      0.44       428\n",
      "\n",
      "     accuracy                           0.53      2127\n",
      "    macro avg       0.51      0.48      0.49      2127\n",
      " weighted avg       0.53      0.53      0.52      2127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, encoder.inverse_transform(y_pred_100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classification model\n",
    "model = Sequential()\n",
    "model.add(Dense(100,  activation='relu', name=\"Input_layer\"))\n",
    "model.add(Dense(100, activation='relu', name=\"Hidden_layer_1\"))\n",
    "model.add(Dense(100, activation='relu', name=\"Hidden_layer_2\"))\n",
    "model.add(Dense(50, activation='relu', name=\"Hidden_layer_3\"))\n",
    "model.add(Dense(4, activation='softmax', name=\"Output_layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 1.3112 - acc: 0.4210 - val_loss: 1.3088 - val_acc: 0.4230\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2942 - acc: 0.4331 - val_loss: 1.3009 - val_acc: 0.4230\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2831 - acc: 0.4331 - val_loss: 1.2917 - val_acc: 0.4230\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2675 - acc: 0.4343 - val_loss: 1.2762 - val_acc: 0.4313\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2454 - acc: 0.4471 - val_loss: 1.2533 - val_acc: 0.4495\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2182 - acc: 0.4664 - val_loss: 1.2360 - val_acc: 0.4659\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1961 - acc: 0.4797 - val_loss: 1.2168 - val_acc: 0.4718\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1777 - acc: 0.4846 - val_loss: 1.2010 - val_acc: 0.4736\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1593 - acc: 0.4938 - val_loss: 1.1960 - val_acc: 0.4777\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1448 - acc: 0.5053 - val_loss: 1.1714 - val_acc: 0.4935\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1265 - acc: 0.5166 - val_loss: 1.1596 - val_acc: 0.5123\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1089 - acc: 0.5320 - val_loss: 1.1553 - val_acc: 0.5012\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0899 - acc: 0.5469 - val_loss: 1.1762 - val_acc: 0.4947\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0772 - acc: 0.5479 - val_loss: 1.1154 - val_acc: 0.5276\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0615 - acc: 0.5617 - val_loss: 1.1028 - val_acc: 0.5405\n",
      "Epoch 16/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0521 - acc: 0.5607 - val_loss: 1.1103 - val_acc: 0.5217\n",
      "Epoch 17/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0439 - acc: 0.5683 - val_loss: 1.0937 - val_acc: 0.5441\n",
      "Epoch 18/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0351 - acc: 0.5727 - val_loss: 1.0720 - val_acc: 0.5458\n",
      "Epoch 19/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0292 - acc: 0.5676 - val_loss: 1.0712 - val_acc: 0.5499\n",
      "Epoch 20/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0251 - acc: 0.5741 - val_loss: 1.1042 - val_acc: 0.5423\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(x=np.asarray(x_train_100.to_list()).astype('float32'),\n",
    "                    y=y_train_encoded,\n",
    "                    batch_size=32, epochs=20,\n",
    "                    validation_data=(np.asarray(x_val_100.to_list()).astype('float32'), y_val_encoded),\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (Dense)         (None, 100)               10100     \n",
      "                                                                 \n",
      " Hidden_layer_1 (Dense)      (None, 100)               10100     \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 100)               10100     \n",
      "                                                                 \n",
      " Hidden_layer_3 (Dense)      (None, 50)                5050      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,554\n",
      "Trainable params: 35,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 0s 732us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return predictions - the chosen class is the one with the highest probability\n",
    "y_pred_100 = model.predict(np.asarray(x_test_100.to_list()).astype('float32'))\n",
    "y_pred_100 = to_categorical(np.argmax(y_pred_100, axis=1), 4)\n",
    "y_pred_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      " Bart Simpson       0.56      0.16      0.25       452\n",
      "Homer Simpson       0.58      0.76      0.65       881\n",
      " Lisa Simpson       0.53      0.31      0.39       366\n",
      "Marge Simpson       0.43      0.63      0.51       428\n",
      "\n",
      "     accuracy                           0.53      2127\n",
      "    macro avg       0.53      0.47      0.45      2127\n",
      " weighted avg       0.54      0.53      0.50      2127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, encoder.inverse_transform(y_pred_100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classification model\n",
    "model = Sequential()\n",
    "model.add(Dense(100,  activation='relu', name=\"Input_layer\"))\n",
    "model.add(Dense(80, activation='relu', name=\"Hidden_layer_1\"))\n",
    "model.add(Dense(80, activation='relu', name=\"Hidden_layer_2\"))\n",
    "model.add(Dense(50, activation='relu', name=\"Hidden_layer_3\"))\n",
    "model.add(Dense(4, activation='softmax', name=\"Output_layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 1.3212 - acc: 0.4146 - val_loss: 1.3097 - val_acc: 0.4230\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2957 - acc: 0.4331 - val_loss: 1.2959 - val_acc: 0.4230\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2821 - acc: 0.4331 - val_loss: 1.2816 - val_acc: 0.4230\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2641 - acc: 0.4339 - val_loss: 1.2620 - val_acc: 0.4295\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2421 - acc: 0.4470 - val_loss: 1.2407 - val_acc: 0.4459\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2200 - acc: 0.4656 - val_loss: 1.2225 - val_acc: 0.4683\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1987 - acc: 0.4790 - val_loss: 1.2057 - val_acc: 0.4689\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1807 - acc: 0.4907 - val_loss: 1.2199 - val_acc: 0.4559\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1665 - acc: 0.4965 - val_loss: 1.1769 - val_acc: 0.5059\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1499 - acc: 0.5057 - val_loss: 1.1814 - val_acc: 0.4982\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1360 - acc: 0.5237 - val_loss: 1.1546 - val_acc: 0.5165\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1163 - acc: 0.5341 - val_loss: 1.1438 - val_acc: 0.5194\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0991 - acc: 0.5391 - val_loss: 1.1348 - val_acc: 0.5194\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0829 - acc: 0.5470 - val_loss: 1.1391 - val_acc: 0.5018\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0675 - acc: 0.5526 - val_loss: 1.0971 - val_acc: 0.5417\n",
      "Epoch 16/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0527 - acc: 0.5539 - val_loss: 1.1591 - val_acc: 0.4947\n",
      "Epoch 17/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0455 - acc: 0.5597 - val_loss: 1.1039 - val_acc: 0.5376\n",
      "Epoch 18/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0383 - acc: 0.5644 - val_loss: 1.1034 - val_acc: 0.5229\n",
      "Epoch 19/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0310 - acc: 0.5710 - val_loss: 1.1094 - val_acc: 0.5282\n",
      "Epoch 20/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0214 - acc: 0.5736 - val_loss: 1.0738 - val_acc: 0.5517\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(x=np.asarray(x_train_100.to_list()).astype('float32'),\n",
    "                    y=y_train_encoded,\n",
    "                    batch_size=32, epochs=20,\n",
    "                    validation_data=(np.asarray(x_val_100.to_list()).astype('float32'), y_val_encoded),\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (Dense)         (None, 100)               10100     \n",
      "                                                                 \n",
      " Hidden_layer_1 (Dense)      (None, 80)                8080      \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 80)                6480      \n",
      "                                                                 \n",
      " Hidden_layer_3 (Dense)      (None, 50)                4050      \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,914\n",
      "Trainable params: 28,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 0s 709us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return predictions - the chosen class is the one with the highest probability\n",
    "y_pred_100 = model.predict(np.asarray(x_test_100.to_list()).astype('float32'))\n",
    "y_pred_100 = to_categorical(np.argmax(y_pred_100, axis=1), 4)\n",
    "y_pred_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      " Bart Simpson       0.48      0.32      0.38       452\n",
      "Homer Simpson       0.61      0.67      0.64       881\n",
      " Lisa Simpson       0.52      0.41      0.46       366\n",
      "Marge Simpson       0.46      0.60      0.52       428\n",
      "\n",
      "     accuracy                           0.54      2127\n",
      "    macro avg       0.52      0.50      0.50      2127\n",
      " weighted avg       0.54      0.54      0.53      2127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, encoder.inverse_transform(y_pred_100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size 200 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_embedding_200(sentence: str):\n",
    "    \"\"\"\n",
    "    Returns the element-wise mean of the embeddings that represent each word in a sentence\n",
    "    \"\"\"\n",
    "    token_list = preprocessing(sentence)\n",
    "    return np.mean(model_200.wv[token_list], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1133    [0.16244797, -0.007762286, -0.5801328, 0.44329...\n",
       "877     [-0.3506315, 0.055515993, -0.6027178, 0.436567...\n",
       "3829    [-0.122201025, 0.056866664, -0.8165569, 0.4008...\n",
       "1182    [-0.03245688, -0.012517424, -0.5956196, 0.2783...\n",
       "1518    [-0.060584597, -0.017061805, -0.63101166, 0.26...\n",
       "                              ...                        \n",
       "3035    [0.008413417, 0.15665813, -0.68438077, 0.27424...\n",
       "3604    [-0.05165259, 0.30557153, -0.6113081, 0.275105...\n",
       "3425    [0.01491911, -0.03480768, -0.5763194, 0.260999...\n",
       "3717    [-0.06740085, -0.1158896, -0.5450491, 0.224405...\n",
       "3266    [-0.030567076, 0.1971119, -0.54006016, 0.42608...\n",
       "Name: sentences, Length: 6806, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform sentences into vectors\n",
    "x_train_200 = x_train.apply(sentence_to_embedding_200)\n",
    "x_val_200 = x_val.apply(sentence_to_embedding_200)\n",
    "x_test_200 = x_test.apply(sentence_to_embedding_200)\n",
    "x_train_200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classification model\n",
    "model = Sequential()\n",
    "model.add(Dense(200,  activation='relu', name=\"Input_layer\"))\n",
    "model.add(Dense(100, activation='relu', name=\"Hidden_layer_1\"))\n",
    "model.add(Dense(100, activation='relu', name=\"Hidden_layer_2\"))\n",
    "model.add(Dense(4, activation='softmax', name=\"Output_layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 1.3075 - acc: 0.4311 - val_loss: 1.2996 - val_acc: 0.4230\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2839 - acc: 0.4331 - val_loss: 1.2868 - val_acc: 0.4230\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2666 - acc: 0.4331 - val_loss: 1.2708 - val_acc: 0.4230\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2453 - acc: 0.4340 - val_loss: 1.2502 - val_acc: 0.4360\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.2201 - acc: 0.4481 - val_loss: 1.2306 - val_acc: 0.4571\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1926 - acc: 0.4690 - val_loss: 1.2092 - val_acc: 0.4865\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.1635 - acc: 0.4925 - val_loss: 1.1791 - val_acc: 0.4935\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.1335 - acc: 0.5169 - val_loss: 1.1562 - val_acc: 0.5147\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - 1s 3ms/step - loss: 1.1067 - acc: 0.5354 - val_loss: 1.1341 - val_acc: 0.5200\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.0819 - acc: 0.5482 - val_loss: 1.1243 - val_acc: 0.5170\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0659 - acc: 0.5586 - val_loss: 1.1255 - val_acc: 0.5188\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0442 - acc: 0.5673 - val_loss: 1.0892 - val_acc: 0.5364\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0330 - acc: 0.5704 - val_loss: 1.0717 - val_acc: 0.5447\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0187 - acc: 0.5745 - val_loss: 1.0619 - val_acc: 0.5599\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.0079 - acc: 0.5820 - val_loss: 1.0708 - val_acc: 0.5558\n",
      "Epoch 16/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 1.0009 - acc: 0.5861 - val_loss: 1.1094 - val_acc: 0.5118\n",
      "Epoch 17/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 0.9898 - acc: 0.5901 - val_loss: 1.0471 - val_acc: 0.5640\n",
      "Epoch 18/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.9885 - acc: 0.5937 - val_loss: 1.0381 - val_acc: 0.5629\n",
      "Epoch 19/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 0.9787 - acc: 0.5927 - val_loss: 1.0431 - val_acc: 0.5617\n",
      "Epoch 20/20\n",
      "213/213 [==============================] - 0s 1ms/step - loss: 0.9719 - acc: 0.5962 - val_loss: 1.0485 - val_acc: 0.5687\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(x=np.asarray(x_train_200.to_list()).astype('float32'),\n",
    "                    y=y_train_encoded,\n",
    "                    batch_size=32, epochs=20,\n",
    "                    validation_data=(np.asarray(x_val_200.to_list()).astype('float32'), y_val_encoded),\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (Dense)         (None, 200)               40200     \n",
      "                                                                 \n",
      " Hidden_layer_1 (Dense)      (None, 100)               20100     \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 100)               10100     \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 70,804\n",
      "Trainable params: 70,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 0s 905us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return predictions - the chosen class is the one with the highest probability\n",
    "y_pred_200 = model.predict(np.asarray(x_test_200.to_list()).astype('float32'))\n",
    "y_pred_200 = to_categorical(np.argmax(y_pred_200, axis=1), 4)\n",
    "y_pred_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      " Bart Simpson       0.59      0.26      0.36       452\n",
      "Homer Simpson       0.56      0.85      0.68       881\n",
      " Lisa Simpson       0.64      0.30      0.41       366\n",
      "Marge Simpson       0.53      0.53      0.53       428\n",
      "\n",
      "     accuracy                           0.57      2127\n",
      "    macro avg       0.58      0.48      0.49      2127\n",
      " weighted avg       0.58      0.57      0.53      2127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, encoder.inverse_transform(y_pred_200)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classification model\n",
    "model = Sequential()\n",
    "model.add(Dense(200,  activation='relu', name=\"Input_layer\"))\n",
    "model.add(Dense(200, activation='relu', name=\"Hidden_layer_1\"))\n",
    "model.add(Dense(200, activation='relu', name=\"Hidden_layer_2\"))\n",
    "model.add(Dense(100, activation='relu', name=\"Hidden_layer_3\"))\n",
    "model.add(Dense(4, activation='softmax', name=\"Output_layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 1.3128 - acc: 0.4286 - val_loss: 1.3088 - val_acc: 0.4230\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.2917 - acc: 0.4331 - val_loss: 1.2983 - val_acc: 0.4230\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.2779 - acc: 0.4331 - val_loss: 1.2839 - val_acc: 0.4230\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.2579 - acc: 0.4331 - val_loss: 1.2644 - val_acc: 0.4242\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.2302 - acc: 0.4455 - val_loss: 1.2396 - val_acc: 0.4365\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.1981 - acc: 0.4733 - val_loss: 1.2103 - val_acc: 0.4747\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.1653 - acc: 0.5025 - val_loss: 1.1831 - val_acc: 0.4976\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.1333 - acc: 0.5207 - val_loss: 1.1634 - val_acc: 0.5123\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.1088 - acc: 0.5313 - val_loss: 1.1245 - val_acc: 0.5347\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.0839 - acc: 0.5400 - val_loss: 1.1135 - val_acc: 0.5341\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.0624 - acc: 0.5507 - val_loss: 1.1048 - val_acc: 0.5429\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.0460 - acc: 0.5635 - val_loss: 1.1039 - val_acc: 0.5411\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.0356 - acc: 0.5651 - val_loss: 1.1752 - val_acc: 0.5176\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.0200 - acc: 0.5698 - val_loss: 1.0792 - val_acc: 0.5411\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.0059 - acc: 0.5851 - val_loss: 1.0733 - val_acc: 0.5429\n",
      "Epoch 16/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.9947 - acc: 0.5868 - val_loss: 1.0743 - val_acc: 0.5423\n",
      "Epoch 17/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.9895 - acc: 0.5912 - val_loss: 1.0437 - val_acc: 0.5711\n",
      "Epoch 18/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.9834 - acc: 0.5867 - val_loss: 1.0348 - val_acc: 0.5705\n",
      "Epoch 19/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.9791 - acc: 0.5926 - val_loss: 1.0433 - val_acc: 0.5664\n",
      "Epoch 20/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.9690 - acc: 0.6011 - val_loss: 1.0575 - val_acc: 0.5629\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(x=np.asarray(x_train_200.to_list()).astype('float32'),\n",
    "                    y=y_train_encoded,\n",
    "                    batch_size=32, epochs=20,\n",
    "                    validation_data=(np.asarray(x_val_200.to_list()).astype('float32'), y_val_encoded),\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (Dense)         (None, 200)               40200     \n",
      "                                                                 \n",
      " Hidden_layer_1 (Dense)      (None, 200)               40200     \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 200)               40200     \n",
      "                                                                 \n",
      " Hidden_layer_3 (Dense)      (None, 100)               20100     \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 141,104\n",
      "Trainable params: 141,104\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 0s 940us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return predictions - the chosen class is the one with the highest probability\n",
    "y_pred_200 = model.predict(np.asarray(x_test_200.to_list()).astype('float32'))\n",
    "y_pred_200 = to_categorical(np.argmax(y_pred_200, axis=1), 4)\n",
    "y_pred_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      " Bart Simpson       0.60      0.29      0.39       452\n",
      "Homer Simpson       0.56      0.86      0.68       881\n",
      " Lisa Simpson       0.68      0.26      0.37       366\n",
      "Marge Simpson       0.53      0.53      0.53       428\n",
      "\n",
      "     accuracy                           0.57      2127\n",
      "    macro avg       0.59      0.48      0.49      2127\n",
      " weighted avg       0.58      0.57      0.53      2127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, encoder.inverse_transform(y_pred_200)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classification model\n",
    "model = Sequential()\n",
    "model.add(Dense(200,  activation='relu', name=\"Input_layer\"))\n",
    "model.add(Dense(160, activation='relu', name=\"Hidden_layer_1\"))\n",
    "model.add(Dense(160, activation='relu', name=\"Hidden_layer_2\"))\n",
    "model.add(Dense(100, activation='relu', name=\"Hidden_layer_3\"))\n",
    "model.add(Dense(4, activation='softmax', name=\"Output_layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - 1s 2ms/step - loss: 1.3118 - acc: 0.4227 - val_loss: 1.3030 - val_acc: 0.4230\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.2854 - acc: 0.4331 - val_loss: 1.2880 - val_acc: 0.4230\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.2660 - acc: 0.4342 - val_loss: 1.2681 - val_acc: 0.4248\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.2402 - acc: 0.4456 - val_loss: 1.2457 - val_acc: 0.4583\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.2109 - acc: 0.4675 - val_loss: 1.2209 - val_acc: 0.4659\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.1857 - acc: 0.4809 - val_loss: 1.2031 - val_acc: 0.4753\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.1605 - acc: 0.4956 - val_loss: 1.2390 - val_acc: 0.4307\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.1359 - acc: 0.5093 - val_loss: 1.1680 - val_acc: 0.4935\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.1107 - acc: 0.5292 - val_loss: 1.1291 - val_acc: 0.5382\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.0858 - acc: 0.5442 - val_loss: 1.1156 - val_acc: 0.5347\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.0611 - acc: 0.5591 - val_loss: 1.1068 - val_acc: 0.5376\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.0377 - acc: 0.5683 - val_loss: 1.1187 - val_acc: 0.5405\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.0258 - acc: 0.5758 - val_loss: 1.0548 - val_acc: 0.5629\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 1.0065 - acc: 0.5837 - val_loss: 1.0786 - val_acc: 0.5341\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.9963 - acc: 0.5868 - val_loss: 1.0316 - val_acc: 0.5687\n",
      "Epoch 16/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.9876 - acc: 0.5882 - val_loss: 1.0222 - val_acc: 0.5758\n",
      "Epoch 17/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.9828 - acc: 0.5901 - val_loss: 1.0847 - val_acc: 0.5505\n",
      "Epoch 18/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.9852 - acc: 0.5961 - val_loss: 1.0997 - val_acc: 0.5552\n",
      "Epoch 19/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.9653 - acc: 0.6014 - val_loss: 1.0078 - val_acc: 0.5852\n",
      "Epoch 20/20\n",
      "213/213 [==============================] - 0s 2ms/step - loss: 0.9575 - acc: 0.6117 - val_loss: 1.0572 - val_acc: 0.5682\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(x=np.asarray(x_train_200.to_list()).astype('float32'),\n",
    "                    y=y_train_encoded,\n",
    "                    batch_size=32, epochs=20,\n",
    "                    validation_data=(np.asarray(x_val_200.to_list()).astype('float32'), y_val_encoded),\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (Dense)         (None, 200)               40200     \n",
      "                                                                 \n",
      " Hidden_layer_1 (Dense)      (None, 160)               32160     \n",
      "                                                                 \n",
      " Hidden_layer_2 (Dense)      (None, 160)               25760     \n",
      "                                                                 \n",
      " Hidden_layer_3 (Dense)      (None, 100)               16100     \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 114,624\n",
      "Trainable params: 114,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 0s 973us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return predictions - the chosen class is the one with the highest probability\n",
    "y_pred_200 = model.predict(np.asarray(x_test_200.to_list()).astype('float32'))\n",
    "y_pred_200 = to_categorical(np.argmax(y_pred_200, axis=1), 4)\n",
    "y_pred_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      " Bart Simpson       0.58      0.28      0.38       452\n",
      "Homer Simpson       0.54      0.90      0.68       881\n",
      " Lisa Simpson       0.65      0.23      0.34       366\n",
      "Marge Simpson       0.61      0.44      0.51       428\n",
      "\n",
      "     accuracy                           0.56      2127\n",
      "    macro avg       0.60      0.46      0.48      2127\n",
      " weighted avg       0.58      0.56      0.52      2127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, encoder.inverse_transform(y_pred_200)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
