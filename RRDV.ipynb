{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re, json\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__paths to change__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input variables\n",
    "documents_path = './input/docs-raw-texts/'\n",
    "queries_path = './input/queries-raw-texts/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read documents methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(path: str) -> list:\n",
    "    \"\"\"\n",
    "    read raw text from naf documents located in the directory path\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for file in sorted(os.listdir(path)):\n",
    "        if file.endswith(\".naf\"):\n",
    "            tree = ET.parse(path + file)\n",
    "            text = tree.find('raw').text\n",
    "            header = tree.find('nafHeader')\n",
    "            if header:\n",
    "                desc = header.find('fileDesc')\n",
    "                if desc:\n",
    "                    title = desc.attrib.get('title')\n",
    "                    text = title + ' ' + text if title else text\n",
    "            data.append(text)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(document: str) -> list:\n",
    "    \"\"\"\n",
    "    remove the english stop words from data\n",
    "    \"\"\"\n",
    "    lower = document.lower()\n",
    "    words = lower.split(' ')\n",
    "    stop_words = stopwords.words('english')\n",
    "    return [word for word in words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonlatin(document: str) -> str:\n",
    "    \"\"\"\n",
    "    replace problematic characters\n",
    "    \"\"\"\n",
    "    document = re.sub('\\n', ' ', document)\n",
    "    document = re.sub('[^a-zA-Z]|[0-9]', ' ', document)\n",
    "    document = re.sub('\\s+', ' ', document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document: str) -> list:\n",
    "    \"\"\"\n",
    "    clean data by removing non-latin characters\n",
    "    stem data sentences\n",
    "    remove stop words from a document\n",
    "    \"\"\"\n",
    "    porter = PorterStemmer()\n",
    "    document = remove_nonlatin(document)\n",
    "    document = porter.stem_sentence(document)\n",
    "    document = remove_stopwords(document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### indexes and doc-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_index(documents: pd.Series) -> pd.Index:\n",
    "    \"\"\"\n",
    "    return a sorted index of every word in the texts\n",
    "    \"\"\"\n",
    "    # get all words in all documents\n",
    "    words = set()\n",
    "    for document in documents:\n",
    "        words.update(set(document))\n",
    "    # sort the words\n",
    "    sorted_words = sorted(list(words))\n",
    "    # get index of sorted words\n",
    "    words_frame = pd.DataFrame(sorted_words, columns=['data'])\n",
    "    words_index = words_frame.set_index('data').index\n",
    "    return words_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_word(word: str, words_index: pd.Index) -> int:\n",
    "    \"\"\"\n",
    "    return the provided word index\n",
    "    \"\"\"\n",
    "    try: return words_index.get_loc(word)\n",
    "    except: return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_term(documents: pd.DataFrame, words_index: pd.Index) -> list:\n",
    "    \"\"\"\n",
    "    return the document term matrix that indicate how many terms repeats in each document\n",
    "    \"\"\"\n",
    "    doc_term = [[0]*len(documents) for _ in range(len(words_index))]\n",
    "    for doc_index, document in documents.iterrows():\n",
    "        for word in document.filtered:\n",
    "            word_index = get_index_word(word, words_index)\n",
    "            if word_index != -1:\n",
    "                doc_term[word_index][doc_index] += 1\n",
    "    return doc_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representación vectorial ponderada tf.idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf(doc_term: list) -> list:\n",
    "    \"\"\"\n",
    "    return the ft score from each word in all the documents\n",
    "    \"\"\"\n",
    "    return [[1 + np.log10(doc) if doc > 0 else 0 for doc in word] for word in doc_term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf(doc_term: list) -> list:\n",
    "    \"\"\"\n",
    "    return the idf score from each word in the entire collection\n",
    "    \"\"\"\n",
    "    word_num = len(doc_term)\n",
    "    return [np.log10(word_num/sum([1 if doc > 0 else 0 for doc in word])) for word in doc_term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(doc_term: list) -> list:\n",
    "    \"\"\"\n",
    "    ponderate the tf-idf scores multiping them\n",
    "    \"\"\"\n",
    "    tf = get_tf(doc_term)\n",
    "    idf = get_idf(doc_term)\n",
    "    return [[tf_scr * idf[i] for tf_scr in words] for i, words in enumerate(tf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term = [\n",
    "    [157, 73, 0, 0, 0, 0],\n",
    "    [4, 157, 0, 1, 0, 0],\n",
    "    [232, 227, 0, 2, 1, 1],\n",
    "    [0, 1, 0, 0, 0, 0, 0],\n",
    "    [57, 0, 0, 0, 0, 0],\n",
    "    [2, 0, 3, 5, 5, 1],\n",
    "    [2, 0, 1, 1, 1, 0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.7387868738260175, 1.5578424688491743, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.5895208854579495, 1.1760168802176616, 0.0, 0.36797678529459443, 0.0, 0.0],\n",
       " [0.49179214833081875,\n",
       "  0.4904094661970059,\n",
       "  0.0,\n",
       "  0.1901169576248441,\n",
       "  0.146128035678238,\n",
       "  0.146128035678238],\n",
       " [0.0, 0.8450980400142568, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [2.3289844390533956, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.1901169576248441,\n",
       "  0.0,\n",
       "  0.21584882741075853,\n",
       "  0.24826714940986994,\n",
       "  0.24826714940986994,\n",
       "  0.146128035678238],\n",
       " [0.3161997914285121,\n",
       "  0.0,\n",
       "  0.24303804868629444,\n",
       "  0.24303804868629444,\n",
       "  0.24303804868629444,\n",
       "  0.0]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = get_tfidf(doc_term)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: obtain the documents and convet it to dataframe\n",
    "data = get_documents(documents_path)\n",
    "documents = pd.DataFrame.from_dict(data)\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: apply the preprocessing method\n",
    "documents['filtered'] = documents.data.apply(preprocessing)\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: get word-index, doc-term, and the tfidf index\n",
    "words_index = get_words_index(documents.filtered)\n",
    "doc_term = get_doc_term(documents, words_index)\n",
    "tfidf = get_tfidf(doc_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similitud del coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similitud_coseno(vector1,vector2):\n",
    "    producto_punto = np.dot(vector1,vector2)\n",
    "    norma_1 = np.linalg.norm(vector1)\n",
    "    norma_2 = np.linalg.norm(vector2)\n",
    "    return producto_punto/(norma_1*norma_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(path):\n",
    "    documents = []\n",
    "    for filename in sorted(os.listdir(path)):\n",
    "        if filename.endswith(\".naf\"):\n",
    "            tree = ET.parse(path+filename)\n",
    "            texto = tree.find('raw').text\n",
    "            documents.append(texto)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(data):\n",
    "    filtered_words = [word.lower() for word in data.split(' ') if word.lower() not in stopwords.words('english')]    \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesar(documentos):\n",
    "# 2. Preprocess the data\n",
    "#remover espacios dobles y triples\n",
    "    import re\n",
    "    documentos = re.sub('\\n', ' ',documentos)\n",
    "    documentos = re.sub('[^a-zA-Z]|[0-9]', ' ',documentos)\n",
    "    documentos = re.sub('\\s+', ' ',documentos)\n",
    "    p=PorterStemmer()\n",
    "    documentos = p.stem_sentence(documentos)\n",
    "\n",
    "    filtrada= remove_stopwords(documentos)\n",
    "    \n",
    "    return filtrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Documento</th>\n",
       "      <th>filtrada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>William Beaumont and the Human Digestion.\\n\\nW...</td>\n",
       "      <td>[accid, acid, activ, affect, ag, alexi, also, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Selma Lagerlöf and the wonderful Adventures of...</td>\n",
       "      <td>[abl, academi, accept, acclaim, accomplish, ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ferdinand de Lesseps and the Suez Canal.\\n\\nFe...</td>\n",
       "      <td>[abandon, act, adopt, affair, africa, afterwar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Walt Disney’s ‘Steamboat Willie’ and the Rise ...</td>\n",
       "      <td>[aboard, accident, accompani, ad, along, also,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eugene Wigner and the Structure of the Atomic ...</td>\n",
       "      <td>[accept, achiev, ad, administr, albert, along,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Documento  \\\n",
       "0  William Beaumont and the Human Digestion.\\n\\nW...   \n",
       "1  Selma Lagerlöf and the wonderful Adventures of...   \n",
       "2  Ferdinand de Lesseps and the Suez Canal.\\n\\nFe...   \n",
       "3  Walt Disney’s ‘Steamboat Willie’ and the Rise ...   \n",
       "4  Eugene Wigner and the Structure of the Atomic ...   \n",
       "\n",
       "                                            filtrada  \n",
       "0  [accid, acid, activ, affect, ag, alexi, also, ...  \n",
       "1  [abl, academi, accept, acclaim, accomplish, ac...  \n",
       "2  [abandon, act, adopt, affair, africa, afterwar...  \n",
       "3  [aboard, accident, accompani, ad, along, also,...  \n",
       "4  [accept, achiev, ad, administr, albert, along,...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos = get_documents('datos/docs-raw-texts/')\n",
    "documentos = pd.DataFrame(datos,columns=['Documento'])\n",
    "documentos['filtrada']=documentos['Documento'].apply(preprocesar)\n",
    "doc_proc= documentos\n",
    "doc_proc.filtrada = doc_proc.filtrada.apply(np.unique)\n",
    "doc_proc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accid': 0,\n",
       " 'acid': 1,\n",
       " 'activ': 2,\n",
       " 'affect': 3,\n",
       " 'ag': 4,\n",
       " 'alexi': 5,\n",
       " 'also': 6,\n",
       " 'american': 7,\n",
       " 'anoth': 8,\n",
       " 'armi': 9,\n",
       " 'around': 10,\n",
       " 'back': 11,\n",
       " 'basi': 12,\n",
       " 'beaumont': 13,\n",
       " 'becam': 14,\n",
       " 'becaus': 15,\n",
       " 'best': 16,\n",
       " 'better': 17,\n",
       " 'bit': 18,\n",
       " 'book': 19,\n",
       " 'born': 20,\n",
       " 'break': 21,\n",
       " 'broken': 22,\n",
       " 'canadian': 23,\n",
       " 'caus': 24,\n",
       " 'chemic': 25,\n",
       " 'children': 26,\n",
       " 'close': 27,\n",
       " 'compani': 28,\n",
       " 'complet': 29,\n",
       " 'connecticut': 30,\n",
       " 'consid': 31,\n",
       " 'di': 32,\n",
       " 'differ': 33,\n",
       " 'digest': 34,\n",
       " 'discov': 35,\n",
       " 'dure': 36,\n",
       " 'earli': 37,\n",
       " 'eight': 38,\n",
       " 'emot': 39,\n",
       " 'even': 40,\n",
       " 'examin': 41,\n",
       " 'exist': 42,\n",
       " 'expect': 43,\n",
       " 'experi': 44,\n",
       " 'famou': 45,\n",
       " 'father': 46,\n",
       " 'find': 47,\n",
       " 'fistula': 48,\n",
       " 'follow': 49,\n",
       " 'food': 50,\n",
       " 'fort': 51,\n",
       " 'french': 52,\n",
       " 'fur': 53,\n",
       " 'fuse': 54,\n",
       " 'gain': 55,\n",
       " 'gastric': 56,\n",
       " 'gave': 57,\n",
       " 'heal': 58,\n",
       " 'hi': 59,\n",
       " 'hole': 60,\n",
       " 'human': 61,\n",
       " 'hydrochlor': 62,\n",
       " 'imag': 63,\n",
       " 'import': 64,\n",
       " 'inform': 65,\n",
       " 'insert': 66,\n",
       " 'insight': 67,\n",
       " 'interest': 68,\n",
       " 'island': 69,\n",
       " 'juic': 70,\n",
       " 'june': 71,\n",
       " 'knowledg': 72,\n",
       " 'known': 73,\n",
       " 'leav': 74,\n",
       " 'lebanon': 75,\n",
       " 'lectur': 76,\n",
       " 'mackinac': 77,\n",
       " 'mai': 78,\n",
       " 'main': 79,\n",
       " 'man': 80,\n",
       " 'martin': 81,\n",
       " 'mate': 82,\n",
       " 'mechan': 83,\n",
       " 'mere': 84,\n",
       " 'michigan': 85,\n",
       " 'movement': 86,\n",
       " 'much': 87,\n",
       " 'muscl': 88,\n",
       " 'must': 89,\n",
       " 'name': 90,\n",
       " 'natur': 91,\n",
       " 'never': 92,\n",
       " 'new': 93,\n",
       " 'nobodi': 94,\n",
       " 'notic': 95,\n",
       " 'novemb': 96,\n",
       " 'numer': 97,\n",
       " 'observ': 98,\n",
       " 'old': 99,\n",
       " 'open': 100,\n",
       " 'order': 101,\n",
       " 'outliv': 102,\n",
       " 'patient': 103,\n",
       " 'pepsin': 104,\n",
       " 'perfor': 105,\n",
       " 'perform': 106,\n",
       " 'period': 107,\n",
       " 'perman': 108,\n",
       " 'physic': 109,\n",
       " 'physician': 110,\n",
       " 'physiolog': 111,\n",
       " 'plattsburgh': 112,\n",
       " 'potenti': 113,\n",
       " 'practic': 114,\n",
       " 'privat': 115,\n",
       " 'process': 116,\n",
       " 'protect': 117,\n",
       " 'publish': 118,\n",
       " 'pull': 119,\n",
       " 'quickli': 120,\n",
       " 'quit': 121,\n",
       " 'rang': 122,\n",
       " 'realli': 123,\n",
       " 'refug': 124,\n",
       " 'rejoin': 125,\n",
       " 'remov': 126,\n",
       " 'research': 127,\n",
       " 'respons': 128,\n",
       " 'rib': 129,\n",
       " 'scientist': 130,\n",
       " 'seriou': 131,\n",
       " 'serv': 132,\n",
       " 'sever': 133,\n",
       " 'shotgun': 134,\n",
       " 'skin': 135,\n",
       " 'sourc': 136,\n",
       " 'st': 137,\n",
       " 'station': 138,\n",
       " 'stomach': 139,\n",
       " 'store': 140,\n",
       " 'string': 141,\n",
       " 'suggest': 142,\n",
       " 'surgeon': 143,\n",
       " 'surviv': 144,\n",
       " 'system': 145,\n",
       " 'temperatur': 146,\n",
       " 'th': 147,\n",
       " 'though': 148,\n",
       " 'ti': 149,\n",
       " 'trader': 150,\n",
       " 'uncomfort': 151,\n",
       " 'understand': 152,\n",
       " 'us': 153,\n",
       " 'video': 154,\n",
       " 'wa': 155,\n",
       " 'war': 156,\n",
       " 'went': 157,\n",
       " 'william': 158,\n",
       " 'work': 159,\n",
       " 'would': 160,\n",
       " 'wound': 161,\n",
       " 'year': 162,\n",
       " 'york': 163,\n",
       " 'young': 164,\n",
       " 'yovisto': 165,\n",
       " 'abl': 166,\n",
       " 'academi': 167,\n",
       " 'accept': 168,\n",
       " 'acclaim': 169,\n",
       " 'accomplish': 170,\n",
       " 'across': 171,\n",
       " 'adapt': 172,\n",
       " 'adventur': 173,\n",
       " 'anim': 174,\n",
       " 'antichrist': 175,\n",
       " 'anyth': 176,\n",
       " 'ar': 177,\n",
       " 'attempt': 178,\n",
       " 'attend': 179,\n",
       " 'author': 180,\n",
       " 'banknot': 181,\n",
       " 'began': 182,\n",
       " 'belong': 183,\n",
       " 'berl': 184,\n",
       " 'bird': 185,\n",
       " 'boi': 186,\n",
       " 'catch': 187,\n",
       " 'chang': 188,\n",
       " 'church': 189,\n",
       " 'colleg': 190,\n",
       " 'competit': 191,\n",
       " 'comput': 192,\n",
       " 'cover': 193,\n",
       " 'critic': 194,\n",
       " 'danish': 195,\n",
       " 'delight': 196,\n",
       " 'domest': 197,\n",
       " 'elsewher': 198,\n",
       " 'enabl': 199,\n",
       " 'excerpt': 200,\n",
       " 'f': 201,\n",
       " 'famili': 202,\n",
       " 'farm': 203,\n",
       " 'femal': 204,\n",
       " 'film': 205,\n",
       " 'financi': 206,\n",
       " 'first': 207,\n",
       " 'fly': 208,\n",
       " 'focu': 209,\n",
       " 'free': 210,\n",
       " 'g': 211,\n",
       " 'gees': 212,\n",
       " 'genom': 213,\n",
       " 'girl': 214,\n",
       " 'give': 215,\n",
       " 'good': 216,\n",
       " 'goos': 217,\n",
       " 'group': 218,\n",
       " 'hold': 219,\n",
       " 'holgersson': 220,\n",
       " 'howev': 221,\n",
       " 'hurt': 222,\n",
       " 'itali': 223,\n",
       " 'jerusalem': 224,\n",
       " 'join': 225,\n",
       " 'krona': 226,\n",
       " 'lagerl': 227,\n",
       " 'landskrona': 228,\n",
       " 'last': 229,\n",
       " 'laureat': 230,\n",
       " 'learn': 231,\n",
       " 'let': 232,\n",
       " 'literari': 233,\n",
       " 'literatur': 234,\n",
       " 'long': 235,\n",
       " 'meanwhil': 236,\n",
       " 'might': 237,\n",
       " 'miracl': 238,\n",
       " 'moreov': 239,\n",
       " 'neck': 240,\n",
       " 'niel': 241,\n",
       " 'nil': 242,\n",
       " 'nobel': 243,\n",
       " 'pave': 244,\n",
       " 'pictur': 245,\n",
       " 'pixar': 246,\n",
       " 'plain': 247,\n",
       " 'pleas': 248,\n",
       " 'poetri': 249,\n",
       " 'popular': 250,\n",
       " 'print': 251,\n",
       " 'prize': 252,\n",
       " 'produc': 253,\n",
       " 'receiv': 254,\n",
       " 'refus': 255,\n",
       " 'regular': 256,\n",
       " 'resa': 257,\n",
       " 'reveng': 258,\n",
       " 'revers': 259,\n",
       " 'royal': 260,\n",
       " 'saga': 261,\n",
       " 'scania': 262,\n",
       " 'school': 263,\n",
       " 'secondari': 264,\n",
       " 'see': 265,\n",
       " 'seek': 266,\n",
       " 'selma': 267,\n",
       " 'seri': 268,\n",
       " 'shrunken': 269,\n",
       " 'side': 270,\n",
       " 'size': 271,\n",
       " 'sta': 272,\n",
       " 'start': 273,\n",
       " 'stockholm': 274,\n",
       " 'stori': 275,\n",
       " 'success': 276,\n",
       " 'successfulli': 277,\n",
       " 'support': 278,\n",
       " 'sverig': 279,\n",
       " 'sweden': 280,\n",
       " 'swedish': 281,\n",
       " 'take': 282,\n",
       " 'talk': 283,\n",
       " 'task': 284,\n",
       " 'teach': 285,\n",
       " 'teacher': 286,\n",
       " 'thei': 287,\n",
       " 'time': 288,\n",
       " 'tini': 289,\n",
       " 'tomt': 290,\n",
       " 'translat': 291,\n",
       " 'travel': 292,\n",
       " 'turn': 293,\n",
       " 'underbara': 294,\n",
       " 'unnot': 295,\n",
       " 'veri': 296,\n",
       " 'wai': 297,\n",
       " 'well': 298,\n",
       " 'white': 299,\n",
       " 'wide': 300,\n",
       " 'wild': 301,\n",
       " 'win': 302,\n",
       " 'wonder': 303,\n",
       " 'world': 304,\n",
       " 'write': 305,\n",
       " 'writer': 306,\n",
       " 'abandon': 307,\n",
       " 'act': 308,\n",
       " 'adopt': 309,\n",
       " 'affair': 310,\n",
       " 'africa': 311,\n",
       " 'afterward': 312,\n",
       " 'alexandria': 313,\n",
       " 'ali': 314,\n",
       " 'allow': 315,\n",
       " 'ani': 316,\n",
       " 'appoint': 317,\n",
       " 'approv': 318,\n",
       " 'april': 319,\n",
       " 'arbitr': 320,\n",
       " 'architect': 321,\n",
       " 'articl': 322,\n",
       " 'assist': 323,\n",
       " 'associ': 324,\n",
       " 'attitud': 325,\n",
       " 'avoid': 326,\n",
       " 'bankrupt': 327,\n",
       " 'baron': 328,\n",
       " 'barth': 329,\n",
       " 'bbc': 330,\n",
       " 'begin': 331,\n",
       " 'behalf': 332,\n",
       " 'bei': 333,\n",
       " 'benjamin': 334,\n",
       " 'berkelei': 335,\n",
       " 'blow': 336,\n",
       " 'bought': 337,\n",
       " 'bribe': 338,\n",
       " 'britain': 339,\n",
       " 'britannica': 340,\n",
       " 'british': 341,\n",
       " 'canal': 342,\n",
       " 'career': 343,\n",
       " 'celebr': 344,\n",
       " 'centuri': 345,\n",
       " 'charg': 346,\n",
       " 'charl': 347,\n",
       " 'chose': 348,\n",
       " 'circul': 349,\n",
       " 'colonel': 350,\n",
       " 'commiss': 351,\n",
       " 'commissari': 352,\n",
       " 'commun': 353,\n",
       " 'compagni': 354,\n",
       " 'companion': 355,\n",
       " 'congress': 356,\n",
       " 'construct': 357,\n",
       " 'consul': 358,\n",
       " 'consular': 359,\n",
       " 'contemporari': 360,\n",
       " 'continu': 361,\n",
       " 'control': 362,\n",
       " 'court': 363,\n",
       " 'cousin': 364,\n",
       " 'cultur': 365,\n",
       " 'cut': 366,\n",
       " 'de': 367,\n",
       " 'deal': 368,\n",
       " 'decemb': 369,\n",
       " 'decis': 370,\n",
       " 'declar': 371,\n",
       " 'depart': 372,\n",
       " 'despit': 373,\n",
       " 'develop': 374,\n",
       " 'dig': 375,\n",
       " 'diplomat': 376,\n",
       " 'direct': 377,\n",
       " 'directli': 378,\n",
       " 'disagr': 379,\n",
       " 'disput': 380,\n",
       " 'disra': 381,\n",
       " 'distanc': 382,\n",
       " 'doom': 383,\n",
       " 'dr': 384,\n",
       " 'drawn': 385,\n",
       " 'du': 386,\n",
       " 'duti': 387,\n",
       " 'east': 388,\n",
       " 'educ': 389,\n",
       " 'effect': 390,\n",
       " 'egypt': 391,\n",
       " 'egyptian': 392,\n",
       " 'eiffel': 393,\n",
       " 'elderli': 394,\n",
       " 'emperor': 395,\n",
       " 'emploi': 396,\n",
       " 'empress': 397,\n",
       " 'end': 398,\n",
       " 'engin': 399,\n",
       " 'enter': 400,\n",
       " 'epidem': 401,\n",
       " 'eugeni': 402,\n",
       " 'european': 403,\n",
       " 'event': 404,\n",
       " 'expedit': 405,\n",
       " 'explor': 406,\n",
       " 'fail': 407,\n",
       " 'failur': 408,\n",
       " 'fascin': 409,\n",
       " 'februari': 410,\n",
       " 'ferdinand': 411,\n",
       " 'fever': 412,\n",
       " 'final': 413,\n",
       " 'fine': 414,\n",
       " 'fortun': 415,\n",
       " 'found': 416,\n",
       " 'fran': 417,\n",
       " 'friend': 418,\n",
       " 'futil': 419,\n",
       " 'gener': 420,\n",
       " 'get': 421,\n",
       " 'given': 422,\n",
       " 'go': 423,\n",
       " 'govern': 424,\n",
       " 'great': 425,\n",
       " 'growth': 426,\n",
       " 'guilti': 427,\n",
       " 'gustav': 428,\n",
       " 'haussmann': 429,\n",
       " 'heavili': 430,\n",
       " 'held': 431,\n",
       " 'henri': 432,\n",
       " 'histori': 433,\n",
       " 'iii': 434,\n",
       " 'immedi': 435,\n",
       " 'imperi': 436,\n",
       " 'imprison': 437,\n",
       " 'inabl': 438,\n",
       " 'inspir': 439,\n",
       " 'intern': 440,\n",
       " 'ismail': 441,\n",
       " 'iv': 442,\n",
       " 'jail': 443,\n",
       " 'jean': 444,\n",
       " 'journalist': 445,\n",
       " 'journei': 446,\n",
       " 'khediv': 447,\n",
       " 'la': 448,\n",
       " 'land': 449,\n",
       " 'laqueur': 450,\n",
       " 'larg': 451,\n",
       " 'later': 452,\n",
       " 'leader': 453,\n",
       " 'lemi': 454,\n",
       " 'lessep': 455,\n",
       " 'level': 456,\n",
       " 'linant': 457,\n",
       " 'liquid': 458,\n",
       " 'lisbon': 459,\n",
       " 'lock': 460,\n",
       " 'made': 461,\n",
       " 'malaria': 462,\n",
       " 'mari': 463,\n",
       " 'maritim': 464,\n",
       " 'mathieu': 465,\n",
       " 'medic': 466,\n",
       " 'mediterranean': 467,\n",
       " 'mehemet': 468,\n",
       " 'middl': 469,\n",
       " 'minist': 470,\n",
       " 'misjudg': 471,\n",
       " 'mismanag': 472,\n",
       " 'modifi': 473,\n",
       " 'mougel': 474,\n",
       " 'napoleon': 475,\n",
       " 'niemey': 476,\n",
       " 'occupi': 477,\n",
       " 'offic': 478,\n",
       " 'offici': 479,\n",
       " 'oi': 480,\n",
       " 'onli': 481,\n",
       " 'onlin': 482,\n",
       " 'opposit': 483,\n",
       " 'organ': 484,\n",
       " 'oscar': 485,\n",
       " 'overcom': 486,\n",
       " 'ow': 487,\n",
       " 'p': 488,\n",
       " 'paid': 489,\n",
       " 'panama': 490,\n",
       " 'pari': 491,\n",
       " 'part': 492,\n",
       " 'pasha': 493,\n",
       " 'perish': 494,\n",
       " 'permiss': 495,\n",
       " 'persuad': 496,\n",
       " 'pickax': 497,\n",
       " 'plan': 498,\n",
       " 'polit': 499,\n",
       " 'politician': 500,\n",
       " 'port': 501,\n",
       " 'posit': 502,\n",
       " 'post': 503,\n",
       " 'prevent': 504,\n",
       " 'prime': 505,\n",
       " 'prison': 506,\n",
       " 'prof': 507,\n",
       " 'profess': 508,\n",
       " 'project': 509,\n",
       " 'provid': 510,\n",
       " 'public': 511,\n",
       " 'read': 512,\n",
       " 'recommend': 513,\n",
       " 'red': 514,\n",
       " 'reduc': 515,\n",
       " 'refer': 516,\n",
       " 'relat': 517,\n",
       " 'renov': 518,\n",
       " 'retir': 519,\n",
       " 'return': 520,\n",
       " 'rous': 521,\n",
       " 'rout': 522,\n",
       " 'rumor': 523,\n",
       " 'said': 524,\n",
       " 'sail': 525,\n",
       " 'scandal': 526,\n",
       " 'scheme': 527,\n",
       " 'scienc': 528,\n",
       " 'sea': 529,\n",
       " 'seen': 530,\n",
       " 'sentenc': 531,\n",
       " 'share': 532,\n",
       " 'ship': 533,\n",
       " 'slightli': 534,\n",
       " 'sold': 535,\n",
       " 'sometim': 536,\n",
       " 'son': 537,\n",
       " 'soon': 538,\n",
       " 'spain': 539,\n",
       " 'spent': 540,\n",
       " 'stage': 541,\n",
       " 'subsequ': 542,\n",
       " 'substanti': 543,\n",
       " 'suez': 544,\n",
       " 'sultan': 545,\n",
       " 'supervis': 546,\n",
       " 'surveyor': 547,\n",
       " 'survivor': 548,\n",
       " 'ten': 549,\n",
       " 'therebi': 550,\n",
       " 'thi': 551,\n",
       " 'thoma': 552,\n",
       " 'tower': 553,\n",
       " 'trade': 554,\n",
       " 'treat': 555,\n",
       " 'trial': 556,\n",
       " 'tunisia': 557,\n",
       " 'uncl': 558,\n",
       " 'undertak': 559,\n",
       " 'univers': 560,\n",
       " 'universel': 561,\n",
       " 'versail': 562,\n",
       " 'vice': 563,\n",
       " 'viceroi': 564,\n",
       " 'vicomt': 565,\n",
       " 'visionari': 566,\n",
       " 'visit': 567,\n",
       " 'voyag': 568,\n",
       " 'w': 569,\n",
       " 'warm': 570,\n",
       " 'welcom': 571,\n",
       " 'west': 572,\n",
       " 'western': 573,\n",
       " 'wish': 574,\n",
       " 'yellow': 575,\n",
       " 'yvelin': 576,\n",
       " 'zenith': 577,\n",
       " 'aboard': 578,\n",
       " 'accident': 579,\n",
       " 'accompani': 580,\n",
       " 'ad': 581,\n",
       " 'along': 582,\n",
       " 'angri': 583,\n",
       " 'appeal': 584,\n",
       " 'appear': 585,\n",
       " 'arrang': 586,\n",
       " 'audienc': 587,\n",
       " 'band': 588,\n",
       " 'beat': 589,\n",
       " 'becom': 590,\n",
       " 'bed': 591,\n",
       " 'behind': 592,\n",
       " 'bell': 593,\n",
       " 'black': 594,\n",
       " 'board': 595,\n",
       " 'boat': 596,\n",
       " 'bridg': 597,\n",
       " 'brother': 598,\n",
       " 'came': 599,\n",
       " 'captain': 600,\n",
       " 'cartoon': 601,\n",
       " 'chappatt': 602,\n",
       " 'charact': 603,\n",
       " 'citi': 604,\n",
       " 'click': 605,\n",
       " 'coloni': 606,\n",
       " 'consist': 607,\n",
       " 'could': 608,\n",
       " 'dave': 609,\n",
       " 'debut': 610,\n",
       " 'disnei': 611,\n",
       " 'distribut': 612,\n",
       " 'distributor': 613,\n",
       " 'doubt': 614,\n",
       " 'employe': 615,\n",
       " 'eventu': 616,\n",
       " 'fall': 617,\n",
       " 'fame': 618,\n",
       " 'fight': 619,\n",
       " 'finish': 620,\n",
       " 'fleischer': 621,\n",
       " 'flickr': 622,\n",
       " 'forth': 623,\n",
       " 'freshwat': 624,\n",
       " 'fun': 625,\n",
       " 'girlfriend': 626,\n",
       " 'green': 627,\n",
       " 'hire': 628,\n",
       " 'impress': 629,\n",
       " 'increas': 630,\n",
       " 'indic': 631,\n",
       " 'initi': 632,\n",
       " 'inkwel': 633,\n",
       " 'instal': 634,\n",
       " 'instant': 635,\n",
       " 'intend': 636,\n",
       " 'keep': 637,\n",
       " 'knock': 638,\n",
       " 'like': 639,\n",
       " 'live': 640,\n",
       " 'lost': 641,\n",
       " 'love': 642,\n",
       " 'lucki': 643,\n",
       " 'make': 644,\n",
       " 'mark': 645,\n",
       " 'max': 646,\n",
       " 'meant': 647,\n",
       " 'mice': 648,\n",
       " 'mickei': 649,\n",
       " 'microphon': 650,\n",
       " 'mid': 651,\n",
       " 'minni': 652,\n",
       " 'mous': 653,\n",
       " 'mouth': 654,\n",
       " 'movi': 655,\n",
       " 'music': 656,\n",
       " 'musician': 657,\n",
       " 'notabl': 658,\n",
       " 'novelti': 659,\n",
       " 'optic': 660,\n",
       " 'oswald': 661,\n",
       " 'parrot': 662,\n",
       " 'partli': 663,\n",
       " 'patrick': 664,\n",
       " 'peel': 665,\n",
       " 'percuss': 666,\n",
       " 'person': 667,\n",
       " 'pete': 668,\n",
       " 'phonofilm': 669,\n",
       " 'potato': 670,\n",
       " 'power': 671,\n",
       " 'precis': 672,\n",
       " 'premier': 673,\n",
       " 'present': 674,\n",
       " 'probabl': 675,\n",
       " 'proce': 676,\n",
       " 'profession': 677,\n",
       " 'rabbit': 678,\n",
       " 'real': 679,\n",
       " 'realiz': 680,\n",
       " 'releas': 681,\n",
       " 'right': 682,\n",
       " 'rise': 683,\n",
       " 'river': 684,\n",
       " 'screen': 685,\n",
       " 'sheet': 686,\n",
       " 'silent': 687,\n",
       " 'small': 688,\n",
       " 'sound': 689,\n",
       " 'soundtrack': 690,\n",
       " 'speaker': 691,\n",
       " 'special': 692,\n",
       " 'star': 693,\n",
       " 'steamboat': 694,\n",
       " 'steer': 695,\n",
       " 'studio': 696,\n",
       " 'synchron': 697,\n",
       " 'test': 698,\n",
       " 'theater': 699,\n",
       " 'third': 700,\n",
       " 'throw': 701,\n",
       " 'togeth': 702,\n",
       " 'track': 703,\n",
       " 'trip': 704,\n",
       " 'two': 705,\n",
       " 'unfortun': 706,\n",
       " 'user': 707,\n",
       " 'walk': 708,\n",
       " 'walt': 709,\n",
       " 'whistl': 710,\n",
       " 'willi': 711,\n",
       " 'wive': 712,\n",
       " 'achiev': 713,\n",
       " 'administr': 714,\n",
       " 'albert': 715,\n",
       " 'applic': 716,\n",
       " 'arnold': 717,\n",
       " 'atom': 718,\n",
       " 'award': 719,\n",
       " 'background': 720,\n",
       " 'berlin': 721,\n",
       " 'bildung': 722,\n",
       " 'bomb': 723,\n",
       " 'budapest': 724,\n",
       " 'chain': 725,\n",
       " 'chicago': 726,\n",
       " 'citizen': 727,\n",
       " 'clinton': 728,\n",
       " 'conscious': 729,\n",
       " 'conserv': 730,\n",
       " 'contain': 731,\n",
       " 'contribut': 732,\n",
       " 'convert': 733,\n",
       " 'cool': 734,\n",
       " 'cp': 735,\n",
       " 'crystallographi': 736,\n",
       " 'david': 737,\n",
       " 'decai': 738,\n",
       " 'design': 739,\n",
       " 'dewitt': 740,\n",
       " 'director': 741,\n",
       " 'disappoint': 742,\n",
       " 'discoveri': 743,\n",
       " 'dissoci': 744,\n",
       " 'doctor': 745,\n",
       " 'einstein': 746,\n",
       " 'elementari': 747,\n",
       " 'enjoi': 748,\n",
       " 'enrico': 749,\n",
       " 'eugen': 750,\n",
       " 'except': 751,\n",
       " 'fermi': 752,\n",
       " 'firm': 753,\n",
       " 'format': 754,\n",
       " 'foundat': 755,\n",
       " 'franklin': 756,\n",
       " 'fundament': 757,\n",
       " 'german': 758,\n",
       " 'goeppert': 759,\n",
       " 'gone': 760,\n",
       " 'gradeplutonium': 761,\n",
       " 'graphit': 762,\n",
       " 'han': 763,\n",
       " 'hilbert': 764,\n",
       " 'hinduism': 765,\n",
       " 'hungarian': 766,\n",
       " 'idea': 767,\n",
       " 'independ': 768,\n",
       " 'institut': 769,\n",
       " 'introduc': 770,\n",
       " 'j': 771,\n",
       " 'januari': 772,\n",
       " 'jensen': 773,\n",
       " 'juli': 774,\n",
       " 'kaiser': 775,\n",
       " 'karl': 776,\n",
       " 'laboratori': 777,\n",
       " 'laid': 778,\n",
       " 'le': 779,\n",
       " 'led': 780,\n",
       " 'left': 781,\n",
       " 'len': 782,\n",
       " 'letter': 783,\n",
       " 'life': 784,\n",
       " 'logic': 785,\n",
       " 'manhattan': 786,\n",
       " 'maria': 787,\n",
       " 'mathematician': 788,\n",
       " 'matrix': 789,\n",
       " 'mayer': 790,\n",
       " 'medal': 791,\n",
       " 'meet': 792,\n",
       " 'michael': 793,\n",
       " 'moder': 794,\n",
       " 'molecul': 795,\n",
       " 'molek': 796,\n",
       " 'mw': 797,\n",
       " 'nation': 798,\n",
       " 'nazi': 799,\n",
       " 'near': 800,\n",
       " 'neutron': 801,\n",
       " 'newspap': 802,\n",
       " 'nonetheless': 803,\n",
       " 'nuclear': 804,\n",
       " 'nucleu': 805,\n",
       " 'oak': 806,\n",
       " 'occur': 807,\n",
       " 'offer': 808,\n",
       " 'paper': 809,\n",
       " 'particl': 810,\n",
       " 'particularli': 811,\n",
       " 'pass': 812,\n",
       " 'paul': 813,\n",
       " 'pervad': 814,\n",
       " 'philosoph': 815,\n",
       " 'philosophi': 816,\n",
       " 'physicist': 817,\n",
       " 'pile': 818,\n",
       " 'polanyi': 819,\n",
       " 'possibl': 820,\n",
       " 'presid': 821,\n",
       " 'princeton': 822,\n",
       " 'principl': 823,\n",
       " 'product': 824,\n",
       " 'prove': 825,\n",
       " 'quantum': 826,\n",
       " 'rai': 827,\n",
       " 'rate': 828,\n",
       " 'rd': 829,\n",
       " 'reaction': 830,\n",
       " 'reactor': 831,\n",
       " 'request': 832,\n",
       " 'rest': 833,\n",
       " 'result': 834,\n",
       " 'ridg': 835,\n",
       " 'roosevelt': 836,\n",
       " 'rule': 837,\n",
       " 'saw': 838,\n",
       " 'shift': 839,\n",
       " 'short': 840,\n",
       " 'someon': 841,\n",
       " 'someth': 842,\n",
       " 'sommerfeld': 843,\n",
       " 'spend': 844,\n",
       " 'state': 845,\n",
       " 'structur': 846,\n",
       " 'studi': 847,\n",
       " 'symmetri': 848,\n",
       " 'szil': 849,\n",
       " 'tanneri': 850,\n",
       " 'team': 851,\n",
       " 'tempera': 852,\n",
       " 'tenn': 853,\n",
       " 'term': 854,\n",
       " 'theoret': 855,\n",
       " 'theori': 856,\n",
       " 'thesi': 857,\n",
       " 'thing': 858,\n",
       " 'thought': 859,\n",
       " 'ttingen': 860,\n",
       " 'tyler': 861,\n",
       " 'und': 862,\n",
       " 'unit': 863,\n",
       " 'uranium': 864,\n",
       " 'urg': 865,\n",
       " 'vanish': 866,\n",
       " 'vedanta': 867,\n",
       " 'von': 868,\n",
       " 'want': 869,\n",
       " 'water': 870,\n",
       " 'weapon': 871,\n",
       " 'weissenberg': 872,\n",
       " 'wick': 873,\n",
       " 'wigner': 874,\n",
       " 'wilhelm': 875,\n",
       " 'wisconsin': 876,\n",
       " 'without': 877,\n",
       " 'x': 878,\n",
       " 'yet': 879,\n",
       " 'zerfal': 880,\n",
       " 'abstract': 881,\n",
       " 'academ': 882,\n",
       " 'accademia': 883,\n",
       " 'acquir': 884,\n",
       " 'addit': 885,\n",
       " 'alreadi': 886,\n",
       " 'although': 887,\n",
       " 'appli': 888,\n",
       " 'artist': 889,\n",
       " 'austrian': 890,\n",
       " 'axiomat': 891,\n",
       " 'befor': 892,\n",
       " 'beltrami': 893,\n",
       " 'bologna': 894,\n",
       " 'bolyai': 895,\n",
       " 'brioschi': 896,\n",
       " 'c': 897,\n",
       " 'capit': 898,\n",
       " 'case': 899,\n",
       " 'certainli': 900,\n",
       " 'chair': 901,\n",
       " 'circl': 902,\n",
       " 'clariti': 903,\n",
       " 'concept': 904,\n",
       " 'concern': 905,\n",
       " 'constant': 906,\n",
       " 'correspond': 907,\n",
       " 'cremona': 908,\n",
       " 'curvatur': 909,\n",
       " 'dei': 910,\n",
       " 'deriv': 911,\n",
       " 'differenti': 912,\n",
       " 'dimens': 913,\n",
       " 'dimension': 914,\n",
       " 'discontinu': 915,\n",
       " 'disk': 916,\n",
       " 'done': 917,\n",
       " 'due': 918,\n",
       " 'elast': 919,\n",
       " 'electr': 920,\n",
       " 'empir': 921,\n",
       " 'enthusiast': 922,\n",
       " 'equiconsist': 923,\n",
       " 'escher': 924,\n",
       " 'especi': 925,\n",
       " 'essai': 926,\n",
       " 'establish': 927,\n",
       " 'euclidean': 928,\n",
       " 'euclidian': 929,\n",
       " 'eugenio': 930,\n",
       " 'expel': 931,\n",
       " 'exposit': 932,\n",
       " 'fashion': 933,\n",
       " 'formula': 934,\n",
       " 'four': 935,\n",
       " 'francesco': 936,\n",
       " 'function': 937,\n",
       " 'geodes': 938,\n",
       " 'geometri': 939,\n",
       " 'ghislieri': 940,\n",
       " 'half': 941,\n",
       " 'hard': 942,\n",
       " 'hardship': 943,\n",
       " 'hyperbol': 944,\n",
       " 'illustr': 945,\n",
       " 'influenc': 946,\n",
       " 'inherit': 947,\n",
       " 'interpret': 948,\n",
       " 'invigor': 949,\n",
       " 'italian': 950,\n",
       " 'kingdom': 951,\n",
       " 'klein': 952,\n",
       " 'less': 953,\n",
       " 'limit': 954,\n",
       " 'lincei': 955,\n",
       " 'line': 956,\n",
       " 'lobachevski': 957,\n",
       " 'lombardi': 958,\n",
       " 'lover': 959,\n",
       " 'magnet': 960,\n",
       " 'matematich': 961,\n",
       " 'mathemat': 962,\n",
       " 'mathhist': 963,\n",
       " 'memoir': 964,\n",
       " 'milan': 965,\n",
       " 'mind': 966,\n",
       " 'miniatur': 967,\n",
       " 'model': 968,\n",
       " 'move': 969,\n",
       " 'musicrath': 970,\n",
       " 'n': 971,\n",
       " 'neg': 972,\n",
       " 'next': 973,\n",
       " 'non': 974,\n",
       " 'note': 975,\n",
       " 'obtain': 976,\n",
       " 'oper': 977,\n",
       " 'opinion': 978,\n",
       " 'ordinari': 979,\n",
       " 'paint': 980,\n",
       " 'pavia': 981,\n",
       " 'plane': 982,\n",
       " 'poincar': 983,\n",
       " 'posthum': 984,\n",
       " 'previous': 985,\n",
       " 'professor': 986,\n",
       " 'proof': 987,\n",
       " 'propos': 988,\n",
       " 'pseudospher': 989,\n",
       " 'railroad': 990,\n",
       " 'ration': 991,\n",
       " 'recept': 992,\n",
       " 'recogn': 993,\n",
       " 'relationship': 994,\n",
       " 'remark': 995,\n",
       " 'replac': 996,\n",
       " 'repres': 997,\n",
       " 'rome': 998,\n",
       " 'scene': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = {}\n",
    "for i in range(len(doc_proc)):\n",
    "    for j in range(len(doc_proc.iloc[i]['filtrada'])):\n",
    "        if doc_proc.iloc[i]['filtrada'][j] not in dictionary:\n",
    "            dictionary[doc_proc.iloc[i]['filtrada'][j]] = len(dictionary)\n",
    "dictionary_size = len(dictionary)\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_vector(doc):\n",
    "    vector = np.zeros(dictionary_size)\n",
    "    for token in doc:\n",
    "        if token in dictionary:\n",
    "            vector[dictionary[token]] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Documento</th>\n",
       "      <th>filtrada</th>\n",
       "      <th>doc_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>William Beaumont and the Human Digestion.\\n\\nW...</td>\n",
       "      <td>[accid, acid, activ, affect, ag, alexi, also, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Selma Lagerlöf and the wonderful Adventures of...</td>\n",
       "      <td>[abl, academi, accept, acclaim, accomplish, ac...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ferdinand de Lesseps and the Suez Canal.\\n\\nFe...</td>\n",
       "      <td>[abandon, act, adopt, affair, africa, afterwar...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Walt Disney’s ‘Steamboat Willie’ and the Rise ...</td>\n",
       "      <td>[aboard, accident, accompani, ad, along, also,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eugene Wigner and the Structure of the Atomic ...</td>\n",
       "      <td>[accept, achiev, ad, administr, albert, along,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>James Parkinson and Parkinson’s Disease.\\n\\nWo...</td>\n",
       "      <td>[abnorm, activist, addit, advanc, advoc, ag, a...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>Juan de la Cierva and the Autogiro.\\n\\nDemonst...</td>\n",
       "      <td>[abil, acceler, accept, accid, accomplish, ach...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>Squire Whipple – The Father of the Iron Bridge...</td>\n",
       "      <td>[academi, across, ag, also, america, american,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>William Playfair and the Beginnings of Infogra...</td>\n",
       "      <td>[accept, account, achiev, actual, adapt, after...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>Juan Bautista de Anza and the Route to San Fra...</td>\n",
       "      <td>[agre, along, alta, altar, american, anza, apa...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>331 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Documento  \\\n",
       "0    William Beaumont and the Human Digestion.\\n\\nW...   \n",
       "1    Selma Lagerlöf and the wonderful Adventures of...   \n",
       "2    Ferdinand de Lesseps and the Suez Canal.\\n\\nFe...   \n",
       "3    Walt Disney’s ‘Steamboat Willie’ and the Rise ...   \n",
       "4    Eugene Wigner and the Structure of the Atomic ...   \n",
       "..                                                 ...   \n",
       "326  James Parkinson and Parkinson’s Disease.\\n\\nWo...   \n",
       "327  Juan de la Cierva and the Autogiro.\\n\\nDemonst...   \n",
       "328  Squire Whipple – The Father of the Iron Bridge...   \n",
       "329  William Playfair and the Beginnings of Infogra...   \n",
       "330  Juan Bautista de Anza and the Route to San Fra...   \n",
       "\n",
       "                                              filtrada  \\\n",
       "0    [accid, acid, activ, affect, ag, alexi, also, ...   \n",
       "1    [abl, academi, accept, acclaim, accomplish, ac...   \n",
       "2    [abandon, act, adopt, affair, africa, afterwar...   \n",
       "3    [aboard, accident, accompani, ad, along, also,...   \n",
       "4    [accept, achiev, ad, administr, albert, along,...   \n",
       "..                                                 ...   \n",
       "326  [abnorm, activist, addit, advanc, advoc, ag, a...   \n",
       "327  [abil, acceler, accept, accid, accomplish, ach...   \n",
       "328  [academi, across, ag, also, america, american,...   \n",
       "329  [accept, account, achiev, actual, adapt, after...   \n",
       "330  [agre, along, alta, altar, american, anza, apa...   \n",
       "\n",
       "                                            doc_vector  \n",
       "0    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
       "2    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...  \n",
       "..                                                 ...  \n",
       "326  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, ...  \n",
       "327  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, ...  \n",
       "328  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...  \n",
       "329  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
       "330  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
       "\n",
       "[331 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_proc['doc_vector'] = doc_proc.filtrada.apply(doc_to_vector)\n",
    "doc_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fabrication of music instruments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>famous German poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Romanticism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>University of Edinburgh research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bridge construction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Walk of Fame stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Scientists who worked on the atomic bomb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Invention of the Internet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>early telecommunication methods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Who explored the South Pole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>famous members of the Royal Navy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Nobel Prize winning inventions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>South America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Edward Teller and Marie Curie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Computing Language for the programming of Arti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>William Hearst movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How did Captain James Cook become an explorer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How did Grace Hopper get famous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Computers in Astronomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>WWII aircraft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Literary critics on Thomas Moore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Nazis confiscate or destroy art and literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Modern Age in English Literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>modern Physiology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Roman Empire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Scientists who have contributed to photosynthesis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Aviation pioneers' publications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Gutenberg Bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Religious beliefs of scientists and explorers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Carl Friedrich Gauss influence on colleagues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Personalities from Hannover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Skinner's experiments with the operant conditi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Napoleon's Russian Campaign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Friends and enemies of Napoleon Bonaparte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>First woman who won a Nobel Prize</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Query\n",
       "0                    Fabrication of music instruments\n",
       "1                                famous German poetry\n",
       "2                                         Romanticism\n",
       "3                    University of Edinburgh research\n",
       "4                                 bridge construction\n",
       "5                                  Walk of Fame stars\n",
       "6            Scientists who worked on the atomic bomb\n",
       "7                           Invention of the Internet\n",
       "8                     early telecommunication methods\n",
       "9                         Who explored the South Pole\n",
       "10                   famous members of the Royal Navy\n",
       "11                     Nobel Prize winning inventions\n",
       "12                                      South America\n",
       "13                      Edward Teller and Marie Curie\n",
       "14  Computing Language for the programming of Arti...\n",
       "15                               William Hearst movie\n",
       "16      How did Captain James Cook become an explorer\n",
       "17                    How did Grace Hopper get famous\n",
       "18                             Computers in Astronomy\n",
       "19                                      WWII aircraft\n",
       "20                   Literary critics on Thomas Moore\n",
       "21     Nazis confiscate or destroy art and literature\n",
       "22                   Modern Age in English Literature\n",
       "23                                  modern Physiology\n",
       "24                                       Roman Empire\n",
       "25  Scientists who have contributed to photosynthesis\n",
       "26                    Aviation pioneers' publications\n",
       "27                                    Gutenberg Bible\n",
       "28      Religious beliefs of scientists and explorers\n",
       "29       Carl Friedrich Gauss influence on colleagues\n",
       "30                        Personalities from Hannover\n",
       "31  Skinner's experiments with the operant conditi...\n",
       "32                        Napoleon's Russian Campaign\n",
       "33          Friends and enemies of Napoleon Bonaparte\n",
       "34                  First woman who won a Nobel Prize"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos_querry = get_documents('datos/queries-raw-texts/')\n",
    "queries = pd.DataFrame(datos_querry,columns=['Query'])\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>filtrada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fabrication of music instruments</td>\n",
       "      <td>[fabric, instrument, music]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>famous German poetry</td>\n",
       "      <td>[famou, german, poetri]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Romanticism</td>\n",
       "      <td>[romantic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>University of Edinburgh research</td>\n",
       "      <td>[edinburgh, research, univers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bridge construction</td>\n",
       "      <td>[bridg, construct]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Query                        filtrada\n",
       "0  Fabrication of music instruments     [fabric, instrument, music]\n",
       "1              famous German poetry         [famou, german, poetri]\n",
       "2                       Romanticism                      [romantic]\n",
       "3  University of Edinburgh research  [edinburgh, research, univers]\n",
       "4               bridge construction              [bridg, construct]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries['filtrada'] = queries.Query.apply(preprocesar)\n",
    "quer_proc = queries\n",
    "quer_proc.filtrada = quer_proc.filtrada.apply(np.unique)\n",
    "quer_proc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>filtrada</th>\n",
       "      <th>query_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fabrication of music instruments</td>\n",
       "      <td>[fabric, instrument, music]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>famous German poetry</td>\n",
       "      <td>[famou, german, poetri]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Romanticism</td>\n",
       "      <td>[romantic]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>University of Edinburgh research</td>\n",
       "      <td>[edinburgh, research, univers]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bridge construction</td>\n",
       "      <td>[bridg, construct]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Walk of Fame stars</td>\n",
       "      <td>[fame, star, walk]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Scientists who worked on the atomic bomb</td>\n",
       "      <td>[atom, bomb, scientist, work]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Invention of the Internet</td>\n",
       "      <td>[internet, invent]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>early telecommunication methods</td>\n",
       "      <td>[earli, method, telecommun]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Who explored the South Pole</td>\n",
       "      <td>[explor, pole, south]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>famous members of the Royal Navy</td>\n",
       "      <td>[famou, member, navi, royal]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Nobel Prize winning inventions</td>\n",
       "      <td>[invent, nobel, prize, win]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>South America</td>\n",
       "      <td>[america, south]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Edward Teller and Marie Curie</td>\n",
       "      <td>[curi, edward, mari, teller]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Computing Language for the programming of Arti...</td>\n",
       "      <td>[artifici, comput, intellig, languag, program]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>William Hearst movie</td>\n",
       "      <td>[hearst, movi, william]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How did Captain James Cook become an explorer</td>\n",
       "      <td>[becom, captain, cook, explor, jame]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How did Grace Hopper get famous</td>\n",
       "      <td>[famou, get, grace, hopper]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Computers in Astronomy</td>\n",
       "      <td>[astronomi, comput]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>WWII aircraft</td>\n",
       "      <td>[aircraft, wwii]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Literary critics on Thomas Moore</td>\n",
       "      <td>[critic, literari, moor, thoma]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Nazis confiscate or destroy art and literature</td>\n",
       "      <td>[art, confisc, destroi, literatur, nazi]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Modern Age in English Literature</td>\n",
       "      <td>[ag, english, literatur, modern]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>modern Physiology</td>\n",
       "      <td>[modern, physiolog]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Roman Empire</td>\n",
       "      <td>[empir, roman]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Scientists who have contributed to photosynthesis</td>\n",
       "      <td>[contribut, photosynthesi, scientist]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Aviation pioneers' publications</td>\n",
       "      <td>[aviat, pioneer, public]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Gutenberg Bible</td>\n",
       "      <td>[bibl, gutenberg]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Religious beliefs of scientists and explorers</td>\n",
       "      <td>[belief, explor, religi, scientist]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Carl Friedrich Gauss influence on colleagues</td>\n",
       "      <td>[carl, colleagu, friedrich, gauss, influenc]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Personalities from Hannover</td>\n",
       "      <td>[hannov, person]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Skinner's experiments with the operant conditi...</td>\n",
       "      <td>[chamber, condit, experi, oper, skinner]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Napoleon's Russian Campaign</td>\n",
       "      <td>[campaign, napoleon, russian]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Friends and enemies of Napoleon Bonaparte</td>\n",
       "      <td>[bonapart, enemi, friend, napoleon]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>First woman who won a Nobel Prize</td>\n",
       "      <td>[first, nobel, prize, woman]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Query  \\\n",
       "0                    Fabrication of music instruments   \n",
       "1                                famous German poetry   \n",
       "2                                         Romanticism   \n",
       "3                    University of Edinburgh research   \n",
       "4                                 bridge construction   \n",
       "5                                  Walk of Fame stars   \n",
       "6            Scientists who worked on the atomic bomb   \n",
       "7                           Invention of the Internet   \n",
       "8                     early telecommunication methods   \n",
       "9                         Who explored the South Pole   \n",
       "10                   famous members of the Royal Navy   \n",
       "11                     Nobel Prize winning inventions   \n",
       "12                                      South America   \n",
       "13                      Edward Teller and Marie Curie   \n",
       "14  Computing Language for the programming of Arti...   \n",
       "15                               William Hearst movie   \n",
       "16      How did Captain James Cook become an explorer   \n",
       "17                    How did Grace Hopper get famous   \n",
       "18                             Computers in Astronomy   \n",
       "19                                      WWII aircraft   \n",
       "20                   Literary critics on Thomas Moore   \n",
       "21     Nazis confiscate or destroy art and literature   \n",
       "22                   Modern Age in English Literature   \n",
       "23                                  modern Physiology   \n",
       "24                                       Roman Empire   \n",
       "25  Scientists who have contributed to photosynthesis   \n",
       "26                    Aviation pioneers' publications   \n",
       "27                                    Gutenberg Bible   \n",
       "28      Religious beliefs of scientists and explorers   \n",
       "29       Carl Friedrich Gauss influence on colleagues   \n",
       "30                        Personalities from Hannover   \n",
       "31  Skinner's experiments with the operant conditi...   \n",
       "32                        Napoleon's Russian Campaign   \n",
       "33          Friends and enemies of Napoleon Bonaparte   \n",
       "34                  First woman who won a Nobel Prize   \n",
       "\n",
       "                                          filtrada  \\\n",
       "0                      [fabric, instrument, music]   \n",
       "1                          [famou, german, poetri]   \n",
       "2                                       [romantic]   \n",
       "3                   [edinburgh, research, univers]   \n",
       "4                               [bridg, construct]   \n",
       "5                               [fame, star, walk]   \n",
       "6                    [atom, bomb, scientist, work]   \n",
       "7                               [internet, invent]   \n",
       "8                      [earli, method, telecommun]   \n",
       "9                            [explor, pole, south]   \n",
       "10                    [famou, member, navi, royal]   \n",
       "11                     [invent, nobel, prize, win]   \n",
       "12                                [america, south]   \n",
       "13                    [curi, edward, mari, teller]   \n",
       "14  [artifici, comput, intellig, languag, program]   \n",
       "15                         [hearst, movi, william]   \n",
       "16            [becom, captain, cook, explor, jame]   \n",
       "17                     [famou, get, grace, hopper]   \n",
       "18                             [astronomi, comput]   \n",
       "19                                [aircraft, wwii]   \n",
       "20                 [critic, literari, moor, thoma]   \n",
       "21        [art, confisc, destroi, literatur, nazi]   \n",
       "22                [ag, english, literatur, modern]   \n",
       "23                             [modern, physiolog]   \n",
       "24                                  [empir, roman]   \n",
       "25           [contribut, photosynthesi, scientist]   \n",
       "26                        [aviat, pioneer, public]   \n",
       "27                               [bibl, gutenberg]   \n",
       "28             [belief, explor, religi, scientist]   \n",
       "29    [carl, colleagu, friedrich, gauss, influenc]   \n",
       "30                                [hannov, person]   \n",
       "31        [chamber, condit, experi, oper, skinner]   \n",
       "32                   [campaign, napoleon, russian]   \n",
       "33             [bonapart, enemi, friend, napoleon]   \n",
       "34                    [first, nobel, prize, woman]   \n",
       "\n",
       "                                         query_vector  \n",
       "0   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "5   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "6   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "7   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "8   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "9   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "10  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "11  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "12  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "13  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "14  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "15  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "16  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "17  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "18  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "19  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "20  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "21  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "22  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "23  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "24  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "25  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "26  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "27  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "28  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "29  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "30  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "31  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "32  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "33  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "34  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quer_proc['query_vector'] = quer_proc.filtrada.apply(doc_to_vector)\n",
    "quer_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similitud_coseno_docs(query_vector):\n",
    "    similitud = doc_proc['doc_vector'].apply(lambda x: similitud_coseno(x,query_vector))\n",
    "    similitud = similitud[similitud>0]\n",
    "    similitud = similitud.sort_values(ascending=False)\n",
    "    output = ''\n",
    "    for index, value in similitud.items():\n",
    "        if output == '':\n",
    "            output += f'd{index}:{value}'\n",
    "        else:\n",
    "            output += f',d{index}:{value}'\n",
    "    return output\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = quer_proc.query_vector.apply(similitud_coseno_docs)\n",
    "f = open(\"salida/RRDV-consultas_resultads.txt\", \"w\")\n",
    "for i in range(len(q)):\n",
    "    f.write(f'q{i+1} {q[i]}\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7a79e9e75522a046d95171e373010a5dca4ce6e8605d007854b2218f1d88052"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
